RuntimeError: Given groups=1, weight of size [16, 1, 5, 5], expected input[5, 4, 400, 300] to have 1 channels, but got 4 channels instead

>>> solved by switching image from RGBA to L; since the MNIST dataset is B&W 

RuntimeError: size mismatch, m1: [5 x 240000], m2: [1568 x 10] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:2070

images
i 0
size batched torch.Size([5, 1, 400, 300])
grasp
size batched grasp torch.Size([5])
one batch of grasp labels:  tensor([3, 4, 3, 2, 4])

>> ???


ges
i of batch:  0
size of batched images torch.Size([5, 3, 400, 300])
grasp
size of batched grasp labels torch.Size([5, 2])
one batch of grasp labels:  tensor([[336, 105],
        [ 68,  56],
        [297, 230],
        [ 38, 112],
        [331,  37]])
----------- LOSS DEBUGGING ----

sizes of outputs: torch.Size([1, 2])
sizes of labels:  torch.Size([5, 2])

outputs  tensor([[-0.1063, -0.0349]], grad_fn=<ThAddmmBackward>)

labels tensor([[336, 105],
        [ 68,  56],
        [297, 230],
        [ 38, 112],
        [331,  37]])

----------- LOSS DEBUGGING ----

Traceback (most recent call last):
  File "imagegen.py", line 275, in <module>
    loss = criterion(outputs, labels)
  File "/home/rui/v3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/rui/v3/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 862, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/rui/v3/lib/python3.6/site-packages/torch/nn/functional.py", line 1550, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/rui/v3/lib/python3.6/site-packages/torch/nn/functional.py", line 1405, in nll_loss
    .format(input.size(0), target.size(0)))
ValueError: Expected input batch_size (1) to match target batch_size (5).




----------- i 6757
inputs size torch.Size([4, 3, 32, 32])
outputs size torch.Size([4, 10])
labels size torch.Size([4])

outputs tensor([[-0.0841,  0.0786, -0.1027,  0.0942,  0.0062, -0.0385, -0.0510,  0.0095, 0.0545,  0.0020],
        [-0.0827,  0.0794, -0.0967,  0.0939,  0.0140, -0.0365, -0.0467,  0.0084, 0.0478,  0.0092],
        [-0.0870,  0.0823, -0.1052,  0.0957,  0.0057, -0.0463, -0.0512,  0.0141, 0.0615, -0.0029],
        [-0.0902,  0.0920, -0.1042,  0.1039, -0.0036, -0.0450, -0.0523,  0.0104, 0.0625,  0.0015]], grad_fn=<ThAddmmBackward>)
ground truth labels size torch.Size([4])


labels tensor([4, 0, 5, 9])

criterion = nn.CrossEntropyLoss
loss = criterion(outputs, labels)


>>> ???


https://jdhao.github.io/2017/11/15/pytorch-datatype-note/
As another example, several loss functions like CrossEntropyLoss require that the target should be torch LongTensor. So before doing operations, make sure that your input Tensor types match the function definitions.

It is easy to convert the type of one Tensor to another Tensor. Suppose x and y are Tensor of different types. You can use x.type(y.type()) or x.type_as(y) to convert x to the type of y.
=======

    x = x.view(-1, 16*5*5)
RuntimeError: invalid argument 2: size '[-1 x 400]' is invalid for input with 558720 elements at /pytorch/aten/src/TH/THStorage.cpp:80


>>>  changed to x = x.view(-1, 558720) by hardcoding... but why? What is this
number?



Traceback (most recent call last):
  File "imagegen_strippedclean.py", line 142, in <module>
    loss = criterion(outputs, labels)
  File "/home/rui/mlenv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/rui/mlenv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 421, in forward
    return F.mse_loss(input, target, reduction=self.reduction)
  File "/home/rui/mlenv/lib/python3.6/site-packages/torch/nn/functional.py", line 1716, in mse_loss
    return _pointwise_loss(lambda a, b: (a - b) ** 2, torch._C._nn.mse_loss, input, target, reduction)
  File "/home/rui/mlenv/lib/python3.6/site-packages/torch/nn/functional.py", line 1674, in _pointwise_loss
    return lambd_optimized(input, target, reduction)
RuntimeError: input and target shapes do not match: input [97 x 2], target [5 x 2] at /pytorch/aten/src/THNN/generic/MSECriterion.c:12



torch.Size([5, 3, 400, 300])
torch.Size([5, 6, 198, 148])
torch.Size([5, 16, 97, 72])
torch.Size([97, 5760])
torch.Size([97, 120])
torch.Size([97, 84])
torch.Size([97, 2])


>>> changed from random incorrect constant to correct size (16*96*72) --> the "5" batch size was getting killed by
the x.view(-1, wrong_constant)


