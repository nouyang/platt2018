{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Sliding Window\n",
    "# Feb 2019\n",
    "#\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import torchvision\n",
    "from IPython.display import display  # to display images\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import io\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sliding Window\n",
    "# Feb 2019\n",
    "# Implement classfication and regression as two separate networks\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import torchvision\n",
    "from IPython.display import display  # to display images\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import io\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataset -------------------------------------------------------\n",
    "IMG_X, IMG_Y = 200, 200\n",
    "# length and width of blocks (fixed for now)\n",
    "block_l, block_w = 20, 30\n",
    "\n",
    "\n",
    "def makeRectangle(l, w, theta, offset=(0, 0)):\n",
    "    c, s = math.cos(theta), math.sin(theta)\n",
    "    rectCoords = [(l / 2.0, w / 2.0), (l / 2.0, -w / 2.0),\n",
    "                  (-l / 2.0, -w / 2.0), (-l / 2.0, w / 2.0), ]\n",
    "    return [(c * x - s * y + offset[0],\n",
    "             s * x + c * y + offset[1]) for (x, y) in rectCoords]\n",
    "\n",
    "\n",
    "# ---- Make depth images ---\n",
    "def make_dataset(dirname, num_images):\n",
    "    true_coords = []\n",
    "    newpath = \"./\" + dirname\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "        print(newpath)\n",
    "    for i in range(num_images):\n",
    "        # orient = 0 # degrees\n",
    "        img = Image.new(\"RGB\", (IMG_X, IMG_Y), \"black\")\n",
    "\n",
    "        # block_l and _w offset so blocks don't run off edge of image\n",
    "        rand_x = int(np.random.rand() * (IMG_X - 2 * block_l)) + block_l\n",
    "        rand_y = int(np.random.rand() * (IMG_Y - 2 * block_w)) + block_w\n",
    "        orient = int(np.random.rand() * 180)  # .random() is range [0.0, 1.0).\n",
    "        orient = math.radians(orient)  # math.cos takes radians!\n",
    "\n",
    "        # switch to degrees so that we normalize between the x,y,orient for MSELoss\n",
    "        true_coords.append(np.array((rand_x, rand_y, math.degrees(orient))))\n",
    "\n",
    "        rect_vertices = makeRectangle(\n",
    "            block_l, block_w, orient, offset=(rand_x, rand_y))\n",
    "\n",
    "        idraw = ImageDraw.Draw(img)\n",
    "        idraw.polygon(rect_vertices, fill=\"white\")\n",
    "\n",
    "        img.save(newpath + \"/rect\" + str(i) + \".png\")\n",
    "    return true_coords\n",
    "\n",
    "\n",
    "# NOTE Define size of dataset\n",
    "train_truth = make_dataset(\"data\", 5000)\n",
    "# print(len(train_truth))\n",
    "test_truth = make_dataset(\"./data/test\", 300)\n",
    "\n",
    "np.save(\"train_truth.npy\", train_truth)\n",
    "np.save(\"test_truth.npy\", test_truth)\n",
    "\n",
    "\n",
    "train_truth = np.load(\"train_truth.npy\")\n",
    "test_truth = np.load(\"test_truth.npy\")  # loading the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Dataloader -------------------------------------------------------\n",
    "class RectDepthImgsDataset(Dataset):\n",
    "    \"\"\"Artificially generated depth images dataset\"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, coords, transform=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.true_coords = coords\n",
    "        self.transform = transform\n",
    "\n",
    "        STEPSIZE = 50  # todo make this input arguments\n",
    "        WINDOWSIZE = (100, 100)\n",
    "        MARGIN_PX = 5 \n",
    "        self.step = STEPSIZE\n",
    "        self.cropSize = WINDOWSIZE\n",
    "        self.detectMargin = MARGIN_PX\n",
    "\n",
    "    def __len__(self):\n",
    "        # print('true coord len', len(self.true_coords))\n",
    "        return len(self.true_coords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # image = self.images[idx]\n",
    "        image = io.imread(self.img_dir + \"/rect\" +\n",
    "                          str(idx) + \".png\", as_gray=True)\n",
    "        # image = torch.FloatTensor(image).permute(2, 0, 1)  # PIL and torch expect difft orders\n",
    "        coords = torch.FloatTensor(self.true_coords[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        crops, labels, cropCoords = self.makeCrops(\n",
    "            image, self.step, self.cropSize, coords, self.detectMargin)\n",
    "\n",
    "        sample = image, torch.FloatTensor(\n",
    "            labels), torch.FloatTensor(cropCoords)\n",
    "        return sample\n",
    "\n",
    "    def makeCrops(self, image, stepSize, windowSize, rectCenter, detectMargin):\n",
    "        \"\"\"\n",
    "        Returns image crops, as well as T/F for those crops\n",
    "        \"\"\"\n",
    "        crops = []\n",
    "        c_x, c_y, theta = rectCenter\n",
    "        margin = detectMargin\n",
    "        hasRects = []\n",
    "        rectCoords = []\n",
    "        for y in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "            for x in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "                end_x, end_y = x + windowSize[1], y + windowSize[0]\n",
    "                hasRect = (x + margin < c_x < end_x - margin) and (\n",
    "                    y + margin < c_y < end_y - margin)\n",
    "                crops.append(image[y:end_y, x:end_x])\n",
    "                hasRects.append(hasRect)\n",
    "                if hasRect:\n",
    "                    rectCoords.append((c_x, c_y, theta))\n",
    "                else:\n",
    "                    # NOTE: Return empty label, when not hasRect\n",
    "                    rectCoords.append((0, 0, 0))\n",
    "        # print('length of truths in makeCrops', len(truths))\n",
    "        return crops, hasRects, rectCoords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class regrNet(nn.Module):\n",
    "    def __init__(self, cropSize, numOutputs):\n",
    "        \"\"\"\n",
    "        We need the image width and height to determine CNN layer sizes\n",
    "        \"\"\"\n",
    "        super(regrNet, self).__init__()\n",
    "        _pool = 2\n",
    "        _stride = 5\n",
    "        _outputlayers = 16\n",
    "\n",
    "        STEPSIZE = 50  # todo make this input arguments\n",
    "        self.step = STEPSIZE\n",
    "        self.cropSize = cropSize\n",
    "        self.numOutputs = numOutputs\n",
    "\n",
    "        def _calc(val):  # use to calculate layer sizes\n",
    "            layer_size = (val - (_stride - 1)) / _pool\n",
    "            return layer_size\n",
    "\n",
    "        self._const = _calc(_calc(self.cropSize[0]))\n",
    "        self._const *= _calc(_calc(self.cropSize[1]))\n",
    "        self._const *= _outputlayers\n",
    "        self._const = int(self._const)\n",
    "\n",
    "        # --- LOCATION OF RECTANGLE\n",
    "        # NOTE: only one channel for now (black/white)\n",
    "        self.conv1 = nn.Conv2d(1, 6, _stride).to(device)\n",
    "        self.pool = nn.MaxPool2d(_pool, _pool).to(device)\n",
    "        self.conv2 = nn.Conv2d(6, _outputlayers, _stride).to(device)\n",
    "        self.fc1 = nn.Linear(self._const, 120).to(device)\n",
    "        self.fc2 = nn.Linear(120, 84).to(device)\n",
    "        self.fc3 = nn.Linear(84, self.numOutputs).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, crops):\n",
    "        \"\"\"\n",
    "        Forward propogation\n",
    "        : param image: images, a tensor of dimensions(N, 3, IMG_X, IMG_Y)\n",
    "        : return: (x, y, theta) and T/F for each window\n",
    "        \"\"\"\n",
    "        # TODO: presumably by doing this i lose some of the multithread goodness\n",
    "        crops = crops.to(device)\n",
    "\n",
    "        # LOCALIZATION\n",
    "        regr_crops = self.pool(F.relu((self.conv1(crops))))\n",
    "        regr_crops = self.pool(F.relu(self.conv2(regr_crops)))\n",
    "        regr_crops = regr_crops.view(-1, self._const)\n",
    "        regr_crops = F.relu(self.fc1(regr_crops))\n",
    "        regr_crops = F.relu(self.fc2(regr_crops))\n",
    "        regr_crops = self.fc3(regr_crops)\n",
    "\n",
    "        objCoords = regr_crops\n",
    "        # reshape to batchsize x number of crops x 3\n",
    "        #objCoords = objCoords.reshape(-1, self.numCrops, self.numOutputs)\n",
    "        return objCoords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifNet(nn.Module):  # CIFAR is 32x32x3, MNIST is 28x28x1)\n",
    "    def __init__(self, IMG_X, IMG_Y):\n",
    "        \"\"\"\n",
    "        We need the image width and height to determine CNN layer sizes\n",
    "        \"\"\"\n",
    "        super(classifNet, self).__init__()\n",
    "        self._imgx = IMG_X\n",
    "        self._imgy = IMG_Y\n",
    "        _pool = 2\n",
    "        _stride = 5\n",
    "        _outputlayers = 16\n",
    "\n",
    "        STEPSIZE = 50  # todo make this input arguments\n",
    "        WINDOWSIZE = (100, 100)\n",
    "        self.step = STEPSIZE\n",
    "        self.cropSize = WINDOWSIZE\n",
    "        self.numCrops = 0\n",
    "        # T/F for now\n",
    "\n",
    "        # calculate number of crops\n",
    "        for x in range(0, IMG_Y - WINDOWSIZE[0] + 1, STEPSIZE):\n",
    "            for y in range(0, IMG_X - WINDOWSIZE[1] + 1, STEPSIZE):\n",
    "                self.numCrops += 1\n",
    "\n",
    "        def _calc(val):  # use to calculate layer sizes\n",
    "            layer_size = (val - (_stride - 1)) / _pool\n",
    "            return layer_size\n",
    "\n",
    "        # print(self._imgx)\n",
    "        # self._const = _calc(_calc(self._imgx))\n",
    "        # self._const *= _calc(_calc(self._imgy))\n",
    "        self._const = _calc(_calc(self.cropSize[0]))\n",
    "        self._const *= _calc(_calc(self.cropSize[1]))\n",
    "        self._const *= _outputlayers\n",
    "        self._const = int(self._const)\n",
    "\n",
    "        # --- CLASSIFICATION OF WINDOWS\n",
    "        # batch, 3 input image channels (RGB), 6 output channels, 5x5 square convolution\n",
    "        # NOTE: we switched to 1 input channel\n",
    "        self.conv1 = nn.Conv2d(self.numCrops, 6, _stride).to(device)\n",
    "        self.pool = nn.MaxPool2d(_pool, _pool).to(device)\n",
    "        self.conv2 = nn.Conv2d(6, _outputlayers, _stride).to(device)\n",
    "        self.fc1 = nn.Linear(self._const, 120).to(device)\n",
    "        self.fc2 = nn.Linear(120, 84).to(device)\n",
    "        self.fc3 = nn.Linear(84, self.numCrops).to(device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # TODO: batch normalization  self.bn = nn.BatchNorm2d()\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propogation\n",
    "        : param image: images, a tensor of dimensions(N, 3, IMG_X, IMG_Y)\n",
    "        : return: (x, y, theta) and T/F for each window\n",
    "        \"\"\"\n",
    "        x = x.to(device)\n",
    "        batch_images = x\n",
    "\n",
    "        all_crops = []\n",
    "        for img in batch_images:\n",
    "            crops = self.makeCrops(img, self.step, self.cropSize)\n",
    "            all_crops.append(crops)\n",
    "            # all_crops.append(crops)\n",
    "        all_crops = torch.stack(all_crops)\n",
    "        feats = all_crops.view(-1, self.numCrops, self.cropSize[0],\n",
    "                               self.cropSize[1]).to(device)\n",
    "\n",
    "        # CLASSIFICATION of the windows\n",
    "        c_crops = self.pool(F.relu((self.conv1(feats))))\n",
    "        c_crops = self.pool(F.relu(self.conv2(c_crops)))\n",
    "        c_crops = c_crops.view(-1, self._const)\n",
    "        c_crops = F.relu(self.fc1(c_crops))\n",
    "        c_crops = F.relu(self.fc2(c_crops))\n",
    "        c_crops = self.fc3(c_crops)\n",
    "        c_crops = self.sigmoid(c_crops)\n",
    "\n",
    "        containsObj = c_crops\n",
    "        return containsObj, all_crops\n",
    "    \n",
    "    def makeCrops(self, image, stepSize, windowSize):\n",
    "        \"\"\"\n",
    "        Returns a generator of cropped boxes(the top left x, y, the image data)\n",
    "        \"\"\"\n",
    "        image = image.type(torch.FloatTensor).to(device)\n",
    "        crops = []\n",
    "\n",
    "        # TODO: look into ordering, why it's y,x !\n",
    "        for y in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "            for x in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "                end_x, end_y = x + windowSize[1], y + windowSize[0]\n",
    "                # print('This is the x and y used: ', x, '; ', y)\n",
    "                crops.append(image[y:end_y, x:end_x])\n",
    "        crops = torch.stack(crops)\n",
    "        # self.numCrops=len(crops)\n",
    "        return crops\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Utility fxn -------------------------------------------------------\n",
    "# Source: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/train.py\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Keeps track of most recent, average, sum, and count of a metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "    epoch, epochs_since_improvement, model1, model2, optimizer1, optimizer2, loss, loss2, best_loss, is_best\n",
    "):\n",
    "    \"\"\"\n",
    "    Save model checkpoint.\n",
    "    :param epoch: epoch number\n",
    "    :param epochs_since_improvement: number of epochs since last improvement\n",
    "    :param model: model\n",
    "    :param optimizer: optimizer\n",
    "    :param loss: validation loss in this epoch\n",
    "    :param best_loss: best validation loss achieved so far (not necessarily in this checkpoint)\n",
    "    :param is_best: is this checkpoint the best so far?\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"epochs_since_improvement\": epochs_since_improvement,\n",
    "        \"loss\": loss,\n",
    "        \"loss2\": loss2,\n",
    "        \"best_loss\": best_loss,\n",
    "        \"model1\": model1,\n",
    "        \"model2\": model2,\n",
    "        \"optimizer1\": optimizer1,\n",
    "        \"optimizer2\": optimizer2,\n",
    "    }\n",
    "    filename = \"checkpoint_v2sliding.pth.tar\"\n",
    "    torch.save(state, filename)\n",
    "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
    "    if is_best:\n",
    "        torch.save(state, \"BEST_\" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(train_loader, c_model, r_model, classifCriterion,\n",
    "          regrCriterion, optimizer1, optimizer2, epoch):\n",
    "    \"\"\"\n",
    "    One epoch's training.\n",
    "    : param train_loader: DataLoader for training data\n",
    "    : param model: model\n",
    "    : param criterion: for classification (crop contains an Obj, t/f)\n",
    "    : param criterion: for regresion (of the x,y, theta)\n",
    "    : param optimizer: optimizer\n",
    "    : param epoch: epoch number\n",
    "    \"\"\"\n",
    "\n",
    "    c_model.train()  # training mode enables dropout\n",
    "    r_model.train()  # training mode enables dropout\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
    "    data_time = AverageMeter()  # data loading time\n",
    "    losses = AverageMeter()  # loss\n",
    "    losses2 = AverageMeter()  # loss\n",
    "    start = time.time()\n",
    "\n",
    "    for i_batch, (images, labels, coords) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        coords = coords.to(device)\n",
    "\n",
    "        # CLASSIFICATION\n",
    "        # Forward pass\n",
    "        # predicted_class, predicted_locs = model(images)\n",
    "        predicted_class, all_crops = c_model(images)\n",
    "\n",
    "        # print('size of predicted vs labels',\n",
    "        # predicted_class, labels)\n",
    "        # print('size of predicted vs labels',\n",
    "        # predicted_class.size(), labels.size())\n",
    "        loss1 = classifCriterion(predicted_class, labels)\n",
    "        optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "        # Update model\n",
    "        optimizer1.step()\n",
    "\n",
    "\n",
    "        # REGRESSION \n",
    "        # Forward pass\n",
    "        for i in range(9):\n",
    "            batchcrop = all_crops[:, i, :, :]\n",
    "            offset = cropCoords[i]\n",
    "            center_truth = coords[:, i, :]\n",
    "\n",
    "            center_est = r_model(batchcrop)\n",
    "            center_est = center_est + offset\n",
    "\n",
    "            loss2 = regrCriterion(center_truth, center_est)\n",
    "            optimizer2.zero_grad()\n",
    "            loss2.backward()\n",
    "            optimizer2.step()\n",
    "\n",
    "\n",
    "        #losses.update(loss1.item() + loss2.item())\n",
    "        losses.update(loss1.item())\n",
    "        losses2.update(loss2.item())\n",
    "        batch_time.update(time.time() - start)\n",
    "        start = time.time()\n",
    "\n",
    "        # Print status\n",
    "        if i_batch % print_freq == 0:\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}]\\t\"\n",
    "                \"Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(\n",
    "                    epoch, i_batch, len(train_loader), batch_time=batch_time,\n",
    "                    loss=losses,\n",
    "                ),\n",
    "                \"RLoss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(\n",
    "                    epoch, i_batch, len(train_loader), batch_time=batch_time,\n",
    "                    loss=losses2,\n",
    "                ),\n",
    "                \n",
    "            )\n",
    "        # free some memory since their histories may be stored\n",
    "        del predicted_class, predicted_coords, images, labels, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, c_model, r_model, c_criterion, r_criterion):\n",
    "    \"\"\"\n",
    "    One epoch's validation.\n",
    "    : param val_loader: DataLoader for validation data\n",
    "    : param model: model\n",
    "    : param criterion: MultiBox loss\n",
    "    : return: average validation loss\n",
    "    \"\"\"\n",
    "    c_model.eval()  # eval mode disables dropout\n",
    "    r_model.eval()  # eval mode disables dropout\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses2 = AverageMeter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Prohibit gradient computation explicity because I had some problems with memory\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for i_batch, (images, labels, coords) in enumerate(train_loader):\n",
    "            # Move to default device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            coords = coords.to(device)\n",
    "\n",
    "            # CLASSIFICATION Eval\n",
    "            predicted_class, all_crops = c_model(images)\n",
    "            loss1 = c_criterion(predicted_class, labels)\n",
    "\n",
    "            # REGRESSION Eval\n",
    "            for i in range(9):\n",
    "                batchcrop = all_crops[:, i, :, :]\n",
    "                offset = cropCoords[i]\n",
    "                center_truth = coords[:, i, :]\n",
    "\n",
    "                center_est = r_model(batchcrop)\n",
    "                center_est = center_est + offset\n",
    "\n",
    "                loss2 = regrCriterion(center_truth, center_est)\n",
    "                optimizer2.zero_grad()\n",
    "                loss2.backward()\n",
    "                optimizer2.step()\n",
    "\n",
    "            # For validation, dn't mask\n",
    "            masked_est = predicted_coords\n",
    "            masked_truth = coords\n",
    "\n",
    "            loss2 = r_criterion(masked_est, masked_truth)\n",
    "\n",
    "            #losses.update(loss1.item() + loss2.item(), images.size(0))\n",
    "            losses.update(loss1.item())\n",
    "            losses2.update(loss2.item())\n",
    "\n",
    "            batch_time.update(time.time() - start)\n",
    "            start = time.time()\n",
    "            \n",
    "            # Print status\n",
    "            if i_batch % print_freq == 0:\n",
    "                print(\n",
    "                    \"[{0}/{1}]\\t\"\n",
    "                    \"Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                    \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(i_batch, len(val_loader), \n",
    "                                                                    batch_time=batch_time, loss=losses),\n",
    "                    \"Regr Loss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(i_batch, len(val_loader), \n",
    "                                                                    batch_time=batch_time, loss=losses2)\n",
    "                )\n",
    "\n",
    "    print(\"\\n * LOSS - {loss.avg:.3f}\\n\".format(loss=losses))\n",
    "    print(\" * REGR LOSS - {loss.avg:.3f}\\n\".format(loss=losses2))\n",
    "\n",
    "    return losses.avg, losses2.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available? device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# -- Load data -------------------------------------------------------\n",
    "batch_size = 32 \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA available? device: \", device)\n",
    "\n",
    "\n",
    "# Dataset is depth images of rectangular blocks\n",
    "train_dataset = RectDepthImgsDataset(img_dir=\"./data\", coords=train_truth)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = RectDepthImgsDataset(img_dir=\"./data/test\", coords=test_truth)\n",
    "\n",
    "# Data loader\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Hyperparamaters -------------------------\n",
    "\n",
    "num_epochs = 100  # number of epochs to run without early-stopping\n",
    "learning_rate = 0.001\n",
    "\n",
    "start_epoch = 0  # start at this epoch\n",
    "# number of epochs since there was an improvement in the validation metric\n",
    "epochs_since_improvement = 0\n",
    "best_loss = 1000.0  # assume a high loss at first\n",
    "workers = 4  # number of workers for loading data in the DataLoader\n",
    "\n",
    "classifModel = classifNet(IMG_X, IMG_Y)\n",
    "classifModel = classifModel.to(device)\n",
    "\n",
    "regrModel = regrNet((100, 100), 3)  # crop size in pixels; output x,y, theta\n",
    "regrModel = regrModel.to(device)\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "classifCriterion = nn.BCELoss()\n",
    "regrCriterion = nn.MSELoss()\n",
    "optimizer1 = torch.optim.Adam(classifModel.parameters(), lr=learning_rate)\n",
    "optimizer2 = torch.optim.Adam(regrModel.parameters(), lr=learning_rate)\n",
    "\n",
    "print_freq = 25  # print training or validation status every __ batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Training and validation.\n",
    "    \"\"\"\n",
    "\n",
    "    global epochs_since_improvement, start_epoch, best_loss, epoch, checkpoint\n",
    "\n",
    "    print(\"Training model now...\")\n",
    "\n",
    "    # -- Begin training -------------------------\n",
    "    for epoch in range(num_epochs):\n",
    "        train(\n",
    "            train_loader=train_loader,\n",
    "            c_model=classifModel,\n",
    "            r_model=regrModel,\n",
    "            classifCriterion=classifCriterion,\n",
    "            regrCriterion=regrCriterion,\n",
    "            optimizer1=optimizer1,\n",
    "            optimizer2=optimizer2,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "\n",
    "        # One epoch's validation\n",
    "        val_loss, regr_loss = validate(val_loader=test_loader,\n",
    "                            c_model=classifModel, r_model=regrModel,\n",
    "                            c_criterion=classifCriterion,\n",
    "                            r_criterion=regrCriterion)\n",
    "\n",
    "        # Did validation loss improve?\n",
    "        is_best = val_loss < best_loss\n",
    "        best_loss = min(val_loss, best_loss)\n",
    "\n",
    "        if not is_best:\n",
    "            epochs_since_improvement += 1\n",
    "            print(\"\\nEpochs since last improvement: %d\\n\" %\n",
    "                  (epochs_since_improvement,))\n",
    "\n",
    "        else:\n",
    "            epochs_since_improvement = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(epoch, epochs_since_improvement, classifModel, regrModel, optimizer1,\n",
    "                        optimizer2, val_loss, regr_loss, best_loss, is_best)\n",
    "        #save_checkpoint(epoch, epochs_since_improvement, classifModel, optimizer1,\n",
    "        #                val_loss, best_loss, is_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running main!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model now...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cropCoords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d6f4fa678a0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# alert when training is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msound_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/rui/Downloads/newyear.ogg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msound_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-1c960971bad3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0moptimizer2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-5eff70fbd9b3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, c_model, r_model, classifCriterion, regrCriterion, optimizer1, optimizer2, epoch)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mbatchcrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_crops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcropCoords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mcenter_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cropCoords' is not defined"
     ]
    }
   ],
   "source": [
    "main()\n",
    "\n",
    "# alert when training is done\n",
    "sound_file = '/home/rui/Downloads/newyear.ogg'\n",
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Ready!\n",
      "\n",
      "Loaded checkpoint from epoch 100. Best loss so far is 0.000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('All Ready!')\n",
    "\n",
    "\n",
    "filename = \"checkpoint_v2sliding.pth.tar\"\n",
    "checkpoint = torch.load(filename)\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "best_loss = checkpoint['best_loss']\n",
    "print('\\nLoaded checkpoint from epoch %d. Best loss so far is %.3f.\\n' % (start_epoch, best_loss))\n",
    "classifModel = checkpoint['model1']\n",
    "regrModel = checkpoint['model2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "FOR N =  1\n",
      "tensor([0., 0., 0., 0., 1., 1., 0., 1., 1.])\n",
      "[125.0, 142.0, 179.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(test_loader)\n",
    "images, labels, coords = dataiter.next()\n",
    "\n",
    "n = 1\n",
    "print(\"\\n\\nFOR N = \", n)\n",
    "imgSample = images[n]\n",
    "max_idx = torch.argmax(coords[n][:,0])\n",
    "x, y, theta = coords[n][max_idx]\n",
    "print(labels[n])\n",
    "print([float(zed) for zed in (x,y,theta)])\n",
    "#print(x,y,theta)\n",
    "#crops, __ = makeCrops(imgSample, 50, (100, 100), (x,y, theta))\n",
    "\n",
    "plt.imshow(imgSample)\n",
    "#fig, axess = plt.subplots(3,3, figsize=(10,10))\n",
    "#axess = np.array(axess).flatten()#order='F') #'F' means to flatten in column-major (Fortran- style) order.\n",
    "#for i in range(9):\n",
    "#    axess[i].imshow(crops[i])\n",
    "#    axess[i].set_ylabel('# '+str(i))\n",
    "plt.suptitle('Window')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss across batch size of  32 is:  tensor(0.0013, device='cuda:0') tensor(79.7352, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# -- Check the results -------------------------------------------------------\n",
    "\n",
    "# -- Utility ---------------------------------------------\n",
    "def makeCrops(image, stepSize, windowSize, true_center):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    image = image.type(torch.FloatTensor)\n",
    "    crops = []\n",
    "    truths = []\n",
    "    c_x, c_y, orient = true_center\n",
    "    # TODO: look into otdering, why it's y,x !\n",
    "    margin = 5 \n",
    "    # --> is x, but is the column\n",
    "    # to slide horizontally, y must come first\n",
    "    for y in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "        for x in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "            end_x, end_y = x + windowSize[1], y + windowSize[0]\n",
    "            hasRect = (x + margin < c_x < end_x - margin) and (\n",
    "                y + margin < c_y < end_y - margin\n",
    "            )\n",
    "            truths.append(hasRect)\n",
    "            crops.append(image[y:end_y, x:end_x])\n",
    "    crops = torch.stack(crops)\n",
    "    print(\"shape of crops\", crops.shape)\n",
    "    return crops, truths\n",
    "\n",
    "\n",
    "# -- Get some validation results ---------------------------------------------\n",
    "classifModel.to(device).eval()\n",
    "regrModel.to(device).eval()\n",
    "\n",
    "c_criterion = nn.BCELoss()\n",
    "r_criterion = nn.MSELoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    dataiter = iter(test_loader)\n",
    "    images, labels, coords = dataiter.next()\n",
    "    \n",
    "    # Move to default device\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    coords = coords.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    predicted_class, all_crops = classifModel(images)\n",
    "    # Loss\n",
    "    loss = c_criterion(predicted_class, labels)\n",
    "\n",
    "    \n",
    "    # Forward pass\n",
    "    predicted_coords = regrModel(all_crops)\n",
    "\n",
    "    labels_est = torch.FloatTensor(\n",
    "        predicted_class.detach().cpu().numpy())\n",
    "    mask2 = torch.round(labels_est).type_as(coords)\n",
    "    mask2.unsqueeze_(2)\n",
    "    mask2 = mask2.repeat(1, 1, 3)\n",
    "    masked_est = mask2 * predicted_coords\n",
    "    masked_truth = coords\n",
    "\n",
    "    # Loss\n",
    "    #loss2 = r_criterion(masked_est, masked_truth)\n",
    "    loss2 = r_criterion(predicted_coords, coords)\n",
    "\n",
    "print(\"loss across batch size of \", labels.size()[0], 'is: ', loss, loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of crops torch.Size([9, 100, 100])\n",
      "!-- FOR N =  2\n",
      "y (crops) \n",
      "\t [0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
      "!-- yhat (crops) \n",
      "\t [0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
      "\n",
      "\n",
      "FOR N =  2\n",
      "!-- center y \n",
      "\t [73.0, 136.0, 92.0]\n",
      "!-- center y est \n",
      "\t  [70.31342315673828, 124.98744201660156, 78.8931884765625]\n",
      "tensor([[ -1.0646,   0.5493,  -0.4236],\n",
      "        [ -1.0791,  -0.9135,  -6.0298],\n",
      "        [ -1.4885,   1.0304,   5.1625],\n",
      "        [ 70.3134, 124.9874,  78.8932],\n",
      "        [ 68.5238, 130.0926,  80.7289],\n",
      "        [  1.6906,   2.3211,   2.7990],\n",
      "        [ 67.1859, 125.6823,  84.0848],\n",
      "        [ 70.1525, 129.6729,  90.5853],\n",
      "        [ -0.7017,   0.4389,  -0.7159]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -- Plot windows and print labels -----------------------------------\n",
    "n = 2\n",
    "imgSample = images[n]\n",
    "crops, __ = makeCrops(imgSample, 50, (100, 100), (x,y, theta))\n",
    "# -- Show original image, and the sliding windo crops -------\n",
    "plt.imshow(imgSample)\n",
    "# ---------------------------------------------------\n",
    "foo_label = labels[n]\n",
    "\n",
    "foo_coord = coords[n] \n",
    "predicted_locs = predicted_coords.view(-1,\n",
    "                                     coords.size(1), coords.size(2))\n",
    "foo_coord_est = predicted_locs[n] # 3 per window\n",
    "foo_label_est = predicted_class[n]\n",
    "\n",
    "max_idx = torch.argmax(foo_coord[:,0])\n",
    "x, y, theta = foo_coord[max_idx]\n",
    "est_max_idx = torch.argmax(foo_coord_est[:,0])\n",
    "x_est, y_est, theta_est = foo_coord_est[est_max_idx]\n",
    "\n",
    "# -- Print x,y for one result -------\n",
    "# -- Print window t/f for one result -------\n",
    "print(\"!-- FOR N = \", n)\n",
    "print(\"y (crops) \\n\\t\", [int(l) for l in foo_label])\n",
    "print(\"!-- yhat (crops) \\n\\t\", [int(np.round(p,0)) for p in foo_label_est] )\n",
    "# -------------------------------------------------\n",
    "sns.set(rc={\"figure.figsize\": (8, 6)})\n",
    "\n",
    "print(\"\\n\\nFOR N = \", n)\n",
    "print(\"!-- center y \\n\\t\", [float(zed) for zed in (x, y, theta)])\n",
    "print(\"!-- center y est \\n\\t \", [float(zed) for zed in (x_est, y_est, theta_est)])\n",
    "print(foo_coord_est)\n",
    "\n",
    "fig, axess = plt.subplots(3,3, figsize=(10,10))\n",
    "axess = np.array(axess).flatten()#order='F') #'F' means to flatten in column-major (Fortran- style) order.\n",
    "for i in range(9):\n",
    "    axess[i].imshow(crops[i])\n",
    "    axess[i].set_ylabel('# '+str(i))\n",
    "plt.suptitle('Window')\n",
    "plt.show()\n",
    "\n",
    "#for (i, crop) in enumerate(crops):\n",
    "    # print(\"1-index number of window: \", i+1, 'x', x, 'y', y, 'has rectangle?', hasRect)\n",
    "    #plt.figure()\n",
    "    #plt.suptitle(\"numero: %d\" % (i))\n",
    "    #plt.imshow(crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!-- \n",
      "True locations, defined for each crop\n",
      " tensor([[  0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.],\n",
      "        [ 73., 136.,  92.],\n",
      "        [ 73., 136.,  92.],\n",
      "        [  0.,   0.,   0.],\n",
      "        [ 73., 136.,  92.],\n",
      "        [ 73., 136.,  92.],\n",
      "        [  0.,   0.,   0.]], device='cuda:0')\n",
      "!-- \n",
      "Full predicted locations (3 per crop)\n",
      " tensor([[ -1.0646,   0.5493,  -0.4236],\n",
      "        [ -1.0791,  -0.9135,  -6.0298],\n",
      "        [ -1.4885,   1.0304,   5.1625],\n",
      "        [ 70.3134, 124.9874,  78.8932],\n",
      "        [ 68.5238, 130.0926,  80.7289],\n",
      "        [  1.6906,   2.3211,   2.7990],\n",
      "        [ 67.1859, 125.6823,  84.0848],\n",
      "        [ 70.1525, 129.6729,  90.5853],\n",
      "        [ -0.7017,   0.4389,  -0.7159]], device='cuda:0')\n",
      "tensor([[0, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8],\n",
      "        [1, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 1]])\n",
      "\n",
      "SAMPLE  0\n",
      "!-- y (crops) \n",
      "\t [0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
      "!-- yhat (crops) \n",
      "\t [0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
      "\n",
      "SAMPLE  1\n",
      "!-- y (crops) \n",
      "\t [0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "!-- yhat (crops) \n",
      "\t [0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "\n",
      "SAMPLE  2\n",
      "!-- y (crops) \n",
      "\t [0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
      "!-- yhat (crops) \n",
      "\t [0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
      "\n",
      "SAMPLE  3\n",
      "!-- y (crops) \n",
      "\t [0, 0, 0, 0, 1, 1, 0, 0, 0]\n",
      "!-- yhat (crops) \n",
      "\t [0, 0, 0, 0, 1, 1, 0, 0, 0]\n",
      "\n",
      "SAMPLE  4\n",
      "!-- y (crops) \n",
      "\t [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "!-- yhat (crops) \n",
      "\t [0, 0, 0, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#print(\"!-- yhat \\n\\t\", [int(round(o, 0)) for o in outputs[n].cpu().numpy()])    \n",
    "print(\"!-- \")\n",
    "print('True locations, defined for each crop\\n', foo_coord)\n",
    "print(\"!-- \")\n",
    "print('Full predicted locations (3 per crop)\\n', foo_coord_est)\n",
    "#print(\"\\n ------ x,y center + orient: \", coords[n], \"\\n\"))\n",
    "print(np.argwhere(foo_coord_est > 0) )\n",
    "\n",
    "# -- Print outputs for multiple results -------\n",
    "for ix in range(5):\n",
    "    print('\\nSAMPLE ', ix)\n",
    "    print(\"!-- y (crops) \\n\\t\", [int(l) for l in labels[ix]])\n",
    "    print(\"!-- yhat (crops) \\n\\t\", [int(np.round(p, 0)) for p in predicted_class[ix]] )\n",
    "\n",
    "# -- Main ---------------------------------------------\n",
    "#if __name__ == '__main__':\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
