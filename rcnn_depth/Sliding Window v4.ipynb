{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Sliding Window\n",
    "# Feb 2019\n",
    "#\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import torchvision\n",
    "from IPython.display import display  # to display images\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import io\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running main!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import torchvision\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import io\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "# from IPython.display import Audio\n",
    "# from IPython.display import display  # to display images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataset -------------------------------------------------------\n",
    "IMG_X, IMG_Y = 200, 200\n",
    "# length and width of blocks (fixed for now)\n",
    "block_l, block_w = 20, 30\n",
    "\n",
    "\n",
    "def makeRectangle(l, w, theta, offset=(0, 0)):\n",
    "    c, s = math.cos(theta), math.sin(theta)\n",
    "    rectCoords = [(l / 2.0, w / 2.0), (l / 2.0, -w / 2.0),\n",
    "                  (-l / 2.0, -w / 2.0), (-l / 2.0, w / 2.0), ]\n",
    "    return [(c * x - s * y + offset[0],\n",
    "             s * x + c * y + offset[1]) for (x, y) in rectCoords]\n",
    "\n",
    "\n",
    "# ---- Make depth images ---\n",
    "def make_dataset(dirname, num_images):\n",
    "    true_coords = []\n",
    "    newpath = \"./\" + dirname\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "        print(newpath)\n",
    "    for i in range(num_images):\n",
    "        # orient = 0 # degrees\n",
    "        img = Image.new(\"RGB\", (IMG_X, IMG_Y), \"black\")\n",
    "\n",
    "        # block_l and _w offset so blocks don't run off edge of image\n",
    "        rand_x = int(np.random.rand() * (IMG_X - 2 * block_l)) + block_l\n",
    "        rand_y = int(np.random.rand() * (IMG_Y - 2 * block_w)) + block_w\n",
    "        orient = int(np.random.rand() * 180)  # .random() is range [0.0, 1.0).\n",
    "        orient = math.radians(orient)  # math.cos takes radians!\n",
    "\n",
    "        # switch to degrees so that we normalize between the x,y,orient for MSELoss\n",
    "        true_coords.append(np.array((rand_x, rand_y, orient)))\n",
    "\n",
    "        rect_vertices = makeRectangle(\n",
    "            block_l, block_w, orient, offset=(rand_x, rand_y))\n",
    "\n",
    "        idraw = ImageDraw.Draw(img)\n",
    "        idraw.polygon(rect_vertices, fill=\"white\")\n",
    "\n",
    "        img.save(newpath + \"/rect\" + str(i) + \".png\")\n",
    "    return true_coords\n",
    "\n",
    "\n",
    "# NOTE Define size of dataset\n",
    "train_truth = make_dataset(\"data\", 5000)\n",
    "# print(len(train_truth))\n",
    "test_truth = make_dataset(\"./data/test\", 300)\n",
    "\n",
    "np.save(\"train_truth.npy\", train_truth)\n",
    "np.save(\"test_truth.npy\", test_truth)\n",
    "\n",
    "\n",
    "train_truth = np.load(\"train_truth.npy\")\n",
    "test_truth = np.load(\"test_truth.npy\")  # loading the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataloader -------------------------------------------------------\n",
    "class RectDepthImgsDataset(Dataset):\n",
    "    \"\"\"Artificially generated depth images dataset\"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, coords, transform=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.true_coords = coords\n",
    "        self.transform = transform\n",
    "\n",
    "        STEPSIZE = 50  # todo make this input arguments\n",
    "        WINDOWSIZE = (100, 100)\n",
    "        MARGIN_PX = 10 \n",
    "        self.step = STEPSIZE\n",
    "        self.cropSize = WINDOWSIZE\n",
    "        self.detectMargin = MARGIN_PX\n",
    "\n",
    "    def __len__(self):\n",
    "        # print('true coord len', len(self.true_coords))\n",
    "        return len(self.true_coords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # image = self.images[idx]\n",
    "        image = io.imread(self.img_dir + \"/rect\" +\n",
    "                          str(idx) + \".png\", as_gray=True)\n",
    "        # image = torch.FloatTensor(image).permute(2, 0, 1)  # PIL and torch expect difft orders\n",
    "        coords = torch.FloatTensor(self.true_coords[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        crops, labels, cropCoords = self.makeCrops(\n",
    "            image, self.step, self.cropSize, coords, self.detectMargin)\n",
    "\n",
    "        sample = image, torch.FloatTensor(\n",
    "            labels), torch.FloatTensor(cropCoords)\n",
    "        return sample\n",
    "\n",
    "    def makeCrops(self, image, stepSize, windowSize, rectCenter, detectMargin):\n",
    "        \"\"\"\n",
    "        Returns image crops, as well as T/F for those crops\n",
    "        \"\"\"\n",
    "        crops = []\n",
    "        c_x, c_y, theta = rectCenter\n",
    "        margin = detectMargin\n",
    "        hasRects = []\n",
    "        rectCoords = []\n",
    "        for y in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "            for x in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "                end_x, end_y = x + windowSize[1], y + windowSize[0]\n",
    "                hasRect = (x + margin < c_x < end_x - margin) and (\n",
    "                    y + margin < c_y < end_y - margin)\n",
    "                crops.append(image[y:end_y, x:end_x])\n",
    "                hasRects.append(hasRect)\n",
    "                if hasRect:\n",
    "                    rectCoords.append((c_x, c_y, theta))\n",
    "                else:\n",
    "                    # NOTE: Return empty label, when not hasRect\n",
    "                    rectCoords.append((0, 0, 0))\n",
    "        # print('length of truths in makeCrops', len(truths))\n",
    "        return crops, hasRects, rectCoords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  --- Define Nets -------------------------------------------------------\n",
    "\n",
    "class regrNet(nn.Module):\n",
    "    def __init__(self, cropSize, numOutputs):\n",
    "        \"\"\"\n",
    "        We need the image width and height to determine CNN layer sizes\n",
    "        \"\"\"\n",
    "        super(regrNet, self).__init__()\n",
    "        _pool = 2\n",
    "        _stride = 5\n",
    "        _outputlayers = 16\n",
    "\n",
    "        STEPSIZE = 50  # todo make this input arguments\n",
    "        self.step = STEPSIZE\n",
    "        self.cropSize = cropSize\n",
    "        self.numOutputs = numOutputs\n",
    "\n",
    "        def _calc(val):  # use to calculate layer sizes\n",
    "            layer_size = (val - (_stride - 1)) / _pool\n",
    "            return layer_size\n",
    "\n",
    "        self._const = _calc(_calc(self.cropSize[0]))\n",
    "        self._const *= _calc(_calc(self.cropSize[1]))\n",
    "        self._const *= _outputlayers\n",
    "        self._const = int(self._const)\n",
    "\n",
    "        # --- LOCATION OF RECTANGLE\n",
    "        # NOTE: only one channel for now (black/white)\n",
    "        self.conv1 = nn.Conv2d(1, 6, _stride).to(device)\n",
    "        self.pool = nn.MaxPool2d(_pool, _pool).to(device)\n",
    "        self.conv2 = nn.Conv2d(6, _outputlayers, _stride).to(device)\n",
    "        self.fc1 = nn.Linear(self._const, 120).to(device)\n",
    "        self.fc2 = nn.Linear(120, 84).to(device)\n",
    "        self.fc3 = nn.Linear(84, self.numOutputs).to(device)\n",
    "\n",
    "    def forward(self, crops):\n",
    "        \"\"\"\n",
    "        Forward propogation\n",
    "        : param image: images, a tensor of dimensions(N, 3, IMG_X, IMG_Y)\n",
    "        : return: (x, y, theta) and T/F for each window\n",
    "        \"\"\"\n",
    "        # TODO: presumably by doing this i lose some of the multithread goodness\n",
    "        crops = crops.to(device)\n",
    "\n",
    "        # LOCALIZATION\n",
    "        regr_crops = self.pool(F.relu((self.conv1(crops))))\n",
    "        regr_crops = self.pool(F.relu(self.conv2(regr_crops)))\n",
    "        regr_crops = regr_crops.view(-1, self._const)\n",
    "        regr_crops = F.relu(self.fc1(regr_crops))\n",
    "        regr_crops = F.relu(self.fc2(regr_crops))\n",
    "        regr_crops = self.fc3(regr_crops)\n",
    "\n",
    "        objCoords = regr_crops\n",
    "        # reshape to batchsize x number of crops x 3\n",
    "        objCoords = objCoords.reshape(-1, self.numOutputs)\n",
    "        return objCoords\n",
    "\n",
    "\n",
    "class classifNet(nn.Module):  # CIFAR is 32x32x3, MNIST is 28x28x1)\n",
    "    def __init__(self, IMG_X, IMG_Y):\n",
    "        \"\"\"\n",
    "        We need the image width and height to determine CNN layer sizes\n",
    "        \"\"\"\n",
    "        super(classifNet, self).__init__()\n",
    "        self._imgx = IMG_X\n",
    "        self._imgy = IMG_Y\n",
    "        _pool = 2\n",
    "        _stride = 5\n",
    "        _outputlayers = 16\n",
    "\n",
    "        STEPSIZE = 50  # todo make this input arguments\n",
    "        WINDOWSIZE = (100, 100)\n",
    "        self.step = STEPSIZE\n",
    "        self.cropSize = WINDOWSIZE\n",
    "        self.numCrops = 0\n",
    "        # T/F for now\n",
    "\n",
    "        # calculate number of crops\n",
    "        for x in range(0, IMG_Y - WINDOWSIZE[0] + 1, STEPSIZE):\n",
    "            for y in range(0, IMG_X - WINDOWSIZE[1] + 1, STEPSIZE):\n",
    "                self.numCrops += 1\n",
    "\n",
    "        def _calc(val):  # use to calculate layer sizes\n",
    "            layer_size = (val - (_stride - 1)) / _pool\n",
    "            return layer_size\n",
    "\n",
    "        # print(self._imgx)\n",
    "        # self._const = _calc(_calc(self._imgx))\n",
    "        # self._const *= _calc(_calc(self._imgy))\n",
    "        self._const = _calc(_calc(self.cropSize[0]))\n",
    "        self._const *= _calc(_calc(self.cropSize[1]))\n",
    "        self._const *= _outputlayers\n",
    "        self._const = int(self._const)\n",
    "\n",
    "        # --- CLASSIFICATION OF WINDOWS\n",
    "        # batch, 3 input image channels (RGB), 6 output channels, 5x5 square convolution\n",
    "        # NOTE: we switched to 1 input channel\n",
    "        self.conv1 = nn.Conv2d(self.numCrops, 6, _stride).to(device)\n",
    "        self.pool = nn.MaxPool2d(_pool, _pool).to(device)\n",
    "        self.conv2 = nn.Conv2d(6, _outputlayers, _stride).to(device)\n",
    "        self.fc1 = nn.Linear(self._const, 120).to(device)\n",
    "        self.fc2 = nn.Linear(120, 84).to(device)\n",
    "        self.fc3 = nn.Linear(84, self.numCrops).to(device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # TODO: batch normalization  self.bn = nn.BatchNorm2d()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propogation\n",
    "        : param image: images, a tensor of dimensions(N, 3, IMG_X, IMG_Y)\n",
    "        : return: (x, y, theta) and T/F for each window\n",
    "        \"\"\"\n",
    "        x = x.to(device)\n",
    "        batch_images = x\n",
    "\n",
    "        all_crops = []\n",
    "        for img in batch_images:\n",
    "            crops, cropCoords = self.makeCrops(img, self.step, self.cropSize)\n",
    "            all_crops.append(crops)\n",
    "        all_crops = torch.stack(all_crops)\n",
    "        feats = all_crops.view(-1, self.numCrops, self.cropSize[0],\n",
    "                               self.cropSize[1]).to(device)\n",
    "\n",
    "        # CLASSIFICATION of the windows\n",
    "        c_crops = self.pool(F.relu((self.conv1(feats))))\n",
    "        c_crops = self.pool(F.relu(self.conv2(c_crops)))\n",
    "        c_crops = c_crops.view(-1, self._const)\n",
    "        c_crops = F.relu(self.fc1(c_crops))\n",
    "        c_crops = F.relu(self.fc2(c_crops))\n",
    "        c_crops = self.fc3(c_crops)\n",
    "        c_crops = self.sigmoid(c_crops)\n",
    "\n",
    "        containsObj = c_crops\n",
    "        return containsObj, all_crops, cropCoords\n",
    "\n",
    "    def makeCrops(self, image, stepSize, windowSize):\n",
    "        \"\"\"\n",
    "        Returns a generator of cropped boxes(the top left x, y, the image data)\n",
    "        \"\"\"\n",
    "        image = image.type(torch.FloatTensor).to(device)\n",
    "        crops = []\n",
    "        cropCoords = []\n",
    "\n",
    "        # TODO: look into ordering, why it's y,x !\n",
    "        for y in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "            for x in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "                end_x, end_y = x + windowSize[1], y + windowSize[0]\n",
    "                # print('This is the x and y used: ', x, '; ', y)\n",
    "                crops.append(image[y:end_y, x:end_x])\n",
    "                cropCoords.append(torch.FloatTensor((y, x)))\n",
    "        crops = torch.stack(crops)\n",
    "        cropCoords = torch.stack(cropCoords)\n",
    "        # self.numCrops=len(crops)\n",
    "        return crops, cropCoords\n",
    "\n",
    "\n",
    "# In[6]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Utility fxn -------------------------------------------------------\n",
    "# Source: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/train.py\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Keeps track of most recent, average, sum, and count of a metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "    epoch, epochs_since_improvement, model1, model2, optimizer1, optimizer2, loss, loss2, best_loss, is_best\n",
    "):\n",
    "    \"\"\"\n",
    "    Save model checkpoint.\n",
    "    :param epoch: epoch number\n",
    "    :param epochs_since_improvement: number of epochs since last improvement\n",
    "    :param model: model\n",
    "    :param optimizer: optimizer\n",
    "    :param loss: validation loss in this epoch\n",
    "    :param best_loss: best validation loss achieved so far (not necessarily in this checkpoint)\n",
    "    :param is_best: is this checkpoint the best so far?\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"epochs_since_improvement\": epochs_since_improvement,\n",
    "        \"loss\": loss,\n",
    "        \"loss2\": loss2,\n",
    "        \"best_loss\": best_loss,\n",
    "        \"model1\": model1,\n",
    "        \"model2\": model2,\n",
    "        \"optimizer1\": optimizer1,\n",
    "        \"optimizer2\": optimizer2,\n",
    "    }\n",
    "    filename = \"checkpoint_v2sliding.pth.tar\"\n",
    "    torch.save(state, filename)\n",
    "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
    "    if is_best:\n",
    "        torch.save(state, \"BEST_\" + filename)\n",
    "\n",
    "        \n",
    "\n",
    "def save_checkpoint_small(\n",
    "    epoch, model1, model2, optimizer1, optimizer2):\n",
    "    \"\"\"\n",
    "    Save model checkpoint.\n",
    "    :param epoch: epoch number\n",
    "    :param epochs_since_improvement: number of epochs since last improvement\n",
    "    :param model: model\n",
    "    :param optimizer: optimizer\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"epochs_since_improvement\": epochs_since_improvement,\n",
    "        \"model1\": model1,\n",
    "        \"model2\": model2,\n",
    "        \"optimizer1\": optimizer1,\n",
    "        \"optimizer2\": optimizer2,\n",
    "    }\n",
    "    filename = \"checkpoint_v2sliding.pth.tar\"\n",
    "    torch.save(state, filename)\n",
    "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
    "    if is_best:\n",
    "        torch.save(state, \"BEST_\" + filename)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  --- Define Train and Test functions -------------------------------------------------------\n",
    "\n",
    "def train(train_loader, c_model, r_model, classifCriterion,\n",
    "          regrCriterion, optimizer1, optimizer2, epoch):\n",
    "    \"\"\"\n",
    "    One epoch's training.\n",
    "    : param train_loader: DataLoader for training data\n",
    "    : param model: model\n",
    "    : param criterion: for classification (crop contains an Obj, t/f)\n",
    "    : param criterion: for regresion (of the x,y, theta)\n",
    "    : param optimizer: optimizer\n",
    "    : param epoch: epoch number\n",
    "    \"\"\"\n",
    "\n",
    "    c_model.train()  # training mode enables dropout\n",
    "    r_model.train()  # training mode enables dropout\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
    "    data_time = AverageMeter()  # data loading time\n",
    "    losses = AverageMeter()  # loss\n",
    "    losses2 = AverageMeter()  # loss\n",
    "    start = time.time()\n",
    "\n",
    "    for i_batch, (images, labels, coords) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        coords = coords.to(device)\n",
    "\n",
    "        # CLASSIFICATION\n",
    "        # Forward pass\n",
    "        # predicted_class, predicted_locs = model(images)\n",
    "        predicted_class, all_crops, cropCoords = c_model(images)\n",
    "        all_crops = all_crops.to(device)\n",
    "        cropCoords = cropCoords.to(device)\n",
    "\n",
    "        # print('size of predicted vs labels',\n",
    "        # predicted_class, labels)\n",
    "        # print('size of predicted vs labels',\n",
    "        # predicted_class.size(), labels.size())\n",
    "        loss1 = classifCriterion(predicted_class, labels)\n",
    "        optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "        # Update model\n",
    "        optimizer1.step()\n",
    "\n",
    "        # REGRESSION\n",
    "        # Forward pass\n",
    "        # all crops is of size (batchsize, numcrops, x, y)\n",
    "        # we'll do it batchsize x 1 crop at a time...\n",
    "        # coords = batchsize, numcrops, x,y, theta\n",
    "        for i in range(9):\n",
    "            # print('!-- ', all_crops.size())\n",
    "            batchcrop = all_crops[:, i, :, :]\n",
    "            batchcrop.unsqueeze_(1)\n",
    "            # print('!-- ', batchcrop.size())\n",
    "\n",
    "            # print('!--', cropCoords.size())\n",
    "            offset = cropCoords[i]\n",
    "            # pad with column of zeros - don't touch the theta\n",
    "            # print('!--', offset.size())\n",
    "            offset = offset.repeat(all_crops.size(0), 1)\n",
    "            offset = torch.cat((offset, torch.zeros((all_crops.size(0),\n",
    "                                                     1)).to(device)),\n",
    "                               dim=1)\n",
    "            center_truth = coords[:, i, :]\n",
    "\n",
    "            center_est = r_model(batchcrop).to(device)\n",
    "            # print('!-- ', center_est)\n",
    "            # print('!-- ', offset)\n",
    "            center_est = center_est + offset\n",
    "\n",
    "            loss2 = regrCriterion(center_truth, center_est)\n",
    "            \n",
    "            optimizer2.zero_grad()\n",
    "            loss2.backward()\n",
    "            \n",
    "            optimizer2.step()\n",
    "            \n",
    "            losses2.update(loss2.item())\n",
    "\n",
    "        losses.update(loss1.item())\n",
    "        batch_time.update(time.time() - start)\n",
    "        start = time.time()\n",
    "\n",
    "        # Print status\n",
    "        if i_batch % print_freq == 0:\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}]\\t\"\n",
    "                \"Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(\n",
    "                    epoch, i_batch, len(train_loader), batch_time=batch_time,\n",
    "                    loss=losses,\n",
    "                ),\n",
    "                \"RLoss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(\n",
    "                    epoch, i_batch, len(train_loader), batch_time=batch_time,\n",
    "                    loss=losses2,\n",
    "                ),\n",
    "\n",
    "            )\n",
    "        # free some memory since their histories may be stored\n",
    "        del predicted_class, images, labels, coords\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "\n",
    "def validate(val_loader, c_model, r_model, c_criterion, r_criterion):\n",
    "    \"\"\"\n",
    "    One epoch's validation.\n",
    "    : param val_loader: DataLoader for validation data\n",
    "    : param model: model\n",
    "    : param criterion: MultiBox loss\n",
    "    : return: average validation loss\n",
    "    \"\"\"\n",
    "    c_model.eval()  # eval mode disables dropout\n",
    "    r_model.eval()  # eval mode disables dropout\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses2 = AverageMeter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Prohibit gradient computation explicity because I had some problems with memory\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for i_batch, (images, labels, coords) in enumerate(train_loader):\n",
    "            # Move to default device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            coords = coords.to(device)\n",
    "\n",
    "            # CLASSIFICATION Eval\n",
    "            predicted_class, all_crops, cropCoords = c_model(images)\n",
    "            loss1 = c_criterion(predicted_class, labels)\n",
    "\n",
    "            all_crops = all_crops.to(device)\n",
    "            cropCoords = cropCoords.to(device)\n",
    "\n",
    "            # REGRESSION Eval\n",
    "\n",
    "            for i in range(9):\n",
    "                batchcrop = all_crops[:, i, :, :]\n",
    "                batchcrop.unsqueeze_(1)\n",
    "                offset = cropCoords[i]\n",
    "                offset = offset.repeat(all_crops.size(0), 1)\n",
    "                offset = torch.cat((offset, torch.zeros((all_crops.size(0),\n",
    "                                                         1)).to(device)),\n",
    "                                   dim=1)\n",
    "                center_truth = coords[:, i, :]\n",
    "\n",
    "                center_est = r_model(batchcrop).to(device)\n",
    "                center_est = center_est + offset\n",
    "\n",
    "                loss2 = regrCriterion(center_truth, center_est)\n",
    "                losses2.update(loss2.item())\n",
    "\n",
    "            losses.update(loss1.item())\n",
    "\n",
    "            batch_time.update(time.time() - start)\n",
    "            start = time.time()\n",
    "\n",
    "            # Print status\n",
    "            if i_batch % print_freq == 0:\n",
    "                print(\n",
    "                    \"[{0}/{1}]\\t\"\n",
    "                    \"Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                    \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(i_batch, len(val_loader),\n",
    "                                                                    batch_time=batch_time, loss=losses),\n",
    "                    \"Regr Loss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(i_batch, len(val_loader),\n",
    "                                                                         batch_time=batch_time, loss=losses2)\n",
    "                )\n",
    "\n",
    "    print(\"\\n * LOSS - {loss.avg:.3f}\\n\".format(loss=losses))\n",
    "    print(\" * REGR LOSS - {loss.avg:.3f}\\n\".format(loss=losses2))\n",
    "\n",
    "    return losses.avg, losses2.avg\n",
    "\n",
    "\n",
    "# In[9]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Load data -------------------------------------------------------\n",
    "\n",
    "batch_size = 15\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA available? device: \", device)\n",
    "\n",
    "\n",
    "# Dataset is depth images of rectangular blocks\n",
    "train_dataset = RectDepthImgsDataset(img_dir=\"./data\", coords=train_truth)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = RectDepthImgsDataset(img_dir=\"./data/test\", coords=test_truth)\n",
    "\n",
    "# Data loader\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# -- Hyperparamaters -------------------------\n",
    "\n",
    "num_epochs = 80  # number of epochs to run without early-stopping\n",
    "learning_rate = 0.001\n",
    "\n",
    "start_epoch = 0  # start at this epoch\n",
    "# number of epochs since there was an improvement in the validation metric\n",
    "epochs_since_improvement = 0\n",
    "best_loss = 1000.0  # assume a high loss at first\n",
    "workers = 4  # number of workers for loading data in the DataLoader\n",
    "\n",
    "classifModel = classifNet(IMG_X, IMG_Y)\n",
    "classifModel = classifModel.to(device)\n",
    "\n",
    "regrModel = regrNet((100, 100), 3)  # crop size in pixels; output x,y, theta\n",
    "regrModel = regrModel.to(device)\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "classifCriterion = nn.BCELoss()\n",
    "regrCriterion = nn.MSELoss()\n",
    "#regrCriterion = nn.SmoothL1Loss()\n",
    "\n",
    "optimizer1 = torch.optim.Adam(classifModel.parameters(), lr=learning_rate)\n",
    "optimizer2 = torch.optim.Adam(regrModel.parameters(), lr=learning_rate)\n",
    "\n",
    "print_freq = 25  # print training or validation status every __ batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Hyperparamaters -------------------------\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Training and validation.\n",
    "    \"\"\"\n",
    "\n",
    "    global epochs_since_improvement, start_epoch, best_loss, epoch, checkpoint\n",
    "\n",
    "    print(\"Training model now...\")\n",
    "\n",
    "    # -- Begin training -------------------------\n",
    "    for epoch in range(num_epochs):\n",
    "        train(\n",
    "            train_loader=train_loader,\n",
    "            c_model=classifModel,\n",
    "            r_model=regrModel,\n",
    "            classifCriterion=classifCriterion,\n",
    "            regrCriterion=regrCriterion,\n",
    "            optimizer1=optimizer1,\n",
    "            optimizer2=optimizer2,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "        # Save checkpoint\n",
    "        save_checkpoint_small(epoch, classifModel, regrModel, optimizer1,\n",
    "                    optimizer2)\n",
    "        \n",
    "        \n",
    "\n",
    "    # One epoch's validation\n",
    "    val_loss, regr_loss = validate(val_loader=test_loader,\n",
    "                                   c_model=classifModel, r_model=regrModel,\n",
    "                                   c_criterion=classifCriterion,\n",
    "                                   r_criterion=regrCriterion)\n",
    "\n",
    "    # Did validation loss improve?\n",
    "    is_best = val_loss < best_loss\n",
    "    best_loss = min(val_loss, best_loss)\n",
    "\n",
    "    if not is_best:\n",
    "        epochs_since_improvement += 1\n",
    "        print(\"\\nEpochs since last improvement: %d\\n\" %\n",
    "              (epochs_since_improvement,))\n",
    "\n",
    "    else:\n",
    "        epochs_since_improvement = 0\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(epoch, epochs_since_improvement, classifModel, regrModel, optimizer1,\n",
    "                    optimizer2, val_loss, regr_loss, best_loss, is_best)\n",
    "    # save_checkpoint(epoch, epochs_since_improvement, classifModel, optimizer1,\n",
    "    #                val_loss, best_loss, is_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model now...\n",
      "Epoch: [0][0/334]\tBatch Time 0.057 (0.057)\tLoss 0.0001 (0.0001)\t RLoss 44783.5938 (39397.8576)\t\n",
      "Epoch: [0][25/334]\tBatch Time 0.043 (0.044)\tLoss 0.0001 (0.0024)\t RLoss 49432.9570 (40187.9061)\t\n",
      "Epoch: [0][50/334]\tBatch Time 0.043 (0.044)\tLoss 0.0001 (0.0026)\t RLoss 50143.5703 (40759.1064)\t\n",
      "Epoch: [0][75/334]\tBatch Time 0.043 (0.044)\tLoss 0.0128 (0.0026)\t RLoss 52549.4375 (41189.4341)\t\n",
      "Epoch: [0][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0020 (0.0021)\t RLoss 37134.6055 (41265.6861)\t\n",
      "Epoch: [0][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0017)\t RLoss 43767.8164 (41142.0097)\t\n",
      "Epoch: [0][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0014)\t RLoss 50310.9453 (41115.8816)\t\n",
      "Epoch: [0][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0014)\t RLoss 53297.8203 (41102.4725)\t\n",
      "Epoch: [0][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 45202.8086 (41123.7743)\t\n",
      "Epoch: [0][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0011)\t RLoss 45124.0664 (41174.1352)\t\n",
      "Epoch: [0][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0014)\t RLoss 42702.4492 (41236.2086)\t\n",
      "Epoch: [0][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0013)\t RLoss 46273.5117 (41262.0620)\t\n",
      "Epoch: [0][300/334]\tBatch Time 0.044 (0.043)\tLoss 0.0001 (0.0012)\t RLoss 48660.5273 (41316.6660)\t\n",
      "Epoch: [0][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 48797.4805 (41253.2793)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0006 (0.0006)\t Regr Loss 1086.1505 (842.7384)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0011)\t Regr Loss 1043.5897 (922.1065)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0020)\t Regr Loss 908.8359 (923.9251)\t\n",
      "[75/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0014)\t Regr Loss 1035.0377 (918.9850)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0014)\t Regr Loss 1059.1896 (915.0093)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0070 (0.0015)\t Regr Loss 1070.1710 (914.7052)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0005 (0.0017)\t Regr Loss 835.3495 (918.0576)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.0075 (0.0015)\t Regr Loss 734.3964 (917.9640)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0014)\t Regr Loss 1121.1615 (916.3050)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 1007.9363 (916.1080)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 1015.2506 (916.1640)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 1154.7715 (917.6373)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.0006 (0.0014)\t Regr Loss 1098.7238 (917.4600)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 1097.4003 (917.5600)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.362\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n",
      "Epoch: [1][0/334]\tBatch Time 0.044 (0.044)\tLoss 0.0005 (0.0005)\t RLoss 54823.8242 (41087.3156)\t\n",
      "Epoch: [1][25/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0002)\t RLoss 44906.6289 (41013.0438)\t\n",
      "Epoch: [1][50/334]\tBatch Time 0.042 (0.043)\tLoss 0.0000 (0.0003)\t RLoss 48782.7070 (41132.7859)\t\n",
      "Epoch: [1][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0006)\t RLoss 44953.1484 (41004.7001)\t\n",
      "Epoch: [1][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0009)\t RLoss 52893.4844 (41012.2249)\t\n",
      "Epoch: [1][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0011)\t RLoss 52687.8281 (41002.1490)\t\n",
      "Epoch: [1][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0009)\t RLoss 47217.3125 (41034.9311)\t\n",
      "Epoch: [1][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0266 (0.0013)\t RLoss 46127.0898 (41197.4220)\t\n",
      "Epoch: [1][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0012)\t RLoss 48765.7891 (41155.1688)\t\n",
      "Epoch: [1][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 44469.9414 (41225.0296)\t\n",
      "Epoch: [1][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0012)\t RLoss 38724.0820 (41190.0955)\t\n",
      "Epoch: [1][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 45373.7773 (41225.1915)\t\n",
      "Epoch: [1][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 45489.0703 (41207.1373)\t\n",
      "Epoch: [1][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 49102.1250 (41264.2454)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0402 (0.0402)\t Regr Loss 872.6544 (940.3458)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0057)\t Regr Loss 1212.1534 (917.1626)\t\n",
      "[50/20]\tBatch Time 0.027 (0.026)\tLoss 0.0000 (0.0051)\t Regr Loss 1074.7483 (921.1169)\t\n",
      "[75/20]\tBatch Time 0.026 (0.026)\tLoss 0.0000 (0.0036)\t Regr Loss 1002.0954 (920.3748)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0028)\t Regr Loss 801.0011 (915.9851)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0003 (0.0023)\t Regr Loss 855.4164 (918.2409)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0019)\t Regr Loss 1042.7924 (915.1043)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.0010 (0.0017)\t Regr Loss 1090.3453 (914.6266)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0015)\t Regr Loss 1241.0045 (915.6281)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0004 (0.0015)\t Regr Loss 881.3193 (914.5531)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0015)\t Regr Loss 919.1505 (913.3014)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 927.8741 (913.8991)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0013)\t Regr Loss 986.8425 (915.5537)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 1156.6393 (916.9238)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.462\n",
      "\n",
      "\n",
      "Epochs since last improvement: 5\n",
      "\n",
      "Epoch: [2][0/334]\tBatch Time 0.046 (0.046)\tLoss 0.0000 (0.0000)\t RLoss 49215.4570 (40296.7390)\t\n",
      "Epoch: [2][25/334]\tBatch Time 0.043 (0.044)\tLoss 0.0002 (0.0004)\t RLoss 48553.3594 (41264.6343)\t\n",
      "Epoch: [2][50/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0008)\t RLoss 47944.5352 (41729.9870)\t\n",
      "Epoch: [2][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0006)\t RLoss 30413.7188 (41716.7544)\t\n",
      "Epoch: [2][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0005)\t RLoss 45216.1445 (41712.8478)\t\n",
      "Epoch: [2][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0011)\t RLoss 43832.4023 (41644.7029)\t\n",
      "Epoch: [2][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0015)\t RLoss 29986.4160 (41487.7176)\t\n",
      "Epoch: [2][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0016)\t RLoss 46100.2227 (41490.0575)\t\n",
      "Epoch: [2][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0076 (0.0015)\t RLoss 39793.1289 (41438.4583)\t\n",
      "Epoch: [2][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0013)\t RLoss 51365.7734 (41369.2296)\t\n",
      "Epoch: [2][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0015)\t RLoss 44212.8633 (41348.7350)\t\n",
      "Epoch: [2][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0014)\t RLoss 47276.1211 (41308.7459)\t\n",
      "Epoch: [2][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0014)\t RLoss 50210.6562 (41302.3236)\t\n",
      "Epoch: [2][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 42405.2500 (41236.6036)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0001)\t Regr Loss 842.4117 (837.4186)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0006)\t Regr Loss 960.7851 (902.4845)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.0009 (0.0006)\t Regr Loss 1327.9036 (907.7283)\t\n",
      "[75/20]\tBatch Time 0.027 (0.027)\tLoss 0.0128 (0.0009)\t Regr Loss 1102.9626 (909.9225)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0069 (0.0012)\t Regr Loss 1097.4851 (915.0337)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0010)\t Regr Loss 1183.5040 (914.2232)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0009)\t Regr Loss 860.7263 (914.8772)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0008)\t Regr Loss 1005.3228 (914.5824)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0008)\t Regr Loss 959.6727 (913.8419)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0007)\t Regr Loss 871.2847 (918.0211)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0010)\t Regr Loss 1029.7261 (919.7073)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0012)\t Regr Loss 943.5222 (917.6127)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0012)\t Regr Loss 1127.8109 (917.5417)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 1217.5811 (916.7904)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.435\n",
      "\n",
      "\n",
      "Epochs since last improvement: 6\n",
      "\n",
      "Epoch: [3][0/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0000)\t RLoss 42384.2695 (38961.2589)\t\n",
      "Epoch: [3][25/334]\tBatch Time 0.044 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 37493.9766 (41397.1092)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][50/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0018)\t RLoss 45277.9492 (41417.8852)\t\n",
      "Epoch: [3][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0017)\t RLoss 42715.4766 (41471.0820)\t\n",
      "Epoch: [3][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0018)\t RLoss 55156.9922 (41225.1805)\t\n",
      "Epoch: [3][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0020)\t RLoss 50590.1016 (41242.2999)\t\n",
      "Epoch: [3][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0019)\t RLoss 42613.8594 (41360.2187)\t\n",
      "Epoch: [3][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0018)\t RLoss 46863.5820 (41257.4040)\t\n",
      "Epoch: [3][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0016)\t RLoss 50841.2070 (41256.3135)\t\n",
      "Epoch: [3][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0015)\t RLoss 31951.0254 (41305.0104)\t\n",
      "Epoch: [3][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0014)\t RLoss 41769.7734 (41261.5089)\t\n",
      "Epoch: [3][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 53188.2578 (41235.5124)\t\n",
      "Epoch: [3][300/334]\tBatch Time 0.044 (0.043)\tLoss 0.0001 (0.0013)\t RLoss 41706.0664 (41314.1614)\t\n",
      "Epoch: [3][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0013)\t RLoss 41803.7188 (41250.9687)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0001)\t Regr Loss 1100.9028 (1017.3814)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0024)\t Regr Loss 509.4925 (895.5033)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0025)\t Regr Loss 1184.1886 (903.9719)\t\n",
      "[75/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0026)\t Regr Loss 992.0243 (906.7824)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0021)\t Regr Loss 1090.9587 (918.5757)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0021)\t Regr Loss 1031.7911 (914.2719)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0019)\t Regr Loss 1032.1635 (915.6657)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0017)\t Regr Loss 1247.0265 (915.4531)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0018)\t Regr Loss 873.8693 (914.4247)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0016)\t Regr Loss 1088.8118 (916.2383)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0003 (0.0015)\t Regr Loss 763.3939 (917.0611)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0014)\t Regr Loss 957.0931 (917.1183)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 1212.2524 (915.9633)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 810.4564 (916.4740)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.365\n",
      "\n",
      "\n",
      "Epochs since last improvement: 7\n",
      "\n",
      "Epoch: [4][0/334]\tBatch Time 0.045 (0.045)\tLoss 0.0000 (0.0000)\t RLoss 47663.8281 (36568.2476)\t\n",
      "Epoch: [4][25/334]\tBatch Time 0.043 (0.044)\tLoss 0.0006 (0.0002)\t RLoss 54046.0742 (41635.8504)\t\n",
      "Epoch: [4][50/334]\tBatch Time 0.044 (0.044)\tLoss 0.0672 (0.0019)\t RLoss 47184.7461 (41684.4058)\t\n",
      "Epoch: [4][75/334]\tBatch Time 0.043 (0.044)\tLoss 0.0000 (0.0016)\t RLoss 42716.8945 (41623.9058)\t\n",
      "Epoch: [4][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0012)\t RLoss 48086.9609 (41461.2211)\t\n",
      "Epoch: [4][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0012)\t RLoss 46893.1953 (41440.9864)\t\n",
      "Epoch: [4][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0004 (0.0010)\t RLoss 52432.0156 (41345.0980)\t\n",
      "Epoch: [4][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0003 (0.0009)\t RLoss 49895.6055 (41325.8561)\t\n",
      "Epoch: [4][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0010)\t RLoss 59496.5391 (41316.6764)\t\n",
      "Epoch: [4][225/334]\tBatch Time 0.044 (0.043)\tLoss 0.0000 (0.0011)\t RLoss 46416.5938 (41208.7707)\t\n",
      "Epoch: [4][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0011)\t RLoss 47195.7812 (41274.2443)\t\n",
      "Epoch: [4][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 41444.7383 (41317.3771)\t\n",
      "Epoch: [4][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0012)\t RLoss 36478.2383 (41275.9713)\t\n",
      "Epoch: [4][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 51144.2617 (41246.1319)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0006 (0.0006)\t Regr Loss 676.2220 (886.4563)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0006)\t Regr Loss 1048.3839 (910.8424)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.0010 (0.0006)\t Regr Loss 883.7709 (911.8711)\t\n",
      "[75/20]\tBatch Time 0.026 (0.027)\tLoss 0.0000 (0.0021)\t Regr Loss 1072.6559 (914.9229)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0017)\t Regr Loss 1030.5751 (911.0157)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0016)\t Regr Loss 916.4724 (912.7392)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0015)\t Regr Loss 954.1357 (914.1742)\t\n",
      "[175/20]\tBatch Time 0.026 (0.027)\tLoss 0.0004 (0.0013)\t Regr Loss 968.5344 (912.8726)\t\n",
      "[200/20]\tBatch Time 0.026 (0.027)\tLoss 0.0000 (0.0012)\t Regr Loss 862.0113 (915.1288)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0011)\t Regr Loss 1156.7074 (917.1064)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0012)\t Regr Loss 1199.6401 (916.7196)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 978.9224 (917.5248)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0013)\t Regr Loss 956.8109 (918.4766)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0006 (0.0012)\t Regr Loss 1193.1731 (917.5245)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.335\n",
      "\n",
      "\n",
      "Epochs since last improvement: 8\n",
      "\n",
      "Epoch: [5][0/334]\tBatch Time 0.044 (0.044)\tLoss 0.0001 (0.0001)\t RLoss 47400.9570 (41021.0591)\t\n",
      "Epoch: [5][25/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0022)\t RLoss 37932.9844 (40371.2883)\t\n",
      "Epoch: [5][50/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0012)\t RLoss 49135.3984 (40501.6781)\t\n",
      "Epoch: [5][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0010)\t RLoss 45344.0156 (41028.8432)\t\n",
      "Epoch: [5][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 46175.8516 (40886.1788)\t\n",
      "Epoch: [5][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0011)\t RLoss 48229.3906 (40872.6385)\t\n",
      "Epoch: [5][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0004 (0.0009)\t RLoss 43154.1602 (40956.4353)\t\n",
      "Epoch: [5][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0008)\t RLoss 42973.8711 (40961.7082)\t\n",
      "Epoch: [5][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0007)\t RLoss 47346.3125 (41080.5952)\t\n",
      "Epoch: [5][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0011)\t RLoss 29274.2676 (41081.7522)\t\n",
      "Epoch: [5][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 50387.7227 (41165.8408)\t\n",
      "Epoch: [5][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 41413.7461 (41228.4117)\t\n",
      "Epoch: [5][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 54486.8906 (41256.9092)\t\n",
      "Epoch: [5][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 50478.8594 (41249.1969)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0002)\t Regr Loss 874.0825 (919.0244)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0004)\t Regr Loss 1043.3097 (915.3373)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0003)\t Regr Loss 1225.5391 (909.0767)\t\n",
      "[75/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0007)\t Regr Loss 1092.6339 (918.6376)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 1281.4785 (919.3976)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0011)\t Regr Loss 1104.8552 (918.7528)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0011)\t Regr Loss 1039.4443 (915.0363)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0012)\t Regr Loss 751.7958 (912.5188)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 0.0129 (0.0013)\t Regr Loss 1158.9949 (914.3310)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 934.5617 (914.7930)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0012)\t Regr Loss 1098.7738 (915.3945)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0012)\t Regr Loss 774.3310 (914.9344)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.0006 (0.0012)\t Regr Loss 1083.1665 (914.5189)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0012)\t Regr Loss 1150.5822 (915.1933)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.490\n",
      "\n",
      "\n",
      "Epochs since last improvement: 9\n",
      "\n",
      "Epoch: [6][0/334]\tBatch Time 0.045 (0.045)\tLoss 0.0002 (0.0002)\t RLoss 40463.5078 (42440.2535)\t\n",
      "Epoch: [6][25/334]\tBatch Time 0.043 (0.044)\tLoss 0.0000 (0.0012)\t RLoss 43893.8750 (41014.9553)\t\n",
      "Epoch: [6][50/334]\tBatch Time 0.043 (0.044)\tLoss 0.0008 (0.0007)\t RLoss 38328.5508 (41171.7835)\t\n",
      "Epoch: [6][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0013)\t RLoss 46716.0625 (41327.0542)\t\n",
      "Epoch: [6][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0010)\t RLoss 48803.9023 (41164.9927)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0010)\t RLoss 59755.6641 (41282.5627)\t\n",
      "Epoch: [6][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0010)\t RLoss 46736.3672 (41308.5775)\t\n",
      "Epoch: [6][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0009)\t RLoss 38389.1992 (41317.2803)\t\n",
      "Epoch: [6][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 45700.2148 (41325.0595)\t\n",
      "Epoch: [6][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0011)\t RLoss 46377.1172 (41326.7555)\t\n",
      "Epoch: [6][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0010)\t RLoss 41559.4766 (41335.8436)\t\n",
      "Epoch: [6][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0010)\t RLoss 48355.2539 (41263.0394)\t\n",
      "Epoch: [6][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 43366.5273 (41249.7917)\t\n",
      "Epoch: [6][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0013)\t RLoss 35690.9844 (41256.7499)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0001)\t Regr Loss 1002.5750 (1041.7709)\t\n",
      "[25/20]\tBatch Time 0.026 (0.026)\tLoss 0.0005 (0.0017)\t Regr Loss 1256.0436 (904.6695)\t\n",
      "[50/20]\tBatch Time 0.026 (0.026)\tLoss 0.0000 (0.0009)\t Regr Loss 739.4132 (902.3403)\t\n",
      "[75/20]\tBatch Time 0.026 (0.026)\tLoss 0.0002 (0.0011)\t Regr Loss 935.9808 (909.9737)\t\n",
      "[100/20]\tBatch Time 0.027 (0.026)\tLoss 0.0000 (0.0009)\t Regr Loss 987.3501 (915.1937)\t\n",
      "[125/20]\tBatch Time 0.027 (0.026)\tLoss 0.0000 (0.0009)\t Regr Loss 1013.9260 (917.3990)\t\n",
      "[150/20]\tBatch Time 0.026 (0.026)\tLoss 0.0000 (0.0008)\t Regr Loss 1122.2540 (919.6256)\t\n",
      "[175/20]\tBatch Time 0.026 (0.026)\tLoss 0.0000 (0.0008)\t Regr Loss 1196.2703 (917.6026)\t\n",
      "[200/20]\tBatch Time 0.027 (0.026)\tLoss 0.0000 (0.0009)\t Regr Loss 1009.1830 (917.4587)\t\n",
      "[225/20]\tBatch Time 0.026 (0.026)\tLoss 0.0068 (0.0010)\t Regr Loss 798.3281 (917.6481)\t\n",
      "[250/20]\tBatch Time 0.026 (0.026)\tLoss 0.0000 (0.0012)\t Regr Loss 918.0663 (917.1765)\t\n",
      "[275/20]\tBatch Time 0.027 (0.026)\tLoss 0.0000 (0.0012)\t Regr Loss 997.7406 (916.5491)\t\n",
      "[300/20]\tBatch Time 0.026 (0.026)\tLoss 0.0004 (0.0012)\t Regr Loss 1043.6913 (917.4618)\t\n",
      "[325/20]\tBatch Time 0.026 (0.026)\tLoss 0.0000 (0.0013)\t Regr Loss 954.8763 (916.2791)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.452\n",
      "\n",
      "\n",
      "Epochs since last improvement: 10\n",
      "\n",
      "Epoch: [7][0/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0000)\t RLoss 35084.3203 (43263.5047)\t\n",
      "Epoch: [7][25/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0012)\t RLoss 51804.2578 (41208.9310)\t\n",
      "Epoch: [7][50/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0012)\t RLoss 54816.1172 (40627.9025)\t\n",
      "Epoch: [7][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0014)\t RLoss 44261.2617 (40946.9287)\t\n",
      "Epoch: [7][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0014)\t RLoss 42118.7656 (40832.0032)\t\n",
      "Epoch: [7][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 38599.5977 (40878.7978)\t\n",
      "Epoch: [7][150/334]\tBatch Time 0.044 (0.043)\tLoss 0.0007 (0.0012)\t RLoss 48446.6172 (40945.3133)\t\n",
      "Epoch: [7][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0015)\t RLoss 57372.0312 (41001.7960)\t\n",
      "Epoch: [7][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 45646.6367 (40974.1352)\t\n",
      "Epoch: [7][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0013)\t RLoss 54414.5469 (41124.9376)\t\n",
      "Epoch: [7][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0014)\t RLoss 47316.6328 (41175.0236)\t\n",
      "Epoch: [7][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0014)\t RLoss 48688.6445 (41181.0731)\t\n",
      "Epoch: [7][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0013)\t RLoss 38305.1406 (41178.8902)\t\n",
      "Epoch: [7][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0161 (0.0013)\t RLoss 42031.4414 (41233.1932)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0000)\t Regr Loss 898.3888 (964.4798)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0002)\t Regr Loss 956.1819 (912.7816)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.0006 (0.0002)\t Regr Loss 1126.7052 (912.8153)\t\n",
      "[75/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0011)\t Regr Loss 889.0241 (909.0582)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0009)\t Regr Loss 875.1196 (912.3635)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 1129.4597 (911.9696)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0015)\t Regr Loss 1052.1029 (913.5893)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.0003 (0.0016)\t Regr Loss 889.1976 (914.7629)\t\n",
      "[200/20]\tBatch Time 0.031 (0.027)\tLoss 0.0000 (0.0016)\t Regr Loss 1042.9537 (912.7143)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0014)\t Regr Loss 994.5889 (913.6142)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0004 (0.0013)\t Regr Loss 1086.3546 (913.3145)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0003 (0.0013)\t Regr Loss 1132.2052 (914.9654)\t\n",
      "[300/20]\tBatch Time 0.032 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 1139.7103 (915.3308)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0013)\t Regr Loss 952.0150 (915.7593)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.719\n",
      "\n",
      "\n",
      "Epochs since last improvement: 11\n",
      "\n",
      "Epoch: [8][0/334]\tBatch Time 0.046 (0.046)\tLoss 0.0000 (0.0000)\t RLoss 39639.2266 (38301.0996)\t\n",
      "Epoch: [8][25/334]\tBatch Time 0.043 (0.044)\tLoss 0.0000 (0.0034)\t RLoss 45055.1953 (42441.0786)\t\n"
     ]
    }
   ],
   "source": [
    "main()\n",
    "\n",
    "# alert when training is done\n",
    "sound_file = '/home/rui/Downloads/newyear.ogg'\n",
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All Ready!')\n",
    "\n",
    "\n",
    "filename = \"checkpoint_v2sliding.pth.tar\"\n",
    "checkpoint = torch.load(filename)\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "best_loss = checkpoint['best_loss']\n",
    "print('\\nLoaded checkpoint from epoch %d. Best loss so far is %.3f.\\n' % (start_epoch, best_loss))\n",
    "classifModel = checkpoint['model1']\n",
    "regrModel = checkpoint['model2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "FOR N =  1\n",
      "tensor([1., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[84.0, 39.0, 2.6179938316345215]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(test_loader)\n",
    "images, labels, coords = dataiter.next()\n",
    "\n",
    "n = 1\n",
    "print(\"\\n\\nFOR N = \", n)\n",
    "imgSample = images[n]\n",
    "max_idx = torch.argmax(coords[n][:,0])\n",
    "x, y, theta = coords[n][max_idx]\n",
    "print(labels[n])\n",
    "print([float(zed) for zed in (x,y,theta)])\n",
    "#print(x,y,theta)\n",
    "#crops, __ = makeCrops(imgSample, 50, (100, 100), (x,y, theta))\n",
    "\n",
    "plt.imshow(imgSample)\n",
    "#fig, axess = plt.subplots(3,3, figsize=(10,10))\n",
    "#axess = np.array(axess).flatten()#order='F') #'F' means to flatten in column-major (Fortran- style) order.\n",
    "#for i in range(9):\n",
    "#    axess[i].imshow(crops[i])\n",
    "#    axess[i].set_ylabel('# '+str(i))\n",
    "plt.suptitle('Window')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss across batch size of  15 is:  tensor(0.0054, device='cuda:0') tensor(1469.0959, device='cuda:0')\n",
      "!-- guesses size torch.Size([9, 15, 3])\n",
      "!-- labels size torch.Size([15, 9])\n",
      "!-- guesses size torch.Size([15, 9, 3])\n",
      "tensor([[  0.,   0.],\n",
      "        [  0.,  50.],\n",
      "        [  0., 100.],\n",
      "        [ 50.,   0.],\n",
      "        [ 50.,  50.],\n",
      "        [ 50., 100.],\n",
      "        [100.,   0.],\n",
      "        [100.,  50.],\n",
      "        [100., 100.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# -- Check the results -------------------------------------------------------\n",
    "\n",
    "# -- Utility ---------------------------------------------\n",
    "def makeCrops(image, stepSize, windowSize, true_center):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    image = image.type(torch.FloatTensor)\n",
    "    crops = []\n",
    "    truths = []\n",
    "    c_x, c_y, orient = true_center\n",
    "    # TODO: look into otdering, why it's y,x !\n",
    "    margin = 10 \n",
    "    # --> is x, but is the column\n",
    "    # to slide horizontally, y must come first\n",
    "    for y in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "        for x in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "            end_x, end_y = x + windowSize[1], y + windowSize[0]\n",
    "            hasRect = (x + margin < c_x < end_x - margin) and (\n",
    "                y + margin < c_y < end_y - margin\n",
    "            )\n",
    "            truths.append(hasRect)\n",
    "            crops.append(image[y:end_y, x:end_x])\n",
    "    crops = torch.stack(crops)\n",
    "    print(\"shape of crops\", crops.shape)\n",
    "    return crops, truths\n",
    "\n",
    "\n",
    "losses = AverageMeter()  # loss\n",
    "losses2 = AverageMeter()  # loss\n",
    "\n",
    "\n",
    "# -- Get some validation results ---------------------------------------------\n",
    "classifModel.to(device).eval()\n",
    "regrModel.to(device).eval()\n",
    "\n",
    "c_criterion = nn.BCELoss()\n",
    "r_criterion = nn.MSELoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    dataiter = iter(test_loader)\n",
    "    images, labels, coords = dataiter.next()\n",
    "    \n",
    "    # Move to default device\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    coords = coords.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    predicted_class, all_crops, cropCoords = classifModel(images)\n",
    "    # Loss\n",
    "    loss = c_criterion(predicted_class, labels)\n",
    "    all_crops = all_crops.to(device)\n",
    "    cropCoords = cropCoords.to(device)\n",
    "    \n",
    "    loss1 = classifCriterion(predicted_class, labels)\n",
    "    \n",
    "    guesses = []\n",
    "    # num batches = 9\n",
    "    for i in range(9):\n",
    "        batchcrop = all_crops[:, i, :, :]\n",
    "        batchcrop.unsqueeze_(1)\n",
    "        offset = cropCoords[i]\n",
    "        offset = offset.repeat(all_crops.size(0), 1)\n",
    "        offset = torch.cat((offset, torch.zeros((all_crops.size(0),\n",
    "                                                 1)).to(device)),\n",
    "                           dim=1)\n",
    "        center_truth = coords[:, i, :]\n",
    "        #print('!-- center truth\\n', center_truth)\n",
    "        center_est = regrModel(batchcrop).to(device)\n",
    "        #print('!-- center est\\n', center_est)\n",
    "        center_est = center_est + offset\n",
    "        #print('!-- offset center est\\n', center_est)\n",
    "        guesses.append(center_est)\n",
    "\n",
    "        loss2 = regrCriterion(center_truth, center_est)\n",
    "        losses2.update(loss2.item())\n",
    "\n",
    "    # Forward gass\n",
    "\n",
    "    #labels_est = torch.FloatTensor(\n",
    "        #predicted_class.detach().cpu().numpy())\n",
    "    \n",
    "    guesses = torch.stack(guesses)\n",
    "\n",
    "    # Loss\n",
    "    #loss2 = r_criterion(masked_est, masked_truth)\n",
    "\n",
    "print(\"loss across batch size of \", labels.size()[0], 'is: ', loss1, loss2)\n",
    "#print(labels)\n",
    "print('!-- guesses size', guesses.size())\n",
    "print('!-- labels size', labels.size())\n",
    "#print(torch.round(predicted_class))\n",
    "\n",
    "predicted_coords = guesses.view(-1, 9, 3)\n",
    "#print(predicted_class)\n",
    "#print(predicted_coords)\n",
    "print('!-- guesses size', predicted_coords.size())\n",
    "\n",
    "print(cropCoords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of crops torch.Size([9, 100, 100])\n",
      "!-- coords tensor([[  0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000],\n",
      "        [ 55.0000, 165.0000,   0.8552],\n",
      "        [  0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000]], device='cuda:0')\n",
      "!-- FOR N =  2\n",
      "y (crops) \n",
      "\t [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "!-- yhat (crops) \n",
      "\t [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "\n",
      "\n",
      "FOR N =  2\n",
      "!-- center y \n",
      "\t [55.0, 165.0, 0.8552113175392151]\n",
      "!-- center y est \n",
      "\t  [75.78984832763672, 37.57746124267578, 1.0292305946350098]\n",
      "tensor([[ 53.7454, 120.8994,   1.9946],\n",
      "        [-54.8233,  -5.9230,   0.0168],\n",
      "        [ 75.7898,  37.5775,   1.0292],\n",
      "        [-54.8233,  -5.9230,   0.0168],\n",
      "        [ -6.7071, 135.1530,   1.8055],\n",
      "        [-36.6498,  16.8935,  -0.1021],\n",
      "        [  2.2721,  88.1888,   1.8436],\n",
      "        [-54.8233,  -5.9230,   0.0168],\n",
      "        [-54.8233,  -5.9230,   0.0168]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -- Plot windows and print labels -----------------------------------\n",
    "n = 2\n",
    "imgSample = images[n]\n",
    "crops, __ = makeCrops(imgSample, 50, (100, 100), (x,y, theta))\n",
    "# -- Show original image, and the sliding windo crops -------\n",
    "plt.imshow(imgSample)\n",
    "# ---------------------------------------------------\n",
    "foo_label = labels[n]\n",
    "\n",
    "foo_coord = coords[n] \n",
    "predicted_locs = predicted_coords.view(-1,\n",
    "                                     coords.size(1), coords.size(2))\n",
    "foo_coord_est = predicted_locs[n] # 3 per window\n",
    "foo_label_est = predicted_class[n]\n",
    "print('!-- coords', foo_coord)\n",
    "\n",
    "max_idx = torch.argmax(foo_coord[:,0])\n",
    "x, y, theta = foo_coord[max_idx]\n",
    "est_max_idx = torch.argmax(foo_coord_est[:,0])\n",
    "x_est, y_est, theta_est = foo_coord_est[est_max_idx]\n",
    "\n",
    "# -- Print x,y for one result -------\n",
    "# -- Print window t/f for one result -------\n",
    "print(\"!-- FOR N = \", n)\n",
    "print(\"y (crops) \\n\\t\", [int(l) for l in foo_label])\n",
    "print(\"!-- yhat (crops) \\n\\t\", [int(np.round(p,0)) for p in foo_label_est] )\n",
    "# -------------------------------------------------\n",
    "sns.set(rc={\"figure.figsize\": (8, 6)})\n",
    "\n",
    "print(\"\\n\\nFOR N = \", n)\n",
    "print(\"!-- center y \\n\\t\", [float(zed) for zed in (x, y, theta)])\n",
    "print(\"!-- center y est \\n\\t \", [float(zed) for zed in (x_est, y_est, theta_est)])\n",
    "print(foo_coord_est)\n",
    "\n",
    "fig, axess = plt.subplots(3,3, figsize=(10,10))\n",
    "axess = np.array(axess).flatten()#order='F') #'F' means to flatten in column-major (Fortran- style) order.\n",
    "for i in range(9):\n",
    "    axess[i].imshow(crops[i])\n",
    "    axess[i].set_ylabel('# '+str(i))\n",
    "plt.suptitle('Window')\n",
    "plt.show()\n",
    "\n",
    "#for (i, crop) in enumerate(crops):\n",
    "    # print(\"1-index number of window: \", i+1, 'x', x, 'y', y, 'has rectangle?', hasRect)\n",
    "    #plt.figure()\n",
    "    #plt.suptitle(\"numero: %d\" % (i))\n",
    "    #plt.imshow(crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(\"!-- yhat \\n\\t\", [int(round(o, 0)) for o in outputs[n].cpu().numpy()])    \n",
    "print(\"!-- \")\n",
    "print('True locations, defined for each crop\\n', foo_coord)\n",
    "print(\"!-- \")\n",
    "print('Full predicted locations (3 per crop)\\n', foo_coord_est)\n",
    "#print(\"\\n ------ x,y center + orient: \", coords[n], \"\\n\"))\n",
    "print(np.argwhere(foo_coord_est > 0) )\n",
    "\n",
    "# -- Print outputs for multiple results -------\n",
    "for ix in range(5):\n",
    "    print('\\nSAMPLE ', ix)\n",
    "    print(\"!-- y (crops) \\n\\t\", [int(l) for l in labels[ix]])\n",
    "    print(\"!-- yhat (crops) \\n\\t\", [int(np.round(p, 0)) for p in predicted_class[ix]] )\n",
    "\n",
    "# -- Main ---------------------------------------------\n",
    "#if __name__ == '__main__':\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
