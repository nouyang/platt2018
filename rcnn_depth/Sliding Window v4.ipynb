{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Sliding Window\n",
    "# Feb 2019\n",
    "#\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import torchvision\n",
    "from IPython.display import display  # to display images\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import io\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running main!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import torchvision\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import io\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "# from IPython.display import Audio\n",
    "# from IPython.display import display  # to display images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataset -------------------------------------------------------\n",
    "IMG_X, IMG_Y = 200, 200\n",
    "# length and width of blocks (fixed for now)\n",
    "block_l, block_w = 20, 30\n",
    "\n",
    "\n",
    "def makeRectangle(l, w, theta, offset=(0, 0)):\n",
    "    c, s = math.cos(theta), math.sin(theta)\n",
    "    rectCoords = [(l / 2.0, w / 2.0), (l / 2.0, -w / 2.0),\n",
    "                  (-l / 2.0, -w / 2.0), (-l / 2.0, w / 2.0), ]\n",
    "    return [(c * x - s * y + offset[0],\n",
    "             s * x + c * y + offset[1]) for (x, y) in rectCoords]\n",
    "\n",
    "\n",
    "# ---- Make depth images ---\n",
    "def make_dataset(dirname, num_images):\n",
    "    true_coords = []\n",
    "    newpath = \"./\" + dirname\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "        print(newpath)\n",
    "    for i in range(num_images):\n",
    "        # orient = 0 # degrees\n",
    "        img = Image.new(\"RGB\", (IMG_X, IMG_Y), \"black\")\n",
    "\n",
    "        # block_l and _w offset so blocks don't run off edge of image\n",
    "        rand_x = int(np.random.rand() * (IMG_X - 2 * block_l)) + block_l\n",
    "        rand_y = int(np.random.rand() * (IMG_Y - 2 * block_w)) + block_w\n",
    "        orient = int(np.random.rand() * 180)  # .random() is range [0.0, 1.0).\n",
    "        orient = math.radians(orient)  # math.cos takes radians!\n",
    "\n",
    "        # switch to degrees so that we normalize between the x,y,orient for MSELoss\n",
    "        true_coords.append(np.array((rand_x, rand_y, orient)))\n",
    "\n",
    "        rect_vertices = makeRectangle(\n",
    "            block_l, block_w, orient, offset=(rand_x, rand_y))\n",
    "\n",
    "        idraw = ImageDraw.Draw(img)\n",
    "        idraw.polygon(rect_vertices, fill=\"white\")\n",
    "\n",
    "        img.save(newpath + \"/rect\" + str(i) + \".png\")\n",
    "    return true_coords\n",
    "\n",
    "\n",
    "# NOTE Define size of dataset\n",
    "train_truth = make_dataset(\"data\", 5000)\n",
    "# print(len(train_truth))\n",
    "test_truth = make_dataset(\"./data/test\", 300)\n",
    "\n",
    "np.save(\"train_truth.npy\", train_truth)\n",
    "np.save(\"test_truth.npy\", test_truth)\n",
    "\n",
    "\n",
    "train_truth = np.load(\"train_truth.npy\")\n",
    "test_truth = np.load(\"test_truth.npy\")  # loading the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataloader -------------------------------------------------------\n",
    "class RectDepthImgsDataset(Dataset):\n",
    "    \"\"\"Artificially generated depth images dataset\"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, coords, transform=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.true_coords = coords\n",
    "        self.transform = transform\n",
    "\n",
    "        STEPSIZE = 50  # todo make this input arguments\n",
    "        WINDOWSIZE = (100, 100)\n",
    "        MARGIN_PX = 10 \n",
    "        self.step = STEPSIZE\n",
    "        self.cropSize = WINDOWSIZE\n",
    "        self.detectMargin = MARGIN_PX\n",
    "\n",
    "    def __len__(self):\n",
    "        # print('true coord len', len(self.true_coords))\n",
    "        return len(self.true_coords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # image = self.images[idx]\n",
    "        image = io.imread(self.img_dir + \"/rect\" +\n",
    "                          str(idx) + \".png\", as_gray=True)\n",
    "        # image = torch.FloatTensor(image).permute(2, 0, 1)  # PIL and torch expect difft orders\n",
    "        coords = torch.FloatTensor(self.true_coords[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        crops, labels, cropCoords = self.makeCrops(\n",
    "            image, self.step, self.cropSize, coords, self.detectMargin)\n",
    "\n",
    "        sample = image, torch.FloatTensor(\n",
    "            labels), torch.FloatTensor(cropCoords)\n",
    "        return sample\n",
    "\n",
    "    def makeCrops(self, image, stepSize, windowSize, rectCenter, detectMargin):\n",
    "        \"\"\"\n",
    "        Returns image crops, as well as T/F for those crops\n",
    "        \"\"\"\n",
    "        crops = []\n",
    "        c_x, c_y, theta = rectCenter\n",
    "        margin = detectMargin\n",
    "        hasRects = []\n",
    "        rectCoords = []\n",
    "        for y in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "            for x in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "                end_x, end_y = x + windowSize[1], y + windowSize[0]\n",
    "                hasRect = (x + margin < c_x < end_x - margin) and (\n",
    "                    y + margin < c_y < end_y - margin)\n",
    "                crops.append(image[y:end_y, x:end_x])\n",
    "                hasRects.append(hasRect)\n",
    "                if hasRect:\n",
    "                    rectCoords.append((c_x, c_y, theta))\n",
    "                else:\n",
    "                    # NOTE: Return empty label, when not hasRect\n",
    "                    rectCoords.append((0, 0, 0))\n",
    "        # print('length of truths in makeCrops', len(truths))\n",
    "        return crops, hasRects, rectCoords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  --- Define Nets -------------------------------------------------------\n",
    "\n",
    "class regrNet(nn.Module):\n",
    "    def __init__(self, cropSize, numOutputs):\n",
    "        \"\"\"\n",
    "        We need the image width and height to determine CNN layer sizes\n",
    "        \"\"\"\n",
    "        super(regrNet, self).__init__()\n",
    "        _pool = 2\n",
    "        _stride = 5\n",
    "        _outputlayers = 16\n",
    "\n",
    "        STEPSIZE = 50  # todo make this input arguments\n",
    "        self.step = STEPSIZE\n",
    "        self.cropSize = cropSize\n",
    "        self.numOutputs = numOutputs\n",
    "\n",
    "        def _calc(val):  # use to calculate layer sizes\n",
    "            layer_size = (val - (_stride - 1)) / _pool\n",
    "            return layer_size\n",
    "\n",
    "        self._const = _calc(_calc(self.cropSize[0]))\n",
    "        self._const *= _calc(_calc(self.cropSize[1]))\n",
    "        self._const *= _outputlayers\n",
    "        self._const = int(self._const)\n",
    "\n",
    "        # --- LOCATION OF RECTANGLE\n",
    "        # NOTE: only one channel for now (black/white)\n",
    "        self.conv1 = nn.Conv2d(1, 6, _stride).to(device)\n",
    "        self.pool = nn.MaxPool2d(_pool, _pool).to(device)\n",
    "        self.conv2 = nn.Conv2d(6, _outputlayers, _stride).to(device)\n",
    "        self.fc1 = nn.Linear(self._const, 120).to(device)\n",
    "        self.fc2 = nn.Linear(120, 84).to(device)\n",
    "        self.fc3 = nn.Linear(84, self.numOutputs).to(device)\n",
    "\n",
    "    def forward(self, crops):\n",
    "        \"\"\"\n",
    "        Forward propogation\n",
    "        : param image: images, a tensor of dimensions(N, 3, IMG_X, IMG_Y)\n",
    "        : return: (x, y, theta) and T/F for each window\n",
    "        \"\"\"\n",
    "        # TODO: presumably by doing this i lose some of the multithread goodness\n",
    "        crops = crops.to(device)\n",
    "\n",
    "        # LOCALIZATION\n",
    "        regr_crops = self.pool(F.relu((self.conv1(crops))))\n",
    "        regr_crops = self.pool(F.relu(self.conv2(regr_crops)))\n",
    "        regr_crops = regr_crops.view(-1, self._const)\n",
    "        regr_crops = F.relu(self.fc1(regr_crops))\n",
    "        regr_crops = F.relu(self.fc2(regr_crops))\n",
    "        regr_crops = self.fc3(regr_crops)\n",
    "\n",
    "        objCoords = regr_crops\n",
    "        # reshape to batchsize x number of crops x 3\n",
    "        objCoords = objCoords.reshape(-1, self.numOutputs)\n",
    "        return objCoords\n",
    "\n",
    "\n",
    "class classifNet(nn.Module):  # CIFAR is 32x32x3, MNIST is 28x28x1)\n",
    "    def __init__(self, IMG_X, IMG_Y):\n",
    "        \"\"\"\n",
    "        We need the image width and height to determine CNN layer sizes\n",
    "        \"\"\"\n",
    "        super(classifNet, self).__init__()\n",
    "        self._imgx = IMG_X\n",
    "        self._imgy = IMG_Y\n",
    "        _pool = 2\n",
    "        _stride = 5\n",
    "        _outputlayers = 16\n",
    "\n",
    "        STEPSIZE = 50  # todo make this input arguments\n",
    "        WINDOWSIZE = (100, 100)\n",
    "        self.step = STEPSIZE\n",
    "        self.cropSize = WINDOWSIZE\n",
    "        self.numCrops = 0\n",
    "        # T/F for now\n",
    "\n",
    "        # calculate number of crops\n",
    "        for x in range(0, IMG_Y - WINDOWSIZE[0] + 1, STEPSIZE):\n",
    "            for y in range(0, IMG_X - WINDOWSIZE[1] + 1, STEPSIZE):\n",
    "                self.numCrops += 1\n",
    "\n",
    "        def _calc(val):  # use to calculate layer sizes\n",
    "            layer_size = (val - (_stride - 1)) / _pool\n",
    "            return layer_size\n",
    "\n",
    "        # print(self._imgx)\n",
    "        # self._const = _calc(_calc(self._imgx))\n",
    "        # self._const *= _calc(_calc(self._imgy))\n",
    "        self._const = _calc(_calc(self.cropSize[0]))\n",
    "        self._const *= _calc(_calc(self.cropSize[1]))\n",
    "        self._const *= _outputlayers\n",
    "        self._const = int(self._const)\n",
    "\n",
    "        # --- CLASSIFICATION OF WINDOWS\n",
    "        # batch, 3 input image channels (RGB), 6 output channels, 5x5 square convolution\n",
    "        # NOTE: we switched to 1 input channel\n",
    "        self.conv1 = nn.Conv2d(self.numCrops, 6, _stride).to(device)\n",
    "        self.pool = nn.MaxPool2d(_pool, _pool).to(device)\n",
    "        self.conv2 = nn.Conv2d(6, _outputlayers, _stride).to(device)\n",
    "        self.fc1 = nn.Linear(self._const, 120).to(device)\n",
    "        self.fc2 = nn.Linear(120, 84).to(device)\n",
    "        self.fc3 = nn.Linear(84, self.numCrops).to(device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # TODO: batch normalization  self.bn = nn.BatchNorm2d()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propogation\n",
    "        : param image: images, a tensor of dimensions(N, 3, IMG_X, IMG_Y)\n",
    "        : return: (x, y, theta) and T/F for each window\n",
    "        \"\"\"\n",
    "        x = x.to(device)\n",
    "        batch_images = x\n",
    "\n",
    "        all_crops = []\n",
    "        for img in batch_images:\n",
    "            crops, cropCoords = self.makeCrops(img, self.step, self.cropSize)\n",
    "            all_crops.append(crops)\n",
    "        all_crops = torch.stack(all_crops)\n",
    "        feats = all_crops.view(-1, self.numCrops, self.cropSize[0],\n",
    "                               self.cropSize[1]).to(device)\n",
    "\n",
    "        # CLASSIFICATION of the windows\n",
    "        c_crops = self.pool(F.relu((self.conv1(feats))))\n",
    "        c_crops = self.pool(F.relu(self.conv2(c_crops)))\n",
    "        c_crops = c_crops.view(-1, self._const)\n",
    "        c_crops = F.relu(self.fc1(c_crops))\n",
    "        c_crops = F.relu(self.fc2(c_crops))\n",
    "        c_crops = self.fc3(c_crops)\n",
    "        c_crops = self.sigmoid(c_crops)\n",
    "\n",
    "        containsObj = c_crops\n",
    "        return containsObj, all_crops, cropCoords\n",
    "\n",
    "    def makeCrops(self, image, stepSize, windowSize):\n",
    "        \"\"\"\n",
    "        Returns a generator of cropped boxes(the top left x, y, the image data)\n",
    "        \"\"\"\n",
    "        image = image.type(torch.FloatTensor).to(device)\n",
    "        crops = []\n",
    "        cropCoords = []\n",
    "\n",
    "        # TODO: look into ordering, why it's y,x !\n",
    "        for y in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "            for x in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "                end_x, end_y = x + windowSize[1], y + windowSize[0]\n",
    "                # print('This is the x and y used: ', x, '; ', y)\n",
    "                crops.append(image[y:end_y, x:end_x])\n",
    "                cropCoords.append(torch.FloatTensor((y, x)))\n",
    "        crops = torch.stack(crops)\n",
    "        cropCoords = torch.stack(cropCoords)\n",
    "        # self.numCrops=len(crops)\n",
    "        return crops, cropCoords\n",
    "\n",
    "\n",
    "# In[6]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Utility fxn -------------------------------------------------------\n",
    "# Source: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/train.py\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Keeps track of most recent, average, sum, and count of a metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "    epoch, epochs_since_improvement, model1, model2, optimizer1, optimizer2, loss, loss2, best_loss, is_best\n",
    "):\n",
    "    \"\"\"\n",
    "    Save model checkpoint.\n",
    "    :param epoch: epoch number\n",
    "    :param epochs_since_improvement: number of epochs since last improvement\n",
    "    :param model: model\n",
    "    :param optimizer: optimizer\n",
    "    :param loss: validation loss in this epoch\n",
    "    :param best_loss: best validation loss achieved so far (not necessarily in this checkpoint)\n",
    "    :param is_best: is this checkpoint the best so far?\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"epochs_since_improvement\": epochs_since_improvement,\n",
    "        \"loss\": loss,\n",
    "        \"loss2\": loss2,\n",
    "        \"best_loss\": best_loss,\n",
    "        \"model1\": model1,\n",
    "        \"model2\": model2,\n",
    "        \"optimizer1\": optimizer1,\n",
    "        \"optimizer2\": optimizer2,\n",
    "    }\n",
    "    filename = \"checkpoint_v2sliding.pth.tar\"\n",
    "    torch.save(state, filename)\n",
    "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
    "    if is_best:\n",
    "        torch.save(state, \"BEST_\" + filename)\n",
    "\n",
    "        \n",
    "\n",
    "def save_checkpoint_small(\n",
    "    epoch, model1, model2, optimizer1, optimizer2):\n",
    "    \"\"\"\n",
    "    Save model checkpoint.\n",
    "    :param epoch: epoch number\n",
    "    :param epochs_since_improvement: number of epochs since last improvement\n",
    "    :param model: model\n",
    "    :param optimizer: optimizer\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"epochs_since_improvement\": epochs_since_improvement,\n",
    "        \"model1\": model1,\n",
    "        \"model2\": model2,\n",
    "        \"optimizer1\": optimizer1,\n",
    "        \"optimizer2\": optimizer2,\n",
    "    }\n",
    "    filename = \"checkpoint_v2sliding.pth.tar\"\n",
    "    torch.save(state, filename)\n",
    "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
    "    if is_best:\n",
    "        torch.save(state, \"BEST_\" + filename)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  --- Define Train and Test functions -------------------------------------------------------\n",
    "\n",
    "def train(train_loader, c_model, r_model, classifCriterion,\n",
    "          regrCriterion, optimizer1, optimizer2, epoch):\n",
    "    \"\"\"\n",
    "    One epoch's training.\n",
    "    : param train_loader: DataLoader for training data\n",
    "    : param model: model\n",
    "    : param criterion: for classification (crop contains an Obj, t/f)\n",
    "    : param criterion: for regresion (of the x,y, theta)\n",
    "    : param optimizer: optimizer\n",
    "    : param epoch: epoch number\n",
    "    \"\"\"\n",
    "\n",
    "    c_model.train()  # training mode enables dropout\n",
    "    r_model.train()  # training mode enables dropout\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
    "    data_time = AverageMeter()  # data loading time\n",
    "    losses = AverageMeter()  # loss\n",
    "    losses2 = AverageMeter()  # loss\n",
    "    start = time.time()\n",
    "\n",
    "    for i_batch, (images, labels, coords) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        coords = coords.to(device)\n",
    "\n",
    "        # CLASSIFICATION\n",
    "        # Forward pass\n",
    "        # predicted_class, predicted_locs = model(images)\n",
    "        predicted_class, all_crops, cropCoords = c_model(images)\n",
    "        all_crops = all_crops.to(device)\n",
    "        cropCoords = cropCoords.to(device)\n",
    "\n",
    "        # print('size of predicted vs labels',\n",
    "        # predicted_class, labels)\n",
    "        # print('size of predicted vs labels',\n",
    "        # predicted_class.size(), labels.size())\n",
    "        loss1 = classifCriterion(predicted_class, labels)\n",
    "        optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "        # Update model\n",
    "        optimizer1.step()\n",
    "\n",
    "        # REGRESSION\n",
    "        # Forward pass\n",
    "        # all crops is of size (batchsize, numcrops, x, y)\n",
    "        # we'll do it batchsize x 1 crop at a time...\n",
    "        # coords = batchsize, numcrops, x,y, theta\n",
    "        for i in range(9):\n",
    "            # print('!-- ', all_crops.size())\n",
    "            batchcrop = all_crops[:, i, :, :]\n",
    "            batchcrop.unsqueeze_(1)\n",
    "            # print('!-- ', batchcrop.size())\n",
    "\n",
    "            # print('!--', cropCoords.size())\n",
    "            offset = cropCoords[i]\n",
    "            # pad with column of zeros - don't touch the theta\n",
    "            # print('!--', offset.size())\n",
    "            offset = offset.repeat(all_crops.size(0), 1)\n",
    "            offset = torch.cat((offset, torch.zeros((all_crops.size(0),\n",
    "                                                     1)).to(device)),\n",
    "                               dim=1)\n",
    "            center_truth = coords[:, i, :]\n",
    "\n",
    "            center_est = r_model(batchcrop).to(device)\n",
    "            # print('!-- ', center_est)\n",
    "            # print('!-- ', offset)\n",
    "            center_est = center_est + offset\n",
    "\n",
    "            loss2 = regrCriterion(center_truth, center_est)\n",
    "            \n",
    "            optimizer2.zero_grad()\n",
    "            loss2.backward()\n",
    "            \n",
    "            optimizer2.step()\n",
    "            \n",
    "            losses2.update(loss2.item())\n",
    "\n",
    "        losses.update(loss1.item())\n",
    "        batch_time.update(time.time() - start)\n",
    "        start = time.time()\n",
    "\n",
    "        # Print status\n",
    "        if i_batch % print_freq == 0:\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}]\\t\"\n",
    "                \"Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(\n",
    "                    epoch, i_batch, len(train_loader), batch_time=batch_time,\n",
    "                    loss=losses,\n",
    "                ),\n",
    "                \"RLoss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(\n",
    "                    epoch, i_batch, len(train_loader), batch_time=batch_time,\n",
    "                    loss=losses2,\n",
    "                ),\n",
    "\n",
    "            )\n",
    "        # free some memory since their histories may be stored\n",
    "        del predicted_class, images, labels, coords\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "\n",
    "def validate(val_loader, c_model, r_model, c_criterion, r_criterion):\n",
    "    \"\"\"\n",
    "    One epoch's validation.\n",
    "    : param val_loader: DataLoader for validation data\n",
    "    : param model: model\n",
    "    : param criterion: MultiBox loss\n",
    "    : return: average validation loss\n",
    "    \"\"\"\n",
    "    c_model.eval()  # eval mode disables dropout\n",
    "    r_model.eval()  # eval mode disables dropout\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses2 = AverageMeter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Prohibit gradient computation explicity because I had some problems with memory\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for i_batch, (images, labels, coords) in enumerate(train_loader):\n",
    "            # Move to default device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            coords = coords.to(device)\n",
    "\n",
    "            # CLASSIFICATION Eval\n",
    "            predicted_class, all_crops, cropCoords = c_model(images)\n",
    "            loss1 = c_criterion(predicted_class, labels)\n",
    "\n",
    "            all_crops = all_crops.to(device)\n",
    "            cropCoords = cropCoords.to(device)\n",
    "\n",
    "            # REGRESSION Eval\n",
    "\n",
    "            for i in range(9):\n",
    "                batchcrop = all_crops[:, i, :, :]\n",
    "                batchcrop.unsqueeze_(1)\n",
    "                offset = cropCoords[i]\n",
    "                offset = offset.repeat(all_crops.size(0), 1)\n",
    "                offset = torch.cat((offset, torch.zeros((all_crops.size(0),\n",
    "                                                         1)).to(device)),\n",
    "                                   dim=1)\n",
    "                center_truth = coords[:, i, :]\n",
    "\n",
    "                center_est = r_model(batchcrop).to(device)\n",
    "                center_est = center_est + offset\n",
    "\n",
    "                loss2 = regrCriterion(center_truth, center_est)\n",
    "                losses2.update(loss2.item())\n",
    "\n",
    "            losses.update(loss1.item())\n",
    "\n",
    "            batch_time.update(time.time() - start)\n",
    "            start = time.time()\n",
    "\n",
    "            # Print status\n",
    "            if i_batch % print_freq == 0:\n",
    "                print(\n",
    "                    \"[{0}/{1}]\\t\"\n",
    "                    \"Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                    \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(i_batch, len(val_loader),\n",
    "                                                                    batch_time=batch_time, loss=losses),\n",
    "                    \"Regr Loss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(i_batch, len(val_loader),\n",
    "                                                                         batch_time=batch_time, loss=losses2)\n",
    "                )\n",
    "\n",
    "    print(\"\\n * LOSS - {loss.avg:.3f}\\n\".format(loss=losses))\n",
    "    print(\" * REGR LOSS - {loss.avg:.3f}\\n\".format(loss=losses2))\n",
    "\n",
    "    return losses.avg, losses2.avg\n",
    "\n",
    "\n",
    "# In[9]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Load data -------------------------------------------------------\n",
    "\n",
    "batch_size = 15\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA available? device: \", device)\n",
    "\n",
    "\n",
    "# Dataset is depth images of rectangular blocks\n",
    "train_dataset = RectDepthImgsDataset(img_dir=\"./data\", coords=train_truth)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = RectDepthImgsDataset(img_dir=\"./data/test\", coords=test_truth)\n",
    "\n",
    "# Data loader\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# -- Hyperparamaters -------------------------\n",
    "\n",
    "num_epochs = 80  # number of epochs to run without early-stopping\n",
    "learning_rate = 0.001\n",
    "\n",
    "start_epoch = 0  # start at this epoch\n",
    "# number of epochs since there was an improvement in the validation metric\n",
    "epochs_since_improvement = 0\n",
    "best_loss = 1000.0  # assume a high loss at first\n",
    "workers = 4  # number of workers for loading data in the DataLoader\n",
    "\n",
    "classifModel = classifNet(IMG_X, IMG_Y)\n",
    "classifModel = classifModel.to(device)\n",
    "\n",
    "regrModel = regrNet((100, 100), 3)  # crop size in pixels; output x,y, theta\n",
    "regrModel = regrModel.to(device)\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "classifCriterion = nn.BCELoss()\n",
    "regrCriterion = nn.MSELoss()\n",
    "#regrCriterion = nn.SmoothL1Loss()\n",
    "\n",
    "optimizer1 = torch.optim.Adam(classifModel.parameters(), lr=learning_rate)\n",
    "optimizer2 = torch.optim.Adam(regrModel.parameters(), lr=learning_rate)\n",
    "\n",
    "print_freq = 25  # print training or validation status every __ batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Hyperparamaters -------------------------\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Training and validation.\n",
    "    \"\"\"\n",
    "\n",
    "    global epochs_since_improvement, start_epoch, best_loss, epoch, checkpoint\n",
    "\n",
    "    print(\"Training model now...\")\n",
    "\n",
    "    # -- Begin training -------------------------\n",
    "    for epoch in range(num_epochs):\n",
    "        train(\n",
    "            train_loader=train_loader,\n",
    "            c_model=classifModel,\n",
    "            r_model=regrModel,\n",
    "            classifCriterion=classifCriterion,\n",
    "            regrCriterion=regrCriterion,\n",
    "            optimizer1=optimizer1,\n",
    "            optimizer2=optimizer2,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "        # Save checkpoint\n",
    "        save_checkpoint_small(epoch, classifModel, regrModel, optimizer1,\n",
    "                    optimizer2)\n",
    "        \n",
    "        \n",
    "\n",
    "    # One epoch's validation\n",
    "    val_loss, regr_loss = validate(val_loader=test_loader,\n",
    "                                   c_model=classifModel, r_model=regrModel,\n",
    "                                   c_criterion=classifCriterion,\n",
    "                                   r_criterion=regrCriterion)\n",
    "\n",
    "    # Did validation loss improve?\n",
    "    is_best = val_loss < best_loss\n",
    "    best_loss = min(val_loss, best_loss)\n",
    "\n",
    "    if not is_best:\n",
    "        epochs_since_improvement += 1\n",
    "        print(\"\\nEpochs since last improvement: %d\\n\" %\n",
    "              (epochs_since_improvement,))\n",
    "\n",
    "    else:\n",
    "        epochs_since_improvement = 0\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(epoch, epochs_since_improvement, classifModel, regrModel, optimizer1,\n",
    "                    optimizer2, val_loss, regr_loss, best_loss, is_best)\n",
    "    # save_checkpoint(epoch, epochs_since_improvement, classifModel, optimizer1,\n",
    "    #                val_loss, best_loss, is_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model now...\n",
      "Epoch: [0][0/334]\tBatch Time 0.057 (0.057)\tLoss 0.0001 (0.0001)\t RLoss 44783.5938 (39397.8576)\t\n",
      "Epoch: [0][25/334]\tBatch Time 0.043 (0.044)\tLoss 0.0001 (0.0024)\t RLoss 49432.9570 (40187.9061)\t\n",
      "Epoch: [0][50/334]\tBatch Time 0.043 (0.044)\tLoss 0.0001 (0.0026)\t RLoss 50143.5703 (40759.1064)\t\n",
      "Epoch: [0][75/334]\tBatch Time 0.043 (0.044)\tLoss 0.0128 (0.0026)\t RLoss 52549.4375 (41189.4341)\t\n",
      "Epoch: [0][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0020 (0.0021)\t RLoss 37134.6055 (41265.6861)\t\n",
      "Epoch: [0][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0017)\t RLoss 43767.8164 (41142.0097)\t\n",
      "Epoch: [0][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0014)\t RLoss 50310.9453 (41115.8816)\t\n",
      "Epoch: [0][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0014)\t RLoss 53297.8203 (41102.4725)\t\n",
      "Epoch: [0][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 45202.8086 (41123.7743)\t\n",
      "Epoch: [0][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0011)\t RLoss 45124.0664 (41174.1352)\t\n",
      "Epoch: [0][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0014)\t RLoss 42702.4492 (41236.2086)\t\n",
      "Epoch: [0][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0013)\t RLoss 46273.5117 (41262.0620)\t\n",
      "Epoch: [0][300/334]\tBatch Time 0.044 (0.043)\tLoss 0.0001 (0.0012)\t RLoss 48660.5273 (41316.6660)\t\n",
      "Epoch: [0][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 48797.4805 (41253.2793)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0006 (0.0006)\t Regr Loss 1086.1505 (842.7384)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0011)\t Regr Loss 1043.5897 (922.1065)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0020)\t Regr Loss 908.8359 (923.9251)\t\n",
      "[75/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0014)\t Regr Loss 1035.0377 (918.9850)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0014)\t Regr Loss 1059.1896 (915.0093)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0070 (0.0015)\t Regr Loss 1070.1710 (914.7052)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0005 (0.0017)\t Regr Loss 835.3495 (918.0576)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.0075 (0.0015)\t Regr Loss 734.3964 (917.9640)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0014)\t Regr Loss 1121.1615 (916.3050)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 1007.9363 (916.1080)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 1015.2506 (916.1640)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 1154.7715 (917.6373)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.0006 (0.0014)\t Regr Loss 1098.7238 (917.4600)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 1097.4003 (917.5600)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.362\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n",
      "Epoch: [1][0/334]\tBatch Time 0.044 (0.044)\tLoss 0.0005 (0.0005)\t RLoss 54823.8242 (41087.3156)\t\n",
      "Epoch: [1][25/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0002)\t RLoss 44906.6289 (41013.0438)\t\n",
      "Epoch: [1][50/334]\tBatch Time 0.042 (0.043)\tLoss 0.0000 (0.0003)\t RLoss 48782.7070 (41132.7859)\t\n",
      "Epoch: [1][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0006)\t RLoss 44953.1484 (41004.7001)\t\n",
      "Epoch: [1][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0009)\t RLoss 52893.4844 (41012.2249)\t\n",
      "Epoch: [1][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0011)\t RLoss 52687.8281 (41002.1490)\t\n",
      "Epoch: [1][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0009)\t RLoss 47217.3125 (41034.9311)\t\n",
      "Epoch: [1][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0266 (0.0013)\t RLoss 46127.0898 (41197.4220)\t\n",
      "Epoch: [1][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0012)\t RLoss 48765.7891 (41155.1688)\t\n",
      "Epoch: [1][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 44469.9414 (41225.0296)\t\n",
      "Epoch: [1][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0012)\t RLoss 38724.0820 (41190.0955)\t\n",
      "Epoch: [1][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 45373.7773 (41225.1915)\t\n",
      "Epoch: [1][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 45489.0703 (41207.1373)\t\n",
      "Epoch: [1][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 49102.1250 (41264.2454)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0402 (0.0402)\t Regr Loss 872.6544 (940.3458)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0057)\t Regr Loss 1212.1534 (917.1626)\t\n",
      "[50/20]\tBatch Time 0.027 (0.026)\tLoss 0.0000 (0.0051)\t Regr Loss 1074.7483 (921.1169)\t\n",
      "[75/20]\tBatch Time 0.026 (0.026)\tLoss 0.0000 (0.0036)\t Regr Loss 1002.0954 (920.3748)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0028)\t Regr Loss 801.0011 (915.9851)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0003 (0.0023)\t Regr Loss 855.4164 (918.2409)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0019)\t Regr Loss 1042.7924 (915.1043)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.0010 (0.0017)\t Regr Loss 1090.3453 (914.6266)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0015)\t Regr Loss 1241.0045 (915.6281)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0004 (0.0015)\t Regr Loss 881.3193 (914.5531)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0015)\t Regr Loss 919.1505 (913.3014)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 927.8741 (913.8991)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0013)\t Regr Loss 986.8425 (915.5537)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 1156.6393 (916.9238)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.462\n",
      "\n",
      "\n",
      "Epochs since last improvement: 5\n",
      "\n",
      "Epoch: [2][0/334]\tBatch Time 0.046 (0.046)\tLoss 0.0000 (0.0000)\t RLoss 49215.4570 (40296.7390)\t\n",
      "Epoch: [2][25/334]\tBatch Time 0.043 (0.044)\tLoss 0.0002 (0.0004)\t RLoss 48553.3594 (41264.6343)\t\n",
      "Epoch: [2][50/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0008)\t RLoss 47944.5352 (41729.9870)\t\n",
      "Epoch: [2][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0006)\t RLoss 30413.7188 (41716.7544)\t\n",
      "Epoch: [2][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0005)\t RLoss 45216.1445 (41712.8478)\t\n",
      "Epoch: [2][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0011)\t RLoss 43832.4023 (41644.7029)\t\n",
      "Epoch: [2][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0015)\t RLoss 29986.4160 (41487.7176)\t\n",
      "Epoch: [2][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0016)\t RLoss 46100.2227 (41490.0575)\t\n",
      "Epoch: [2][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0076 (0.0015)\t RLoss 39793.1289 (41438.4583)\t\n",
      "Epoch: [2][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0013)\t RLoss 51365.7734 (41369.2296)\t\n",
      "Epoch: [2][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0015)\t RLoss 44212.8633 (41348.7350)\t\n",
      "Epoch: [2][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0014)\t RLoss 47276.1211 (41308.7459)\t\n",
      "Epoch: [2][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0014)\t RLoss 50210.6562 (41302.3236)\t\n",
      "Epoch: [2][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 42405.2500 (41236.6036)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0001)\t Regr Loss 842.4117 (837.4186)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0006)\t Regr Loss 960.7851 (902.4845)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.0009 (0.0006)\t Regr Loss 1327.9036 (907.7283)\t\n",
      "[75/20]\tBatch Time 0.027 (0.027)\tLoss 0.0128 (0.0009)\t Regr Loss 1102.9626 (909.9225)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0069 (0.0012)\t Regr Loss 1097.4851 (915.0337)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0010)\t Regr Loss 1183.5040 (914.2232)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0009)\t Regr Loss 860.7263 (914.8772)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0008)\t Regr Loss 1005.3228 (914.5824)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0008)\t Regr Loss 959.6727 (913.8419)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0007)\t Regr Loss 871.2847 (918.0211)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0010)\t Regr Loss 1029.7261 (919.7073)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0012)\t Regr Loss 943.5222 (917.6127)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0012)\t Regr Loss 1127.8109 (917.5417)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 1217.5811 (916.7904)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.435\n",
      "\n",
      "\n",
      "Epochs since last improvement: 6\n",
      "\n",
      "Epoch: [3][0/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0000)\t RLoss 42384.2695 (38961.2589)\t\n",
      "Epoch: [3][25/334]\tBatch Time 0.044 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 37493.9766 (41397.1092)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][50/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0018)\t RLoss 45277.9492 (41417.8852)\t\n",
      "Epoch: [3][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0017)\t RLoss 42715.4766 (41471.0820)\t\n",
      "Epoch: [3][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0018)\t RLoss 55156.9922 (41225.1805)\t\n",
      "Epoch: [3][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0020)\t RLoss 50590.1016 (41242.2999)\t\n",
      "Epoch: [3][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0019)\t RLoss 42613.8594 (41360.2187)\t\n",
      "Epoch: [3][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0018)\t RLoss 46863.5820 (41257.4040)\t\n",
      "Epoch: [3][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0016)\t RLoss 50841.2070 (41256.3135)\t\n",
      "Epoch: [3][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0015)\t RLoss 31951.0254 (41305.0104)\t\n",
      "Epoch: [3][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0014)\t RLoss 41769.7734 (41261.5089)\t\n",
      "Epoch: [3][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 53188.2578 (41235.5124)\t\n",
      "Epoch: [3][300/334]\tBatch Time 0.044 (0.043)\tLoss 0.0001 (0.0013)\t RLoss 41706.0664 (41314.1614)\t\n",
      "Epoch: [3][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0013)\t RLoss 41803.7188 (41250.9687)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0001)\t Regr Loss 1100.9028 (1017.3814)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0024)\t Regr Loss 509.4925 (895.5033)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0025)\t Regr Loss 1184.1886 (903.9719)\t\n",
      "[75/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0026)\t Regr Loss 992.0243 (906.7824)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0021)\t Regr Loss 1090.9587 (918.5757)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0021)\t Regr Loss 1031.7911 (914.2719)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0019)\t Regr Loss 1032.1635 (915.6657)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0017)\t Regr Loss 1247.0265 (915.4531)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0018)\t Regr Loss 873.8693 (914.4247)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0016)\t Regr Loss 1088.8118 (916.2383)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0003 (0.0015)\t Regr Loss 763.3939 (917.0611)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0014)\t Regr Loss 957.0931 (917.1183)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 1212.2524 (915.9633)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 810.4564 (916.4740)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.365\n",
      "\n",
      "\n",
      "Epochs since last improvement: 7\n",
      "\n",
      "Epoch: [4][0/334]\tBatch Time 0.045 (0.045)\tLoss 0.0000 (0.0000)\t RLoss 47663.8281 (36568.2476)\t\n",
      "Epoch: [4][25/334]\tBatch Time 0.043 (0.044)\tLoss 0.0006 (0.0002)\t RLoss 54046.0742 (41635.8504)\t\n",
      "Epoch: [4][50/334]\tBatch Time 0.044 (0.044)\tLoss 0.0672 (0.0019)\t RLoss 47184.7461 (41684.4058)\t\n",
      "Epoch: [4][75/334]\tBatch Time 0.043 (0.044)\tLoss 0.0000 (0.0016)\t RLoss 42716.8945 (41623.9058)\t\n",
      "Epoch: [4][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0012)\t RLoss 48086.9609 (41461.2211)\t\n",
      "Epoch: [4][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0012)\t RLoss 46893.1953 (41440.9864)\t\n",
      "Epoch: [4][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0004 (0.0010)\t RLoss 52432.0156 (41345.0980)\t\n",
      "Epoch: [4][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0003 (0.0009)\t RLoss 49895.6055 (41325.8561)\t\n",
      "Epoch: [4][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0010)\t RLoss 59496.5391 (41316.6764)\t\n",
      "Epoch: [4][225/334]\tBatch Time 0.044 (0.043)\tLoss 0.0000 (0.0011)\t RLoss 46416.5938 (41208.7707)\t\n",
      "Epoch: [4][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0011)\t RLoss 47195.7812 (41274.2443)\t\n",
      "Epoch: [4][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 41444.7383 (41317.3771)\t\n",
      "Epoch: [4][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0012)\t RLoss 36478.2383 (41275.9713)\t\n",
      "Epoch: [4][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 51144.2617 (41246.1319)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0006 (0.0006)\t Regr Loss 676.2220 (886.4563)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0006)\t Regr Loss 1048.3839 (910.8424)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.0010 (0.0006)\t Regr Loss 883.7709 (911.8711)\t\n",
      "[75/20]\tBatch Time 0.026 (0.027)\tLoss 0.0000 (0.0021)\t Regr Loss 1072.6559 (914.9229)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0017)\t Regr Loss 1030.5751 (911.0157)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0016)\t Regr Loss 916.4724 (912.7392)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0015)\t Regr Loss 954.1357 (914.1742)\t\n",
      "[175/20]\tBatch Time 0.026 (0.027)\tLoss 0.0004 (0.0013)\t Regr Loss 968.5344 (912.8726)\t\n",
      "[200/20]\tBatch Time 0.026 (0.027)\tLoss 0.0000 (0.0012)\t Regr Loss 862.0113 (915.1288)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0011)\t Regr Loss 1156.7074 (917.1064)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0012)\t Regr Loss 1199.6401 (916.7196)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 978.9224 (917.5248)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0013)\t Regr Loss 956.8109 (918.4766)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0006 (0.0012)\t Regr Loss 1193.1731 (917.5245)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.335\n",
      "\n",
      "\n",
      "Epochs since last improvement: 8\n",
      "\n",
      "Epoch: [5][0/334]\tBatch Time 0.044 (0.044)\tLoss 0.0001 (0.0001)\t RLoss 47400.9570 (41021.0591)\t\n",
      "Epoch: [5][25/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0022)\t RLoss 37932.9844 (40371.2883)\t\n",
      "Epoch: [5][50/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0012)\t RLoss 49135.3984 (40501.6781)\t\n",
      "Epoch: [5][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0010)\t RLoss 45344.0156 (41028.8432)\t\n",
      "Epoch: [5][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 46175.8516 (40886.1788)\t\n",
      "Epoch: [5][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0011)\t RLoss 48229.3906 (40872.6385)\t\n",
      "Epoch: [5][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0004 (0.0009)\t RLoss 43154.1602 (40956.4353)\t\n",
      "Epoch: [5][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0008)\t RLoss 42973.8711 (40961.7082)\t\n",
      "Epoch: [5][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0007)\t RLoss 47346.3125 (41080.5952)\t\n",
      "Epoch: [5][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0011)\t RLoss 29274.2676 (41081.7522)\t\n",
      "Epoch: [5][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 50387.7227 (41165.8408)\t\n",
      "Epoch: [5][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 41413.7461 (41228.4117)\t\n",
      "Epoch: [5][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 54486.8906 (41256.9092)\t\n",
      "Epoch: [5][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 50478.8594 (41249.1969)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0002)\t Regr Loss 874.0825 (919.0244)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0004)\t Regr Loss 1043.3097 (915.3373)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0003)\t Regr Loss 1225.5391 (909.0767)\t\n",
      "[75/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0007)\t Regr Loss 1092.6339 (918.6376)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 1281.4785 (919.3976)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0011)\t Regr Loss 1104.8552 (918.7528)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0011)\t Regr Loss 1039.4443 (915.0363)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0012)\t Regr Loss 751.7958 (912.5188)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 0.0129 (0.0013)\t Regr Loss 1158.9949 (914.3310)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 934.5617 (914.7930)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0012)\t Regr Loss 1098.7738 (915.3945)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0012)\t Regr Loss 774.3310 (914.9344)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.0006 (0.0012)\t Regr Loss 1083.1665 (914.5189)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0012)\t Regr Loss 1150.5822 (915.1933)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.490\n",
      "\n",
      "\n",
      "Epochs since last improvement: 9\n",
      "\n",
      "Epoch: [6][0/334]\tBatch Time 0.045 (0.045)\tLoss 0.0002 (0.0002)\t RLoss 40463.5078 (42440.2535)\t\n",
      "Epoch: [6][25/334]\tBatch Time 0.043 (0.044)\tLoss 0.0000 (0.0012)\t RLoss 43893.8750 (41014.9553)\t\n",
      "Epoch: [6][50/334]\tBatch Time 0.043 (0.044)\tLoss 0.0008 (0.0007)\t RLoss 38328.5508 (41171.7835)\t\n",
      "Epoch: [6][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0013)\t RLoss 46716.0625 (41327.0542)\t\n",
      "Epoch: [6][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0010)\t RLoss 48803.9023 (41164.9927)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0010)\t RLoss 59755.6641 (41282.5627)\t\n",
      "Epoch: [6][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0010)\t RLoss 46736.3672 (41308.5775)\t\n",
      "Epoch: [6][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0009)\t RLoss 38389.1992 (41317.2803)\t\n",
      "Epoch: [6][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 45700.2148 (41325.0595)\t\n",
      "Epoch: [6][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0011)\t RLoss 46377.1172 (41326.7555)\t\n",
      "Epoch: [6][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0010)\t RLoss 41559.4766 (41335.8436)\t\n",
      "Epoch: [6][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0010)\t RLoss 48355.2539 (41263.0394)\t\n",
      "Epoch: [6][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 43366.5273 (41249.7917)\t\n",
      "Epoch: [6][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0013)\t RLoss 35690.9844 (41256.7499)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0001)\t Regr Loss 1002.5750 (1041.7709)\t\n",
      "[25/20]\tBatch Time 0.026 (0.026)\tLoss 0.0005 (0.0017)\t Regr Loss 1256.0436 (904.6695)\t\n",
      "[50/20]\tBatch Time 0.026 (0.026)\tLoss 0.0000 (0.0009)\t Regr Loss 739.4132 (902.3403)\t\n",
      "[75/20]\tBatch Time 0.026 (0.026)\tLoss 0.0002 (0.0011)\t Regr Loss 935.9808 (909.9737)\t\n",
      "[100/20]\tBatch Time 0.027 (0.026)\tLoss 0.0000 (0.0009)\t Regr Loss 987.3501 (915.1937)\t\n",
      "[125/20]\tBatch Time 0.027 (0.026)\tLoss 0.0000 (0.0009)\t Regr Loss 1013.9260 (917.3990)\t\n",
      "[150/20]\tBatch Time 0.026 (0.026)\tLoss 0.0000 (0.0008)\t Regr Loss 1122.2540 (919.6256)\t\n",
      "[175/20]\tBatch Time 0.026 (0.026)\tLoss 0.0000 (0.0008)\t Regr Loss 1196.2703 (917.6026)\t\n",
      "[200/20]\tBatch Time 0.027 (0.026)\tLoss 0.0000 (0.0009)\t Regr Loss 1009.1830 (917.4587)\t\n",
      "[225/20]\tBatch Time 0.026 (0.026)\tLoss 0.0068 (0.0010)\t Regr Loss 798.3281 (917.6481)\t\n",
      "[250/20]\tBatch Time 0.026 (0.026)\tLoss 0.0000 (0.0012)\t Regr Loss 918.0663 (917.1765)\t\n",
      "[275/20]\tBatch Time 0.027 (0.026)\tLoss 0.0000 (0.0012)\t Regr Loss 997.7406 (916.5491)\t\n",
      "[300/20]\tBatch Time 0.026 (0.026)\tLoss 0.0004 (0.0012)\t Regr Loss 1043.6913 (917.4618)\t\n",
      "[325/20]\tBatch Time 0.026 (0.026)\tLoss 0.0000 (0.0013)\t Regr Loss 954.8763 (916.2791)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.452\n",
      "\n",
      "\n",
      "Epochs since last improvement: 10\n",
      "\n",
      "Epoch: [7][0/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0000)\t RLoss 35084.3203 (43263.5047)\t\n",
      "Epoch: [7][25/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0012)\t RLoss 51804.2578 (41208.9310)\t\n",
      "Epoch: [7][50/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0012)\t RLoss 54816.1172 (40627.9025)\t\n",
      "Epoch: [7][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0014)\t RLoss 44261.2617 (40946.9287)\t\n",
      "Epoch: [7][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0014)\t RLoss 42118.7656 (40832.0032)\t\n",
      "Epoch: [7][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 38599.5977 (40878.7978)\t\n",
      "Epoch: [7][150/334]\tBatch Time 0.044 (0.043)\tLoss 0.0007 (0.0012)\t RLoss 48446.6172 (40945.3133)\t\n",
      "Epoch: [7][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0015)\t RLoss 57372.0312 (41001.7960)\t\n",
      "Epoch: [7][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 45646.6367 (40974.1352)\t\n",
      "Epoch: [7][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0013)\t RLoss 54414.5469 (41124.9376)\t\n",
      "Epoch: [7][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0014)\t RLoss 47316.6328 (41175.0236)\t\n",
      "Epoch: [7][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0014)\t RLoss 48688.6445 (41181.0731)\t\n",
      "Epoch: [7][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0013)\t RLoss 38305.1406 (41178.8902)\t\n",
      "Epoch: [7][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0161 (0.0013)\t RLoss 42031.4414 (41233.1932)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0000)\t Regr Loss 898.3888 (964.4798)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0002)\t Regr Loss 956.1819 (912.7816)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.0006 (0.0002)\t Regr Loss 1126.7052 (912.8153)\t\n",
      "[75/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0011)\t Regr Loss 889.0241 (909.0582)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0009)\t Regr Loss 875.1196 (912.3635)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 1129.4597 (911.9696)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0015)\t Regr Loss 1052.1029 (913.5893)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.0003 (0.0016)\t Regr Loss 889.1976 (914.7629)\t\n",
      "[200/20]\tBatch Time 0.031 (0.027)\tLoss 0.0000 (0.0016)\t Regr Loss 1042.9537 (912.7143)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0014)\t Regr Loss 994.5889 (913.6142)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0004 (0.0013)\t Regr Loss 1086.3546 (913.3145)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0003 (0.0013)\t Regr Loss 1132.2052 (914.9654)\t\n",
      "[300/20]\tBatch Time 0.032 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 1139.7103 (915.3308)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0013)\t Regr Loss 952.0150 (915.7593)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.719\n",
      "\n",
      "\n",
      "Epochs since last improvement: 11\n",
      "\n",
      "Epoch: [8][0/334]\tBatch Time 0.046 (0.046)\tLoss 0.0000 (0.0000)\t RLoss 39639.2266 (38301.0996)\t\n",
      "Epoch: [8][25/334]\tBatch Time 0.043 (0.044)\tLoss 0.0000 (0.0034)\t RLoss 45055.1953 (42441.0786)\t\n",
      "Epoch: [8][50/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0024)\t RLoss 40613.1016 (41410.5159)\t\n",
      "Epoch: [8][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0018)\t RLoss 45217.9883 (41322.0140)\t\n",
      "Epoch: [8][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0020)\t RLoss 60972.7070 (41134.4883)\t\n",
      "Epoch: [8][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0003 (0.0016)\t RLoss 39359.9258 (41268.8531)\t\n",
      "Epoch: [8][150/334]\tBatch Time 0.044 (0.043)\tLoss 0.0000 (0.0016)\t RLoss 42357.5703 (41177.6456)\t\n",
      "Epoch: [8][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0015)\t RLoss 43565.0391 (41263.6200)\t\n",
      "Epoch: [8][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0020 (0.0014)\t RLoss 42080.3633 (41366.5801)\t\n",
      "Epoch: [8][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 45398.3828 (41441.4246)\t\n",
      "Epoch: [8][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0015)\t RLoss 54620.6367 (41329.6282)\t\n",
      "Epoch: [8][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0014)\t RLoss 54367.9727 (41266.9649)\t\n",
      "Epoch: [8][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0013)\t RLoss 47388.8711 (41223.6296)\t\n",
      "Epoch: [8][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 52368.3594 (41277.5181)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0000)\t Regr Loss 1052.2689 (915.3653)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0002)\t Regr Loss 1008.6811 (919.1959)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0014)\t Regr Loss 1157.0778 (919.9083)\t\n",
      "[75/20]\tBatch Time 0.026 (0.027)\tLoss 0.0005 (0.0010)\t Regr Loss 925.5485 (910.2290)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0011)\t Regr Loss 1130.6650 (913.8874)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0011)\t Regr Loss 806.8646 (915.5971)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0010)\t Regr Loss 790.7239 (917.1461)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0011)\t Regr Loss 973.3868 (916.9300)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0012)\t Regr Loss 1100.3679 (915.2849)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0010)\t Regr Loss 1100.7617 (916.7270)\t\n",
      "[250/20]\tBatch Time 0.026 (0.027)\tLoss 0.0000 (0.0011)\t Regr Loss 924.4349 (918.6466)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0003 (0.0012)\t Regr Loss 958.8383 (918.4837)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0012)\t Regr Loss 986.9500 (916.9579)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0013)\t Regr Loss 921.3740 (916.7222)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.430\n",
      "\n",
      "\n",
      "Epochs since last improvement: 12\n",
      "\n",
      "Epoch: [9][0/334]\tBatch Time 0.044 (0.044)\tLoss 0.0000 (0.0000)\t RLoss 57313.8047 (38838.6463)\t\n",
      "Epoch: [9][25/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0014)\t RLoss 50004.9531 (41209.3137)\t\n",
      "Epoch: [9][50/334]\tBatch Time 0.044 (0.043)\tLoss 0.0003 (0.0015)\t RLoss 49095.8750 (41966.0852)\t\n",
      "Epoch: [9][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0010)\t RLoss 45799.6523 (41652.3278)\t\n",
      "Epoch: [9][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0011)\t RLoss 46699.1797 (41483.7939)\t\n",
      "Epoch: [9][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0004 (0.0010)\t RLoss 43191.1992 (41296.0107)\t\n",
      "Epoch: [9][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0008)\t RLoss 42621.9141 (41210.4418)\t\n",
      "Epoch: [9][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0009)\t RLoss 38726.9961 (41189.1899)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0008)\t RLoss 50379.0078 (41233.0102)\t\n",
      "Epoch: [9][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0011)\t RLoss 50265.3242 (41224.8178)\t\n",
      "Epoch: [9][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 49354.0117 (41262.5472)\t\n",
      "Epoch: [9][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0010 (0.0011)\t RLoss 50142.0742 (41233.7138)\t\n",
      "Epoch: [9][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 52719.3320 (41190.0544)\t\n",
      "Epoch: [9][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0013)\t RLoss 40323.8438 (41211.8350)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0000)\t Regr Loss 872.9604 (959.3231)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0037)\t Regr Loss 771.0143 (915.9419)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.0004 (0.0020)\t Regr Loss 1040.5995 (921.6694)\t\n",
      "[75/20]\tBatch Time 0.027 (0.027)\tLoss 0.0007 (0.0014)\t Regr Loss 1130.5649 (924.8327)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0011)\t Regr Loss 1093.1064 (924.0297)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0010)\t Regr Loss 895.2391 (922.5353)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 1125.9229 (919.9273)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.0003 (0.0015)\t Regr Loss 982.8040 (917.9613)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 1112.5969 (919.8749)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0058 (0.0014)\t Regr Loss 1034.3505 (920.8562)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 919.3978 (920.6379)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0014)\t Regr Loss 822.9429 (919.2520)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.0069 (0.0013)\t Regr Loss 1077.2528 (917.0446)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 975.8076 (917.6805)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.340\n",
      "\n",
      "\n",
      "Epochs since last improvement: 13\n",
      "\n",
      "Epoch: [10][0/334]\tBatch Time 0.045 (0.045)\tLoss 0.0000 (0.0000)\t RLoss 45840.4961 (37561.9689)\t\n",
      "Epoch: [10][25/334]\tBatch Time 0.044 (0.044)\tLoss 0.0000 (0.0002)\t RLoss 45108.8711 (40939.7360)\t\n",
      "Epoch: [10][50/334]\tBatch Time 0.043 (0.044)\tLoss 0.0003 (0.0004)\t RLoss 49359.2773 (40917.8758)\t\n",
      "Epoch: [10][75/334]\tBatch Time 0.043 (0.044)\tLoss 0.0001 (0.0009)\t RLoss 45608.0742 (41010.1299)\t\n",
      "Epoch: [10][100/334]\tBatch Time 0.043 (0.044)\tLoss 0.0001 (0.0007)\t RLoss 43027.9570 (40984.3539)\t\n",
      "Epoch: [10][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0007)\t RLoss 48845.1250 (41117.0158)\t\n",
      "Epoch: [10][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0006)\t RLoss 48203.4414 (41086.6034)\t\n",
      "Epoch: [10][175/334]\tBatch Time 0.044 (0.043)\tLoss 0.0002 (0.0014)\t RLoss 38148.5547 (40939.9306)\t\n",
      "Epoch: [10][200/334]\tBatch Time 0.044 (0.043)\tLoss 0.0011 (0.0013)\t RLoss 48386.3711 (41117.9362)\t\n",
      "Epoch: [10][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0013)\t RLoss 51921.6562 (41137.0179)\t\n",
      "Epoch: [10][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0005 (0.0012)\t RLoss 41916.5898 (41106.7991)\t\n",
      "Epoch: [10][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0396 (0.0014)\t RLoss 46650.8047 (41185.2681)\t\n",
      "Epoch: [10][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0014)\t RLoss 40773.1367 (41245.3127)\t\n",
      "Epoch: [10][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0008 (0.0013)\t RLoss 46811.3359 (41221.7774)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0008 (0.0008)\t Regr Loss 1037.9025 (955.0317)\t\n",
      "[25/20]\tBatch Time 0.026 (0.027)\tLoss 0.0008 (0.0002)\t Regr Loss 845.3653 (936.7735)\t\n",
      "[50/20]\tBatch Time 0.026 (0.026)\tLoss 0.0000 (0.0003)\t Regr Loss 1032.0077 (924.3914)\t\n",
      "[75/20]\tBatch Time 0.026 (0.026)\tLoss 0.0000 (0.0003)\t Regr Loss 1011.9030 (919.2965)\t\n",
      "[100/20]\tBatch Time 0.026 (0.026)\tLoss 0.0000 (0.0002)\t Regr Loss 1217.4292 (917.7520)\t\n",
      "[125/20]\tBatch Time 0.026 (0.026)\tLoss 0.0001 (0.0006)\t Regr Loss 1019.3923 (913.3047)\t\n",
      "[150/20]\tBatch Time 0.026 (0.026)\tLoss 0.0075 (0.0006)\t Regr Loss 967.5081 (913.6628)\t\n",
      "[175/20]\tBatch Time 0.027 (0.026)\tLoss 0.0431 (0.0010)\t Regr Loss 989.2541 (917.3870)\t\n",
      "[200/20]\tBatch Time 0.026 (0.026)\tLoss 0.0001 (0.0012)\t Regr Loss 860.6881 (918.6160)\t\n",
      "[225/20]\tBatch Time 0.026 (0.026)\tLoss 0.0000 (0.0012)\t Regr Loss 1076.2303 (919.1496)\t\n",
      "[250/20]\tBatch Time 0.027 (0.026)\tLoss 0.0000 (0.0012)\t Regr Loss 765.8820 (916.4926)\t\n",
      "[275/20]\tBatch Time 0.026 (0.026)\tLoss 0.0000 (0.0011)\t Regr Loss 1003.1729 (917.3621)\t\n",
      "[300/20]\tBatch Time 0.026 (0.026)\tLoss 0.0008 (0.0013)\t Regr Loss 911.6754 (916.6714)\t\n",
      "[325/20]\tBatch Time 0.026 (0.026)\tLoss 0.0000 (0.0013)\t Regr Loss 1102.0695 (916.5861)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 917.150\n",
      "\n",
      "\n",
      "Epochs since last improvement: 14\n",
      "\n",
      "Epoch: [11][0/334]\tBatch Time 0.044 (0.044)\tLoss 0.0001 (0.0001)\t RLoss 42292.6094 (39756.7867)\t\n",
      "Epoch: [11][25/334]\tBatch Time 0.043 (0.043)\tLoss 0.0011 (0.0001)\t RLoss 39767.1367 (40479.0091)\t\n",
      "Epoch: [11][50/334]\tBatch Time 0.043 (0.043)\tLoss 0.0008 (0.0005)\t RLoss 42741.6758 (40355.0111)\t\n",
      "Epoch: [11][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0005)\t RLoss 44897.5469 (40559.7456)\t\n",
      "Epoch: [11][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0003 (0.0007)\t RLoss 48419.0430 (40876.0761)\t\n",
      "Epoch: [11][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0011)\t RLoss 43279.7188 (40932.5115)\t\n",
      "Epoch: [11][150/334]\tBatch Time 0.044 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 49261.0195 (40979.9264)\t\n",
      "Epoch: [11][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0012)\t RLoss 47144.4961 (41042.2889)\t\n",
      "Epoch: [11][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0005 (0.0013)\t RLoss 46925.6367 (41179.3180)\t\n",
      "Epoch: [11][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0015)\t RLoss 48169.1172 (41253.3021)\t\n",
      "Epoch: [11][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0014)\t RLoss 32006.7676 (41199.4103)\t\n",
      "Epoch: [11][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0013)\t RLoss 55356.2148 (41245.6233)\t\n",
      "Epoch: [11][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 40632.5703 (41275.6890)\t\n",
      "Epoch: [11][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 45126.2734 (41267.3738)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0000)\t Regr Loss 760.9016 (985.6910)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0006)\t Regr Loss 847.2173 (898.5419)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0007)\t Regr Loss 1267.9596 (908.1368)\t\n",
      "[75/20]\tBatch Time 0.027 (0.027)\tLoss 0.0397 (0.0016)\t Regr Loss 1017.4681 (910.3525)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0008 (0.0013)\t Regr Loss 948.1354 (909.6897)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0012)\t Regr Loss 690.4124 (909.2358)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0003 (0.0010)\t Regr Loss 1177.2874 (913.2829)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0010)\t Regr Loss 791.6480 (916.5854)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0010)\t Regr Loss 698.6378 (916.3967)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0256 (0.0010)\t Regr Loss 861.5922 (917.9026)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0011)\t Regr Loss 1214.4552 (916.2362)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0011)\t Regr Loss 1014.3538 (917.8365)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.0003 (0.0010)\t Regr Loss 1016.2620 (916.6555)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0004 (0.0012)\t Regr Loss 1056.7688 (916.1204)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.737\n",
      "\n",
      "\n",
      "Epochs since last improvement: 15\n",
      "\n",
      "Epoch: [12][0/334]\tBatch Time 0.045 (0.045)\tLoss 0.0000 (0.0000)\t RLoss 55650.2344 (41258.7678)\t\n",
      "Epoch: [12][25/334]\tBatch Time 0.043 (0.044)\tLoss 0.0007 (0.0002)\t RLoss 47292.2031 (41258.2898)\t\n",
      "Epoch: [12][50/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0007)\t RLoss 44491.5859 (41154.1700)\t\n",
      "Epoch: [12][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0125 (0.0011)\t RLoss 51096.6641 (41435.7422)\t\n",
      "Epoch: [12][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0008)\t RLoss 32799.4453 (41377.2026)\t\n",
      "Epoch: [12][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0003 (0.0007)\t RLoss 48785.0391 (41245.8047)\t\n",
      "Epoch: [12][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0007)\t RLoss 49692.0312 (41346.0285)\t\n",
      "Epoch: [12][175/334]\tBatch Time 0.044 (0.043)\tLoss 0.0000 (0.0008)\t RLoss 46256.5312 (41403.3980)\t\n",
      "Epoch: [12][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0010)\t RLoss 36401.3477 (41385.4347)\t\n",
      "Epoch: [12][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0011)\t RLoss 36862.4141 (41350.6317)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0006 (0.0012)\t RLoss 45431.2461 (41325.5744)\t\n",
      "Epoch: [12][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0011)\t RLoss 39498.5742 (41292.0827)\t\n",
      "Epoch: [12][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0013)\t RLoss 37636.4727 (41285.3135)\t\n",
      "Epoch: [12][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0014 (0.0013)\t RLoss 47315.7109 (41248.5509)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0002)\t Regr Loss 842.9429 (878.8635)\t\n",
      "[25/20]\tBatch Time 0.026 (0.027)\tLoss 0.0000 (0.0001)\t Regr Loss 851.2546 (921.3842)\t\n",
      "[50/20]\tBatch Time 0.026 (0.027)\tLoss 0.0000 (0.0008)\t Regr Loss 1027.9542 (930.4847)\t\n",
      "[75/20]\tBatch Time 0.026 (0.027)\tLoss 0.0000 (0.0009)\t Regr Loss 1036.2924 (935.3270)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0015)\t Regr Loss 937.0187 (930.9729)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 1102.3346 (931.1750)\t\n",
      "[150/20]\tBatch Time 0.026 (0.027)\tLoss 0.0002 (0.0013)\t Regr Loss 957.7491 (928.0521)\t\n",
      "[175/20]\tBatch Time 0.026 (0.027)\tLoss 0.0000 (0.0015)\t Regr Loss 934.3389 (926.3653)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 0.0004 (0.0013)\t Regr Loss 1008.3452 (923.1082)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0671 (0.0015)\t Regr Loss 1037.9290 (923.1597)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 1327.9036 (919.4719)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 1132.7532 (918.2308)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 958.5483 (918.8397)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 1109.5554 (916.6055)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.653\n",
      "\n",
      "\n",
      "Epochs since last improvement: 16\n",
      "\n",
      "Epoch: [13][0/334]\tBatch Time 0.045 (0.045)\tLoss 0.0000 (0.0000)\t RLoss 44937.6875 (40958.2018)\t\n",
      "Epoch: [13][25/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0018)\t RLoss 56113.3594 (41919.0379)\t\n",
      "Epoch: [13][50/334]\tBatch Time 0.043 (0.043)\tLoss 0.0006 (0.0010)\t RLoss 55112.9609 (41122.0793)\t\n",
      "Epoch: [13][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0009)\t RLoss 49469.1914 (41369.1871)\t\n",
      "Epoch: [13][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0014)\t RLoss 36378.4688 (41382.1978)\t\n",
      "Epoch: [13][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0017)\t RLoss 47261.9766 (41258.3829)\t\n",
      "Epoch: [13][150/334]\tBatch Time 0.044 (0.043)\tLoss 0.0000 (0.0017)\t RLoss 42701.0469 (41038.7187)\t\n",
      "Epoch: [13][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0015)\t RLoss 44354.1836 (41075.9459)\t\n",
      "Epoch: [13][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0006 (0.0015)\t RLoss 42951.5586 (41034.8262)\t\n",
      "Epoch: [13][225/334]\tBatch Time 0.042 (0.043)\tLoss 0.0021 (0.0015)\t RLoss 45799.1523 (41054.0433)\t\n",
      "Epoch: [13][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 32974.5898 (41061.0182)\t\n",
      "Epoch: [13][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 38070.4453 (41112.7971)\t\n",
      "Epoch: [13][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 33162.8164 (41181.7554)\t\n",
      "Epoch: [13][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0013)\t RLoss 46169.3633 (41235.9102)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0006 (0.0006)\t Regr Loss 1026.6637 (884.0482)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0021)\t Regr Loss 1129.2316 (925.9193)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0017)\t Regr Loss 964.1166 (914.8577)\t\n",
      "[75/20]\tBatch Time 0.026 (0.027)\tLoss 0.0001 (0.0014)\t Regr Loss 917.7629 (912.9084)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0013 (0.0011)\t Regr Loss 1118.7202 (913.4664)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0009)\t Regr Loss 966.1852 (913.2991)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0009)\t Regr Loss 860.1353 (911.6105)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 1225.0374 (912.0527)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 794.2967 (914.3065)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0007 (0.0013)\t Regr Loss 1166.5017 (916.4194)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 981.3170 (916.4148)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0012)\t Regr Loss 946.1830 (916.5071)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0011)\t Regr Loss 1070.7971 (916.2273)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 925.8413 (915.9282)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.554\n",
      "\n",
      "\n",
      "Epochs since last improvement: 17\n",
      "\n",
      "Epoch: [14][0/334]\tBatch Time 0.046 (0.046)\tLoss 0.0004 (0.0004)\t RLoss 48001.9336 (39786.6029)\t\n",
      "Epoch: [14][25/334]\tBatch Time 0.043 (0.044)\tLoss 0.0007 (0.0028)\t RLoss 37534.6289 (42008.5958)\t\n",
      "Epoch: [14][50/334]\tBatch Time 0.043 (0.043)\tLoss 0.0004 (0.0018)\t RLoss 49662.7344 (42183.5311)\t\n",
      "Epoch: [14][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 41318.5273 (41713.6635)\t\n",
      "Epoch: [14][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0017)\t RLoss 48339.8672 (41461.8397)\t\n",
      "Epoch: [14][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0014)\t RLoss 48625.5664 (41424.1049)\t\n",
      "Epoch: [14][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 36112.5000 (41352.4325)\t\n",
      "Epoch: [14][175/334]\tBatch Time 0.044 (0.043)\tLoss 0.0002 (0.0013)\t RLoss 50100.4648 (41382.6636)\t\n",
      "Epoch: [14][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 54614.9219 (41403.6420)\t\n",
      "Epoch: [14][225/334]\tBatch Time 0.044 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 38480.6523 (41389.5738)\t\n",
      "Epoch: [14][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0011 (0.0013)\t RLoss 46664.1367 (41326.3132)\t\n",
      "Epoch: [14][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0014)\t RLoss 45254.3945 (41285.0095)\t\n",
      "Epoch: [14][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0013)\t RLoss 42375.8008 (41278.9114)\t\n",
      "Epoch: [14][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0012)\t RLoss 41312.2422 (41200.3044)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0000)\t Regr Loss 702.5175 (878.7418)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0002)\t Regr Loss 856.6635 (923.1019)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0004)\t Regr Loss 914.4714 (919.7007)\t\n",
      "[75/20]\tBatch Time 0.027 (0.027)\tLoss 0.0348 (0.0017)\t Regr Loss 1063.7709 (921.0892)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0020)\t Regr Loss 1073.6328 (920.6339)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0019)\t Regr Loss 873.6620 (920.2324)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0016)\t Regr Loss 918.4594 (919.6591)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0014)\t Regr Loss 1078.3496 (914.5153)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0013)\t Regr Loss 948.4200 (917.0779)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 1188.1178 (917.6581)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0008 (0.0012)\t Regr Loss 956.1581 (919.3172)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0012)\t Regr Loss 878.5283 (918.4719)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0012)\t Regr Loss 939.4658 (918.7243)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0012)\t Regr Loss 1188.6564 (916.5748)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.552\n",
      "\n",
      "\n",
      "Epochs since last improvement: 18\n",
      "\n",
      "Epoch: [15][0/334]\tBatch Time 0.044 (0.044)\tLoss 0.0000 (0.0000)\t RLoss 48085.0586 (38747.4400)\t\n",
      "Epoch: [15][25/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0027)\t RLoss 53174.4609 (40473.3001)\t\n",
      "Epoch: [15][50/334]\tBatch Time 0.042 (0.043)\tLoss 0.0001 (0.0029)\t RLoss 45362.9688 (40536.4641)\t\n",
      "Epoch: [15][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0021)\t RLoss 44266.6406 (41065.9973)\t\n",
      "Epoch: [15][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0050 (0.0016)\t RLoss 51167.4766 (41250.8747)\t\n",
      "Epoch: [15][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0013)\t RLoss 44347.9141 (41412.7138)\t\n",
      "Epoch: [15][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0262 (0.0016)\t RLoss 50882.4141 (41430.2822)\t\n",
      "Epoch: [15][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0016 (0.0014)\t RLoss 37164.6094 (41165.7825)\t\n",
      "Epoch: [15][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 47036.5234 (41296.5209)\t\n",
      "Epoch: [15][225/334]\tBatch Time 0.044 (0.043)\tLoss 0.0006 (0.0011)\t RLoss 49073.8359 (41152.4143)\t\n",
      "Epoch: [15][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0010)\t RLoss 42021.1289 (41138.9625)\t\n",
      "Epoch: [15][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0010)\t RLoss 52673.0586 (41170.5009)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0011)\t RLoss 39827.1172 (41234.6329)\t\n",
      "Epoch: [15][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0013)\t RLoss 48141.1172 (41228.8930)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0431 (0.0431)\t Regr Loss 1314.0059 (893.6581)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0018)\t Regr Loss 990.8666 (897.6789)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0029)\t Regr Loss 1033.3652 (903.8615)\t\n",
      "[75/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0025)\t Regr Loss 1075.3976 (908.8064)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0021)\t Regr Loss 849.2405 (912.7281)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0017)\t Regr Loss 1085.0596 (911.7802)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0348 (0.0019)\t Regr Loss 1139.3868 (910.4242)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0018)\t Regr Loss 973.3317 (913.0607)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0018)\t Regr Loss 1040.7963 (913.8087)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0017)\t Regr Loss 812.4012 (912.7193)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0015)\t Regr Loss 904.4865 (914.6385)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0019 (0.0014)\t Regr Loss 959.8088 (913.9688)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0014)\t Regr Loss 1020.1852 (914.9563)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0013)\t Regr Loss 960.3814 (916.0604)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 917.147\n",
      "\n",
      "\n",
      "Epochs since last improvement: 19\n",
      "\n",
      "Epoch: [16][0/334]\tBatch Time 0.046 (0.046)\tLoss 0.0000 (0.0000)\t RLoss 48366.3984 (39774.2147)\t\n",
      "Epoch: [16][25/334]\tBatch Time 0.043 (0.044)\tLoss 0.0933 (0.0051)\t RLoss 43909.3984 (41438.4085)\t\n",
      "Epoch: [16][50/334]\tBatch Time 0.043 (0.044)\tLoss 0.0000 (0.0029)\t RLoss 31166.0098 (41212.0817)\t\n",
      "Epoch: [16][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0130 (0.0024)\t RLoss 38693.0898 (41383.6577)\t\n",
      "Epoch: [16][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0021)\t RLoss 54056.2891 (41454.3421)\t\n",
      "Epoch: [16][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0017)\t RLoss 50622.2148 (41338.7574)\t\n",
      "Epoch: [16][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0015)\t RLoss 39682.2969 (41406.3771)\t\n",
      "Epoch: [16][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 43503.4023 (41372.5144)\t\n",
      "Epoch: [16][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0014)\t RLoss 48993.9180 (41299.1819)\t\n",
      "Epoch: [16][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0012)\t RLoss 51024.4180 (41304.1899)\t\n",
      "Epoch: [16][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0011)\t RLoss 46898.6875 (41315.5581)\t\n",
      "Epoch: [16][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0003 (0.0012)\t RLoss 44395.6133 (41323.3977)\t\n",
      "Epoch: [16][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0007 (0.0011)\t RLoss 46826.1133 (41235.2399)\t\n",
      "Epoch: [16][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0007 (0.0013)\t RLoss 43765.3984 (41246.8367)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0000)\t Regr Loss 1064.1255 (946.2475)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0020)\t Regr Loss 1039.2349 (925.8161)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0019)\t Regr Loss 1118.2208 (914.2109)\t\n",
      "[75/20]\tBatch Time 0.027 (0.027)\tLoss 0.0004 (0.0018)\t Regr Loss 863.9719 (918.7613)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0017)\t Regr Loss 1007.9038 (912.3430)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0005 (0.0017)\t Regr Loss 1003.4505 (914.9335)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0015)\t Regr Loss 969.7380 (915.9837)\t\n",
      "[175/20]\tBatch Time 0.026 (0.027)\tLoss 0.0003 (0.0015)\t Regr Loss 942.8203 (915.2263)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0014)\t Regr Loss 1001.3417 (913.4156)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0016)\t Regr Loss 1227.7550 (913.1269)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 1003.4788 (913.9193)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 914.0867 (913.5872)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 1077.3533 (915.2658)\t\n",
      "[325/20]\tBatch Time 0.026 (0.027)\tLoss 0.0001 (0.0013)\t Regr Loss 970.9803 (916.5695)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.420\n",
      "\n",
      "\n",
      "Epochs since last improvement: 20\n",
      "\n",
      "Epoch: [17][0/334]\tBatch Time 0.044 (0.044)\tLoss 0.0054 (0.0054)\t RLoss 41261.0547 (41057.0077)\t\n",
      "Epoch: [17][25/334]\tBatch Time 0.043 (0.043)\tLoss 0.0012 (0.0006)\t RLoss 42401.5703 (40753.1665)\t\n",
      "Epoch: [17][50/334]\tBatch Time 0.044 (0.043)\tLoss 0.0002 (0.0004)\t RLoss 40433.7109 (41081.8729)\t\n",
      "Epoch: [17][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0006)\t RLoss 46412.4219 (40911.1999)\t\n",
      "Epoch: [17][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0008)\t RLoss 52831.8477 (40955.5358)\t\n",
      "Epoch: [17][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0007)\t RLoss 49568.8359 (41056.2132)\t\n",
      "Epoch: [17][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0006)\t RLoss 50417.0000 (40984.1141)\t\n",
      "Epoch: [17][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0007)\t RLoss 47513.1367 (40965.0867)\t\n",
      "Epoch: [17][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0009)\t RLoss 43900.7461 (41037.1881)\t\n",
      "Epoch: [17][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0011)\t RLoss 35746.6133 (41094.1326)\t\n",
      "Epoch: [17][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0010)\t RLoss 43238.3594 (41052.9360)\t\n",
      "Epoch: [17][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0011)\t RLoss 50111.8281 (41191.9446)\t\n",
      "Epoch: [17][300/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0011)\t RLoss 46620.9062 (41178.9640)\t\n",
      "Epoch: [17][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 34137.4805 (41271.5867)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0000)\t Regr Loss 1127.9263 (978.2296)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0002)\t Regr Loss 863.8085 (912.5151)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0014)\t Regr Loss 829.2074 (912.4865)\t\n",
      "[75/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0009)\t Regr Loss 1060.5149 (913.2308)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0017)\t Regr Loss 983.5575 (911.4421)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0018)\t Regr Loss 1016.4865 (912.6980)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0017)\t Regr Loss 1020.3463 (912.3076)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0015)\t Regr Loss 1045.8961 (910.0464)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0013)\t Regr Loss 890.3382 (911.7779)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0013)\t Regr Loss 865.7924 (912.2239)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0012)\t Regr Loss 1041.4630 (912.8504)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0011)\t Regr Loss 1078.0459 (914.7627)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.0011 (0.0012)\t Regr Loss 852.4805 (915.5918)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0011)\t Regr Loss 1227.9894 (915.9315)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 916.532\n",
      "\n",
      "\n",
      "Epochs since last improvement: 21\n",
      "\n",
      "Epoch: [18][0/334]\tBatch Time 0.046 (0.046)\tLoss 0.0000 (0.0000)\t RLoss 35650.4062 (41883.1062)\t\n",
      "Epoch: [18][25/334]\tBatch Time 0.043 (0.044)\tLoss 0.0000 (0.0006)\t RLoss 46039.5781 (41193.6065)\t\n",
      "Epoch: [18][50/334]\tBatch Time 0.043 (0.044)\tLoss 0.0001 (0.0014)\t RLoss 55373.9922 (41553.9729)\t\n",
      "Epoch: [18][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0011)\t RLoss 39796.0391 (41161.7184)\t\n",
      "Epoch: [18][100/334]\tBatch Time 0.043 (0.043)\tLoss 0.0004 (0.0012)\t RLoss 43806.6562 (41102.4339)\t\n",
      "Epoch: [18][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.0004 (0.0014)\t RLoss 46443.0469 (41104.5763)\t\n",
      "Epoch: [18][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0016)\t RLoss 48272.3906 (41009.1054)\t\n",
      "Epoch: [18][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0014)\t RLoss 43242.6719 (41107.0347)\t\n",
      "Epoch: [18][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 47345.8398 (41128.2397)\t\n",
      "Epoch: [18][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0012)\t RLoss 45060.5039 (41195.8173)\t\n",
      "Epoch: [18][250/334]\tBatch Time 0.043 (0.043)\tLoss 0.0002 (0.0012)\t RLoss 48794.1680 (41241.2062)\t\n",
      "Epoch: [18][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.0001 (0.0011)\t RLoss 46905.6523 (41263.1303)\t\n",
      "Epoch: [18][300/334]\tBatch Time 0.042 (0.043)\tLoss 0.0057 (0.0011)\t RLoss 41033.4922 (41225.4201)\t\n",
      "Epoch: [18][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (0.0013)\t RLoss 49423.4336 (41227.5198)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0002)\t Regr Loss 1098.2421 (995.8251)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0001 (0.0001)\t Regr Loss 861.7584 (924.8197)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0002)\t Regr Loss 1156.2130 (928.4096)\t\n",
      "[75/20]\tBatch Time 0.026 (0.027)\tLoss 0.0000 (0.0009)\t Regr Loss 1044.1531 (929.6476)\t\n",
      "[100/20]\tBatch Time 0.026 (0.027)\tLoss 0.0000 (0.0008)\t Regr Loss 1117.2324 (927.8944)\t\n",
      "[125/20]\tBatch Time 0.026 (0.027)\tLoss 0.0000 (0.0010)\t Regr Loss 918.0738 (922.9659)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (0.0009)\t Regr Loss 931.4429 (917.6901)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.0051 (0.0012)\t Regr Loss 1060.6443 (919.0404)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 2.0275 (0.0169)\t Regr Loss 1916.2196 (922.9012)\t\n",
      "[225/20]\tBatch Time 0.041 (0.027)\tLoss 1.0584 (0.1403)\t Regr Loss 2200.1035 (970.4034)\t\n",
      "[250/20]\tBatch Time 0.028 (0.027)\tLoss 2.2997 (0.2788)\t Regr Loss 2721.8416 (1024.5685)\t\n",
      "[275/20]\tBatch Time 0.035 (0.027)\tLoss 0.2048 (0.3616)\t Regr Loss 899.9097 (1059.3634)\t\n",
      "[300/20]\tBatch Time 0.035 (0.028)\tLoss 1.4396 (0.4634)\t Regr Loss 2464.3118 (1102.1348)\t\n",
      "[325/20]\tBatch Time 0.027 (0.028)\tLoss 0.6597 (0.5428)\t Regr Loss 947.5815 (1135.3475)\t\n",
      "\n",
      " * LOSS - 0.572\n",
      "\n",
      " * REGR LOSS - 1147.244\n",
      "\n",
      "\n",
      "Epochs since last improvement: 22\n",
      "\n",
      "Epoch: [19][0/334]\tBatch Time 0.047 (0.047)\tLoss 1.2624 (1.2624)\t RLoss 49861.1289 (70631.9516)\t\n",
      "Epoch: [19][25/334]\tBatch Time 0.043 (0.045)\tLoss 2.2212 (1.5024)\t RLoss 91167.1484 (68349.0873)\t\n",
      "Epoch: [19][50/334]\tBatch Time 0.043 (0.044)\tLoss 1.5548 (1.3128)\t RLoss 80187.0312 (66211.1066)\t\n",
      "Epoch: [19][75/334]\tBatch Time 0.043 (0.044)\tLoss 1.5235 (1.3690)\t RLoss 93782.1953 (65857.1328)\t\n",
      "Epoch: [19][100/334]\tBatch Time 0.042 (0.044)\tLoss 1.7164 (1.4018)\t RLoss 75699.1016 (65919.2810)\t\n",
      "Epoch: [19][125/334]\tBatch Time 0.043 (0.043)\tLoss 1.2811 (1.4014)\t RLoss 47921.1406 (66167.8464)\t\n",
      "Epoch: [19][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.7630 (1.3754)\t RLoss 49877.4258 (65720.7674)\t\n",
      "Epoch: [19][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.7643 (1.3408)\t RLoss 41874.8047 (65197.9179)\t\n",
      "Epoch: [19][200/334]\tBatch Time 0.043 (0.043)\tLoss 1.1936 (1.3694)\t RLoss 46655.7422 (65952.8820)\t\n",
      "Epoch: [19][225/334]\tBatch Time 0.043 (0.043)\tLoss 2.2326 (1.4173)\t RLoss 94720.0156 (66838.1397)\t\n",
      "Epoch: [19][250/334]\tBatch Time 0.043 (0.043)\tLoss 3.2149 (1.4451)\t RLoss 149886.1406 (67140.9046)\t\n",
      "Epoch: [19][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.9441 (1.4034)\t RLoss 54507.5156 (66522.2708)\t\n",
      "Epoch: [19][300/334]\tBatch Time 0.043 (0.043)\tLoss 1.5842 (1.4121)\t RLoss 54283.8594 (66628.5680)\t\n",
      "Epoch: [19][325/334]\tBatch Time 0.043 (0.043)\tLoss 4.3072 (1.4394)\t RLoss 52148.2070 (66943.8859)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 1.7375 (1.7375)\t Regr Loss 1063.7462 (1550.2540)\t\n",
      "[25/20]\tBatch Time 0.026 (0.027)\tLoss 3.0217 (1.6435)\t Regr Loss 3212.1663 (1586.4426)\t\n",
      "[50/20]\tBatch Time 0.026 (0.027)\tLoss 1.0162 (1.6472)\t Regr Loss 1451.7577 (1583.8404)\t\n",
      "[75/20]\tBatch Time 0.027 (0.027)\tLoss 1.6414 (1.6361)\t Regr Loss 2644.1335 (1570.7238)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 2.9241 (1.5497)\t Regr Loss 4607.6099 (1533.4162)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 1.2203 (1.5023)\t Regr Loss 1434.9296 (1511.0025)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 2.5442 (1.5086)\t Regr Loss 2236.0955 (1506.8663)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.7674 (1.5500)\t Regr Loss 1703.1378 (1525.3509)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 0.4130 (1.4981)\t Regr Loss 1905.3031 (1503.3487)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 1.2742 (1.4999)\t Regr Loss 1075.8489 (1505.5097)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 0.1538 (1.4599)\t Regr Loss 1666.9503 (1490.8254)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0209 (1.4204)\t Regr Loss 975.2920 (1476.9575)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 1.9718 (1.4450)\t Regr Loss 2540.0466 (1486.9037)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 1.8989 (1.4386)\t Regr Loss 1562.3104 (1486.5073)\t\n",
      "\n",
      " * LOSS - 1.435\n",
      "\n",
      " * REGR LOSS - 1485.015\n",
      "\n",
      "\n",
      "Epochs since last improvement: 23\n",
      "\n",
      "Epoch: [20][0/334]\tBatch Time 0.045 (0.045)\tLoss 0.6391 (0.6391)\t RLoss 42080.6875 (49742.9260)\t\n",
      "Epoch: [20][25/334]\tBatch Time 0.043 (0.043)\tLoss 2.3788 (1.5094)\t RLoss 105429.7969 (67741.3934)\t\n",
      "Epoch: [20][50/334]\tBatch Time 0.043 (0.043)\tLoss 0.3313 (1.6357)\t RLoss 47545.1836 (68780.5570)\t\n",
      "Epoch: [20][75/334]\tBatch Time 0.043 (0.043)\tLoss 4.5385 (1.6080)\t RLoss 112779.0547 (68869.1735)\t\n",
      "Epoch: [20][100/334]\tBatch Time 0.043 (0.043)\tLoss 1.0201 (1.5209)\t RLoss 95775.9844 (67883.6707)\t\n",
      "Epoch: [20][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.4539 (1.5089)\t RLoss 80320.5156 (67667.7735)\t\n",
      "Epoch: [20][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.5985 (1.5139)\t RLoss 47505.5312 (67355.8681)\t\n",
      "Epoch: [20][175/334]\tBatch Time 0.043 (0.043)\tLoss 0.0000 (1.4872)\t RLoss 50848.3555 (67137.8642)\t\n",
      "Epoch: [20][200/334]\tBatch Time 0.043 (0.043)\tLoss 1.7019 (1.4758)\t RLoss 50036.2852 (67093.6944)\t\n",
      "Epoch: [20][225/334]\tBatch Time 0.043 (0.043)\tLoss 2.3702 (1.5172)\t RLoss 67332.8516 (67915.0690)\t\n",
      "Epoch: [20][250/334]\tBatch Time 0.043 (0.043)\tLoss 1.3602 (1.5163)\t RLoss 49230.2695 (67921.7780)\t\n",
      "Epoch: [20][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.9890 (1.4746)\t RLoss 76036.7422 (67318.7492)\t\n",
      "Epoch: [20][300/334]\tBatch Time 0.043 (0.043)\tLoss 1.4496 (1.4764)\t RLoss 75026.8828 (67283.7260)\t\n",
      "Epoch: [20][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.9271 (1.4340)\t RLoss 49673.3438 (66768.5048)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 2.0841 (2.0841)\t Regr Loss 1456.4407 (1628.6758)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.2739 (1.7275)\t Regr Loss 1127.2859 (1560.4278)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 1.0978 (1.4830)\t Regr Loss 1666.4659 (1501.7691)\t\n",
      "[75/20]\tBatch Time 0.027 (0.027)\tLoss 0.4094 (1.5046)\t Regr Loss 986.7354 (1512.3078)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 0.7011 (1.4745)\t Regr Loss 903.6299 (1505.7188)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 1.2055 (1.3941)\t Regr Loss 1104.5153 (1476.6063)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 1.4626 (1.3553)\t Regr Loss 2010.8824 (1461.9289)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.4145 (1.3693)\t Regr Loss 1131.7218 (1461.3040)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 1.2065 (1.3895)\t Regr Loss 1838.9236 (1468.9407)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 3.0772 (1.4032)\t Regr Loss 1751.8782 (1468.6508)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 1.7036 (1.4222)\t Regr Loss 1442.6930 (1478.3511)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 2.8483 (1.4520)\t Regr Loss 3283.3445 (1492.8991)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.7717 (1.4459)\t Regr Loss 1988.3478 (1486.1936)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 0.2892 (1.4360)\t Regr Loss 1082.8439 (1486.6920)\t\n",
      "\n",
      " * LOSS - 1.439\n",
      "\n",
      " * REGR LOSS - 1485.758\n",
      "\n",
      "\n",
      "Epochs since last improvement: 24\n",
      "\n",
      "Epoch: [21][0/334]\tBatch Time 0.043 (0.043)\tLoss 2.9736 (2.9736)\t RLoss 152014.2031 (86637.2435)\t\n",
      "Epoch: [21][25/334]\tBatch Time 0.043 (0.043)\tLoss 1.0089 (1.3790)\t RLoss 54279.3320 (67507.3202)\t\n",
      "Epoch: [21][50/334]\tBatch Time 0.043 (0.043)\tLoss 0.9528 (1.4699)\t RLoss 82439.8594 (67833.8729)\t\n",
      "Epoch: [21][75/334]\tBatch Time 0.044 (0.043)\tLoss 1.8486 (1.5026)\t RLoss 66862.1562 (67999.0187)\t\n",
      "Epoch: [21][100/334]\tBatch Time 0.043 (0.043)\tLoss 2.3396 (1.4877)\t RLoss 86748.3516 (68424.5339)\t\n",
      "Epoch: [21][125/334]\tBatch Time 0.043 (0.043)\tLoss 2.0275 (1.4948)\t RLoss 82850.8203 (69028.9509)\t\n",
      "Epoch: [21][150/334]\tBatch Time 0.043 (0.043)\tLoss 1.8364 (1.4901)\t RLoss 55281.8477 (68601.6324)\t\n",
      "Epoch: [21][175/334]\tBatch Time 0.042 (0.043)\tLoss 2.3873 (1.5070)\t RLoss 81068.6016 (68326.2914)\t\n",
      "Epoch: [21][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.9106 (1.4818)\t RLoss 65634.5703 (67806.9873)\t\n",
      "Epoch: [21][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.7060 (1.4689)\t RLoss 41700.5273 (67635.3589)\t\n",
      "Epoch: [21][250/334]\tBatch Time 0.043 (0.043)\tLoss 3.1479 (1.4452)\t RLoss 47255.9492 (67066.9181)\t\n",
      "Epoch: [21][275/334]\tBatch Time 0.043 (0.043)\tLoss 1.8055 (1.4483)\t RLoss 57042.5234 (67151.8032)\t\n",
      "Epoch: [21][300/334]\tBatch Time 0.043 (0.043)\tLoss 1.1512 (1.4550)\t RLoss 48727.5547 (67272.9746)\t\n",
      "Epoch: [21][325/334]\tBatch Time 0.044 (0.043)\tLoss 1.2686 (1.4406)\t RLoss 72453.0312 (66901.3221)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 2.5444 (2.5444)\t Regr Loss 1424.0227 (1891.5487)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.6832 (1.7051)\t Regr Loss 1861.8348 (1599.3986)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.2711 (1.5242)\t Regr Loss 1176.9093 (1554.0991)\t\n",
      "[75/20]\tBatch Time 0.027 (0.027)\tLoss 1.4525 (1.5920)\t Regr Loss 1430.6144 (1582.8866)\t\n",
      "[100/20]\tBatch Time 0.026 (0.027)\tLoss 1.8223 (1.5072)\t Regr Loss 2051.0947 (1541.6993)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 2.3091 (1.5495)\t Regr Loss 898.2076 (1546.0023)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 0.3875 (1.5464)\t Regr Loss 941.2545 (1535.5105)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 0.5818 (1.5167)\t Regr Loss 1628.7974 (1522.0754)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 2.5320 (1.5146)\t Regr Loss 1673.6666 (1517.9374)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 2.1719 (1.4893)\t Regr Loss 3250.7129 (1509.5828)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 1.3917 (1.4404)\t Regr Loss 2005.7925 (1491.4316)\t\n",
      "[275/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (1.4374)\t Regr Loss 999.4598 (1487.0474)\t\n",
      "[300/20]\tBatch Time 0.027 (0.027)\tLoss 0.9525 (1.4425)\t Regr Loss 1002.6602 (1488.1156)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 1.5324 (1.4310)\t Regr Loss 2329.2197 (1482.7664)\t\n",
      "\n",
      " * LOSS - 1.441\n",
      "\n",
      " * REGR LOSS - 1487.713\n",
      "\n",
      "\n",
      "Epochs since last improvement: 25\n",
      "\n",
      "Epoch: [22][0/334]\tBatch Time 0.046 (0.046)\tLoss 0.2748 (0.2748)\t RLoss 46280.4531 (49833.9868)\t\n",
      "Epoch: [22][25/334]\tBatch Time 0.043 (0.044)\tLoss 1.9217 (1.6891)\t RLoss 97960.8203 (68968.9640)\t\n",
      "Epoch: [22][50/334]\tBatch Time 0.043 (0.044)\tLoss 1.8908 (1.6571)\t RLoss 53258.7227 (70369.2401)\t\n",
      "Epoch: [22][75/334]\tBatch Time 0.043 (0.044)\tLoss 3.4274 (1.5539)\t RLoss 72234.0625 (68945.2969)\t\n",
      "Epoch: [22][100/334]\tBatch Time 0.043 (0.043)\tLoss 1.6689 (1.5627)\t RLoss 48275.5508 (69468.0040)\t\n",
      "Epoch: [22][125/334]\tBatch Time 0.043 (0.043)\tLoss 3.6633 (1.5662)\t RLoss 70531.1094 (69051.7345)\t\n",
      "Epoch: [22][150/334]\tBatch Time 0.043 (0.043)\tLoss 1.2703 (1.5066)\t RLoss 82608.4609 (67945.9165)\t\n",
      "Epoch: [22][175/334]\tBatch Time 0.043 (0.043)\tLoss 1.7891 (1.5163)\t RLoss 131887.8125 (68148.3594)\t\n",
      "Epoch: [22][200/334]\tBatch Time 0.043 (0.043)\tLoss 2.0573 (1.4849)\t RLoss 75525.6719 (67677.8016)\t\n",
      "Epoch: [22][225/334]\tBatch Time 0.043 (0.043)\tLoss 0.3727 (1.4705)\t RLoss 66180.8359 (67351.1700)\t\n",
      "Epoch: [22][250/334]\tBatch Time 0.043 (0.043)\tLoss 1.8191 (1.4257)\t RLoss 68862.8984 (66665.5809)\t\n",
      "Epoch: [22][275/334]\tBatch Time 0.043 (0.043)\tLoss 0.3709 (1.4075)\t RLoss 47649.5820 (66383.5632)\t\n",
      "Epoch: [22][300/334]\tBatch Time 0.044 (0.043)\tLoss 0.8487 (1.4247)\t RLoss 68080.9062 (66627.7118)\t\n",
      "Epoch: [22][325/334]\tBatch Time 0.043 (0.043)\tLoss 1.8692 (1.4074)\t RLoss 91035.0312 (66459.9188)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.9648 (0.9648)\t Regr Loss 1083.2794 (1371.6124)\t\n",
      "[25/20]\tBatch Time 0.027 (0.027)\tLoss 0.0000 (1.3136)\t Regr Loss 1004.5681 (1428.6288)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.1178 (1.3596)\t Regr Loss 1194.8898 (1433.1947)\t\n",
      "[75/20]\tBatch Time 0.027 (0.027)\tLoss 1.6900 (1.3326)\t Regr Loss 1984.2476 (1447.3627)\t\n",
      "[100/20]\tBatch Time 0.027 (0.027)\tLoss 1.3577 (1.3049)\t Regr Loss 1221.1355 (1423.2640)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.9162 (1.2621)\t Regr Loss 2194.5222 (1418.7627)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 2.3755 (1.2675)\t Regr Loss 2020.5780 (1422.9436)\t\n",
      "[175/20]\tBatch Time 0.027 (0.027)\tLoss 2.6587 (1.3302)\t Regr Loss 1218.3713 (1451.5949)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 0.9010 (1.3656)\t Regr Loss 1930.7589 (1462.9996)\t\n",
      "[225/20]\tBatch Time 0.027 (0.027)\tLoss 1.6591 (1.3740)\t Regr Loss 1038.8940 (1466.6076)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 2.7989 (1.4087)\t Regr Loss 2772.6833 (1481.2280)\t\n",
      "[275/20]\tBatch Time 0.026 (0.027)\tLoss 0.3243 (1.4064)\t Regr Loss 1034.2694 (1477.6770)\t\n",
      "[300/20]\tBatch Time 0.026 (0.027)\tLoss 0.3404 (1.4109)\t Regr Loss 786.2761 (1479.1468)\t\n",
      "[325/20]\tBatch Time 0.026 (0.027)\tLoss 1.0859 (1.4427)\t Regr Loss 1012.2420 (1487.5915)\t\n",
      "\n",
      " * LOSS - 1.435\n",
      "\n",
      " * REGR LOSS - 1485.504\n",
      "\n",
      "\n",
      "Epochs since last improvement: 26\n",
      "\n",
      "Epoch: [23][0/334]\tBatch Time 0.044 (0.044)\tLoss 0.5962 (0.5962)\t RLoss 49022.7773 (63032.1413)\t\n",
      "Epoch: [23][25/334]\tBatch Time 0.043 (0.043)\tLoss 0.9706 (1.4215)\t RLoss 37717.7695 (65286.8004)\t\n",
      "Epoch: [23][50/334]\tBatch Time 0.043 (0.043)\tLoss 0.9985 (1.3752)\t RLoss 94382.5547 (64338.1895)\t\n",
      "Epoch: [23][75/334]\tBatch Time 0.043 (0.043)\tLoss 1.0646 (1.4462)\t RLoss 100423.1484 (66840.9329)\t\n",
      "Epoch: [23][100/334]\tBatch Time 0.043 (0.043)\tLoss 1.0853 (1.4727)\t RLoss 93494.5469 (67587.0697)\t\n",
      "Epoch: [23][125/334]\tBatch Time 0.043 (0.043)\tLoss 0.7470 (1.4702)\t RLoss 92695.8516 (67348.8563)\t\n",
      "Epoch: [23][150/334]\tBatch Time 0.044 (0.044)\tLoss 0.6579 (1.4654)\t RLoss 46206.7109 (67249.1400)\t\n",
      "Epoch: [23][175/334]\tBatch Time 0.052 (0.044)\tLoss 0.9926 (1.4545)\t RLoss 44086.3359 (66841.5513)\t\n",
      "Epoch: [23][200/334]\tBatch Time 0.045 (0.044)\tLoss 1.6351 (1.4149)\t RLoss 44859.6289 (66410.5098)\t\n",
      "Epoch: [23][225/334]\tBatch Time 0.046 (0.044)\tLoss 1.1385 (1.4356)\t RLoss 62239.2578 (66792.5197)\t\n",
      "Epoch: [23][250/334]\tBatch Time 0.046 (0.044)\tLoss 2.1896 (1.4055)\t RLoss 84762.2812 (66492.1577)\t\n",
      "Epoch: [23][275/334]\tBatch Time 0.049 (0.045)\tLoss 0.6353 (1.4144)\t RLoss 50269.7656 (66695.1935)\t\n",
      "Epoch: [23][300/334]\tBatch Time 0.046 (0.045)\tLoss 1.9203 (1.4154)\t RLoss 64561.7383 (66868.1759)\t\n",
      "Epoch: [23][325/334]\tBatch Time 0.052 (0.045)\tLoss 0.0001 (1.4178)\t RLoss 39239.1875 (66959.5215)\t\n",
      "[0/20]\tBatch Time 0.039 (0.039)\tLoss 1.1015 (1.1015)\t Regr Loss 1263.0933 (1569.2662)\t\n",
      "[25/20]\tBatch Time 0.030 (0.033)\tLoss 1.4554 (1.3926)\t Regr Loss 1100.6528 (1475.1362)\t\n",
      "[50/20]\tBatch Time 0.036 (0.034)\tLoss 0.4413 (1.4208)\t Regr Loss 981.1725 (1477.2091)\t\n",
      "[75/20]\tBatch Time 0.027 (0.032)\tLoss 2.1899 (1.3606)\t Regr Loss 1269.4750 (1468.9323)\t\n",
      "[100/20]\tBatch Time 0.027 (0.031)\tLoss 1.8998 (1.3861)\t Regr Loss 1444.3912 (1460.9258)\t\n",
      "[125/20]\tBatch Time 0.027 (0.030)\tLoss 1.6079 (1.4382)\t Regr Loss 1455.6119 (1479.5565)\t\n",
      "[150/20]\tBatch Time 0.026 (0.030)\tLoss 1.5239 (1.4502)\t Regr Loss 2350.9290 (1485.3500)\t\n",
      "[175/20]\tBatch Time 0.027 (0.029)\tLoss 3.0810 (1.4448)\t Regr Loss 2666.2693 (1491.2573)\t\n",
      "[200/20]\tBatch Time 0.027 (0.029)\tLoss 1.1803 (1.3895)\t Regr Loss 2158.9070 (1474.9861)\t\n",
      "[225/20]\tBatch Time 0.027 (0.029)\tLoss 0.8499 (1.3763)\t Regr Loss 1179.5948 (1471.0462)\t\n",
      "[250/20]\tBatch Time 0.027 (0.028)\tLoss 0.2737 (1.3611)\t Regr Loss 1092.0906 (1466.7316)\t\n",
      "[275/20]\tBatch Time 0.027 (0.028)\tLoss 3.2907 (1.3578)\t Regr Loss 2021.2085 (1468.1792)\t\n",
      "[300/20]\tBatch Time 0.027 (0.028)\tLoss 2.9091 (1.3769)\t Regr Loss 2478.3354 (1474.8311)\t\n",
      "[325/20]\tBatch Time 0.027 (0.028)\tLoss 0.1553 (1.3979)\t Regr Loss 931.6219 (1484.2153)\t\n",
      "\n",
      " * LOSS - 1.399\n",
      "\n",
      " * REGR LOSS - 1484.422\n",
      "\n",
      "\n",
      "Epochs since last improvement: 27\n",
      "\n",
      "Epoch: [24][0/334]\tBatch Time 0.046 (0.046)\tLoss 1.0642 (1.0642)\t RLoss 37246.9883 (63717.6146)\t\n",
      "Epoch: [24][25/334]\tBatch Time 0.043 (0.044)\tLoss 1.9548 (1.3646)\t RLoss 113545.1016 (65121.8475)\t\n",
      "Epoch: [24][50/334]\tBatch Time 0.043 (0.043)\tLoss 0.9364 (1.3345)\t RLoss 104591.4141 (65603.8530)\t\n",
      "Epoch: [24][75/334]\tBatch Time 0.043 (0.043)\tLoss 0.7195 (1.3087)\t RLoss 111516.0938 (65675.9573)\t\n",
      "Epoch: [24][100/334]\tBatch Time 0.043 (0.043)\tLoss 2.3757 (1.3220)\t RLoss 99832.7188 (65821.9742)\t\n",
      "Epoch: [24][125/334]\tBatch Time 0.043 (0.043)\tLoss 3.4423 (1.3397)\t RLoss 48729.2305 (66277.9490)\t\n",
      "Epoch: [24][150/334]\tBatch Time 0.043 (0.043)\tLoss 0.3415 (1.3237)\t RLoss 53159.7383 (65944.2678)\t\n",
      "Epoch: [24][175/334]\tBatch Time 0.044 (0.043)\tLoss 0.6303 (1.3205)\t RLoss 107074.0781 (66070.8488)\t\n",
      "Epoch: [24][200/334]\tBatch Time 0.043 (0.043)\tLoss 0.7003 (1.3175)\t RLoss 52115.9141 (66079.4903)\t\n",
      "Epoch: [24][225/334]\tBatch Time 0.043 (0.043)\tLoss 1.9162 (1.3664)\t RLoss 67748.9766 (66759.4495)\t\n",
      "Epoch: [24][250/334]\tBatch Time 0.044 (0.043)\tLoss 0.2279 (1.3790)\t RLoss 49404.0156 (66724.9631)\t\n",
      "Epoch: [24][275/334]\tBatch Time 0.044 (0.043)\tLoss 2.9961 (1.3958)\t RLoss 53997.1992 (67080.0465)\t\n",
      "Epoch: [24][300/334]\tBatch Time 0.043 (0.043)\tLoss 1.4467 (1.3818)\t RLoss 46631.4023 (66741.9444)\t\n",
      "Epoch: [24][325/334]\tBatch Time 0.043 (0.043)\tLoss 0.2080 (1.3959)\t RLoss 45685.6562 (66757.1489)\t\n",
      "[0/20]\tBatch Time 0.027 (0.027)\tLoss 0.0002 (0.0002)\t Regr Loss 1252.3251 (958.8072)\t\n",
      "[25/20]\tBatch Time 0.026 (0.027)\tLoss 0.7731 (1.1430)\t Regr Loss 1017.7703 (1392.3839)\t\n",
      "[50/20]\tBatch Time 0.027 (0.027)\tLoss 0.1813 (1.1009)\t Regr Loss 928.7910 (1420.0133)\t\n",
      "[75/20]\tBatch Time 0.027 (0.027)\tLoss 2.5553 (1.1702)\t Regr Loss 3361.0881 (1443.0556)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100/20]\tBatch Time 0.026 (0.027)\tLoss 1.1101 (1.2258)\t Regr Loss 1172.8392 (1462.5427)\t\n",
      "[125/20]\tBatch Time 0.027 (0.027)\tLoss 0.4149 (1.2614)\t Regr Loss 954.1509 (1459.9615)\t\n",
      "[150/20]\tBatch Time 0.027 (0.027)\tLoss 1.9124 (1.3066)\t Regr Loss 963.9612 (1472.4677)\t\n",
      "[175/20]\tBatch Time 0.026 (0.027)\tLoss 0.0001 (1.3079)\t Regr Loss 1203.1615 (1469.1895)\t\n",
      "[200/20]\tBatch Time 0.027 (0.027)\tLoss 1.2033 (1.3253)\t Regr Loss 983.1295 (1470.0737)\t\n",
      "[225/20]\tBatch Time 0.026 (0.027)\tLoss 0.5008 (1.3302)\t Regr Loss 1420.3223 (1468.5320)\t\n",
      "[250/20]\tBatch Time 0.027 (0.027)\tLoss 2.6245 (1.3520)\t Regr Loss 2385.0618 (1475.4704)\t\n",
      "[275/20]\tBatch Time 0.026 (0.027)\tLoss 2.7680 (1.3968)\t Regr Loss 1338.0658 (1487.1556)\t\n",
      "[300/20]\tBatch Time 0.026 (0.027)\tLoss 0.0000 (1.4016)\t Regr Loss 1013.2794 (1487.9196)\t\n",
      "[325/20]\tBatch Time 0.027 (0.027)\tLoss 3.1585 (1.3963)\t Regr Loss 4851.1655 (1485.3786)\t\n",
      "\n",
      " * LOSS - 1.396\n",
      "\n",
      " * REGR LOSS - 1483.566\n",
      "\n",
      "\n",
      "Epochs since last improvement: 28\n",
      "\n",
      "Epoch: [25][0/334]\tBatch Time 0.044 (0.044)\tLoss 2.9766 (2.9766)\t RLoss 83226.7266 (80033.2413)\t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d6f4fa678a0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# alert when training is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msound_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/rui/Downloads/newyear.ogg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msound_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f383a06d6242>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0moptimizer1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0moptimizer2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-2b9b65a1df96>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, c_model, r_model, classifCriterion, regrCriterion, optimizer1, optimizer2, epoch)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mloss2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0moptimizer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mlosses2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()\n",
    "\n",
    "# alert when training is done\n",
    "sound_file = '/home/rui/Downloads/newyear.ogg'\n",
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Ready!\n",
      "\n",
      "Loaded checkpoint from epoch 25. Best loss so far is 0.001.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('All Ready!')\n",
    "\n",
    "\n",
    "filename = \"checkpoint_v2sliding.pth.tar\"\n",
    "checkpoint = torch.load(filename)\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "best_loss = checkpoint['best_loss']\n",
    "print('\\nLoaded checkpoint from epoch %d. Best loss so far is %.3f.\\n' % (start_epoch, best_loss))\n",
    "classifModel = checkpoint['model1']\n",
    "regrModel = checkpoint['model2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "FOR N =  1\n",
      "tensor([0., 0., 1., 0., 0., 1., 0., 0., 0.])\n",
      "[177.0, 64.0, 1.3439035415649414]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(test_loader)\n",
    "images, labels, coords = dataiter.next()\n",
    "\n",
    "n = 1\n",
    "print(\"\\n\\nFOR N = \", n)\n",
    "imgSample = images[n]\n",
    "max_idx = torch.argmax(coords[n][:,0])\n",
    "x, y, theta = coords[n][max_idx]\n",
    "print(labels[n])\n",
    "print([float(zed) for zed in (x,y,theta)])\n",
    "#print(x,y,theta)\n",
    "#crops, __ = makeCrops(imgSample, 50, (100, 100), (x,y, theta))\n",
    "\n",
    "plt.imshow(imgSample)\n",
    "#fig, axess = plt.subplots(3,3, figsize=(10,10))\n",
    "#axess = np.array(axess).flatten()#order='F') #'F' means to flatten in column-major (Fortran- style) order.\n",
    "#for i in range(9):\n",
    "#    axess[i].imshow(crops[i])\n",
    "#    axess[i].set_ylabel('# '+str(i))\n",
    "plt.suptitle('Window')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss across batch size of  15 is:  tensor(6.8014, device='cuda:0') tensor(4438.8721, device='cuda:0')\n",
      "!-- guesses size torch.Size([9, 15, 3])\n",
      "!-- labels size torch.Size([15, 9])\n",
      "!-- guesses size torch.Size([15, 9, 3])\n",
      "tensor([[  0.,   0.],\n",
      "        [  0.,  50.],\n",
      "        [  0., 100.],\n",
      "        [ 50.,   0.],\n",
      "        [ 50.,  50.],\n",
      "        [ 50., 100.],\n",
      "        [100.,   0.],\n",
      "        [100.,  50.],\n",
      "        [100., 100.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# -- Check the results -------------------------------------------------------\n",
    "\n",
    "# -- Utility ---------------------------------------------\n",
    "def makeCrops(image, stepSize, windowSize, true_center):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    image = image.type(torch.FloatTensor)\n",
    "    crops = []\n",
    "    truths = []\n",
    "    c_x, c_y, orient = true_center\n",
    "    # TODO: look into otdering, why it's y,x !\n",
    "    margin = 10 \n",
    "    # --> is x, but is the column\n",
    "    # to slide horizontally, y must come first\n",
    "    for y in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "        for x in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "            end_x, end_y = x + windowSize[1], y + windowSize[0]\n",
    "            hasRect = (x + margin < c_x < end_x - margin) and (\n",
    "                y + margin < c_y < end_y - margin\n",
    "            )\n",
    "            truths.append(hasRect)\n",
    "            crops.append(image[y:end_y, x:end_x])\n",
    "    crops = torch.stack(crops)\n",
    "    print(\"shape of crops\", crops.shape)\n",
    "    return crops, truths\n",
    "\n",
    "\n",
    "losses = AverageMeter()  # loss\n",
    "losses2 = AverageMeter()  # loss\n",
    "\n",
    "\n",
    "# -- Get some validation results ---------------------------------------------\n",
    "classifModel.to(device).eval()\n",
    "regrModel.to(device).eval()\n",
    "\n",
    "c_criterion = nn.BCELoss()\n",
    "r_criterion = nn.MSELoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    dataiter = iter(test_loader)\n",
    "    images, labels, coords = dataiter.next()\n",
    "    \n",
    "    # Move to default device\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    coords = coords.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    predicted_class, all_crops, cropCoords = classifModel(images)\n",
    "    # Loss\n",
    "    loss = c_criterion(predicted_class, labels)\n",
    "    all_crops = all_crops.to(device)\n",
    "    cropCoords = cropCoords.to(device)\n",
    "    \n",
    "    loss1 = classifCriterion(predicted_class, labels)\n",
    "    \n",
    "    guesses = []\n",
    "    # num batches = 9\n",
    "    for i in range(9):\n",
    "        batchcrop = all_crops[:, i, :, :]\n",
    "        batchcrop.unsqueeze_(1)\n",
    "        offset = cropCoords[i]\n",
    "        offset = offset.repeat(all_crops.size(0), 1)\n",
    "        offset = torch.cat((offset, torch.zeros((all_crops.size(0),\n",
    "                                                 1)).to(device)),\n",
    "                           dim=1)\n",
    "        center_truth = coords[:, i, :]\n",
    "        #print('!-- center truth\\n', center_truth)\n",
    "        center_est = regrModel(batchcrop).to(device)\n",
    "        #print('!-- center est\\n', center_est)\n",
    "        center_est = center_est + offset\n",
    "        #print('!-- offset center est\\n', center_est)\n",
    "        guesses.append(center_est)\n",
    "\n",
    "        loss2 = regrCriterion(center_truth, center_est)\n",
    "        losses2.update(loss2.item())\n",
    "\n",
    "    # Forward gass\n",
    "\n",
    "    #labels_est = torch.FloatTensor(\n",
    "        #predicted_class.detach().cpu().numpy())\n",
    "    \n",
    "    guesses = torch.stack(guesses)\n",
    "\n",
    "    # Loss\n",
    "    #loss2 = r_criterion(masked_est, masked_truth)\n",
    "\n",
    "print(\"loss across batch size of \", labels.size()[0], 'is: ', loss1, loss2)\n",
    "#print(labels)\n",
    "print('!-- guesses size', guesses.size())\n",
    "print('!-- labels size', labels.size())\n",
    "#print(torch.round(predicted_class))\n",
    "\n",
    "predicted_coords = guesses.view(-1, 9, 3)\n",
    "#print(predicted_class)\n",
    "#print(predicted_coords)\n",
    "print('!-- guesses size', predicted_coords.size())\n",
    "\n",
    "print(cropCoords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of crops torch.Size([9, 100, 100])\n",
      "!-- coords tensor([[  0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000],\n",
      "        [ 63.0000, 168.0000,   2.1468],\n",
      "        [ 63.0000, 168.0000,   2.1468],\n",
      "        [  0.0000,   0.0000,   0.0000]], device='cuda:0')\n",
      "!-- FOR N =  2\n",
      "y (crops) \n",
      "\t [0, 0, 0, 0, 0, 0, 1, 1, 0]\n",
      "!-- yhat (crops) \n",
      "\t [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "\n",
      "\n",
      "FOR N =  2\n",
      "!-- center y \n",
      "\t [63.0, 168.0, 2.1467549800872803]\n",
      "!-- center y est \n",
      "\t  [67.95642852783203, 85.26524353027344, 1.9021010398864746]\n",
      "tensor([[-40.0403,  -4.0040,   0.0102],\n",
      "        [-54.8233,  -5.9230,   0.0168],\n",
      "        [-54.8233,  -5.9230,   0.0168],\n",
      "        [-22.4451,  24.3090,   0.1506],\n",
      "        [-34.4039, -10.2041,  -0.0906],\n",
      "        [-32.7578,  -1.0648,  -0.0029],\n",
      "        [-47.2349, -35.7809,  -0.3068],\n",
      "        [ 15.7197, 139.2145,   1.7844],\n",
      "        [ 67.9564,  85.2652,   1.9021]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -- Plot windows and print labels -----------------------------------\n",
    "n = 2\n",
    "imgSample = images[n]\n",
    "crops, __ = makeCrops(imgSample, 50, (100, 100), (x,y, theta))\n",
    "# -- Show original image, and the sliding windo crops -------\n",
    "plt.imshow(imgSample)\n",
    "# ---------------------------------------------------\n",
    "foo_label = labels[n]\n",
    "\n",
    "foo_coord = coords[n] \n",
    "predicted_locs = predicted_coords.view(-1,\n",
    "                                     coords.size(1), coords.size(2))\n",
    "foo_coord_est = predicted_locs[n] # 3 per window\n",
    "foo_label_est = predicted_class[n]\n",
    "print('!-- coords', foo_coord)\n",
    "\n",
    "max_idx = torch.argmax(foo_coord[:,0])\n",
    "x, y, theta = foo_coord[max_idx]\n",
    "est_max_idx = torch.argmax(foo_coord_est[:,0])\n",
    "x_est, y_est, theta_est = foo_coord_est[est_max_idx]\n",
    "\n",
    "# -- Print x,y for one result -------\n",
    "# -- Print window t/f for one result -------\n",
    "print(\"!-- FOR N = \", n)\n",
    "print(\"y (crops) \\n\\t\", [int(l) for l in foo_label])\n",
    "print(\"!-- yhat (crops) \\n\\t\", [int(np.round(p,0)) for p in foo_label_est] )\n",
    "# -------------------------------------------------\n",
    "sns.set(rc={\"figure.figsize\": (8, 6)})\n",
    "\n",
    "print(\"\\n\\nFOR N = \", n)\n",
    "print(\"!-- center y \\n\\t\", [float(zed) for zed in (x, y, theta)])\n",
    "print(\"!-- center y est \\n\\t \", [float(zed) for zed in (x_est, y_est, theta_est)])\n",
    "print(foo_coord_est)\n",
    "\n",
    "fig, axess = plt.subplots(3,3, figsize=(10,10))\n",
    "axess = np.array(axess).flatten()#order='F') #'F' means to flatten in column-major (Fortran- style) order.\n",
    "for i in range(9):\n",
    "    axess[i].imshow(crops[i])\n",
    "    axess[i].set_ylabel('# '+str(i))\n",
    "plt.suptitle('Window')\n",
    "plt.show()\n",
    "\n",
    "#for (i, crop) in enumerate(crops):\n",
    "    # print(\"1-index number of window: \", i+1, 'x', x, 'y', y, 'has rectangle?', hasRect)\n",
    "    #plt.figure()\n",
    "    #plt.suptitle(\"numero: %d\" % (i))\n",
    "    #plt.imshow(crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!-- \n",
      "True locations, defined for each crop\n",
      " tensor([[  0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000],\n",
      "        [ 63.0000, 168.0000,   2.1468],\n",
      "        [ 63.0000, 168.0000,   2.1468],\n",
      "        [  0.0000,   0.0000,   0.0000]], device='cuda:0')\n",
      "!-- \n",
      "Full predicted locations (3 per crop)\n",
      " tensor([[-40.0403,  -4.0040,   0.0102],\n",
      "        [-54.8233,  -5.9230,   0.0168],\n",
      "        [-54.8233,  -5.9230,   0.0168],\n",
      "        [-22.4451,  24.3090,   0.1506],\n",
      "        [-34.4039, -10.2041,  -0.0906],\n",
      "        [-32.7578,  -1.0648,  -0.0029],\n",
      "        [-47.2349, -35.7809,  -0.3068],\n",
      "        [ 15.7197, 139.2145,   1.7844],\n",
      "        [ 67.9564,  85.2652,   1.9021]], device='cuda:0')\n",
      "tensor([[0, 1, 2, 3, 3, 7, 7, 7, 8, 8, 8],\n",
      "        [2, 2, 2, 1, 2, 0, 1, 2, 0, 1, 2]])\n",
      "\n",
      "SAMPLE  0\n",
      "!-- y (crops) \n",
      "\t [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "!-- yhat (crops) \n",
      "\t [0, 0, 0, 0, 1, 1, 0, 1, 1]\n",
      "\n",
      "SAMPLE  1\n",
      "!-- y (crops) \n",
      "\t [1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "!-- yhat (crops) \n",
      "\t [1, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "\n",
      "SAMPLE  2\n",
      "!-- y (crops) \n",
      "\t [0, 0, 0, 0, 0, 0, 1, 1, 0]\n",
      "!-- yhat (crops) \n",
      "\t [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "\n",
      "SAMPLE  3\n",
      "!-- y (crops) \n",
      "\t [0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "!-- yhat (crops) \n",
      "\t [0, 0, 0, 0, 1, 0, 0, 1, 0]\n",
      "\n",
      "SAMPLE  4\n",
      "!-- y (crops) \n",
      "\t [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "!-- yhat (crops) \n",
      "\t [0, 0, 0, 0, 1, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#print(\"!-- yhat \\n\\t\", [int(round(o, 0)) for o in outputs[n].cpu().numpy()])    \n",
    "print(\"!-- \")\n",
    "print('True locations, defined for each crop\\n', foo_coord)\n",
    "print(\"!-- \")\n",
    "print('Full predicted locations (3 per crop)\\n', foo_coord_est)\n",
    "#print(\"\\n ------ x,y center + orient: \", coords[n], \"\\n\"))\n",
    "print(np.argwhere(foo_coord_est > 0) )\n",
    "\n",
    "# -- Print outputs for multiple results -------\n",
    "for ix in range(5):\n",
    "    print('\\nSAMPLE ', ix)\n",
    "    print(\"!-- y (crops) \\n\\t\", [int(l) for l in labels[ix]])\n",
    "    print(\"!-- yhat (crops) \\n\\t\", [int(np.round(p, 0)) for p in predicted_class[ix]] )\n",
    "\n",
    "# -- Main ---------------------------------------------\n",
    "#if __name__ == '__main__':\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
