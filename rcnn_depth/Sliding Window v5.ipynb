{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Sliding Window\n",
    "# Feb 2019\n",
    "#\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import torchvision\n",
    "from IPython.display import display  # to display images\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import io\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sliding Window\n",
    "# Feb 2019\n",
    "# Implement classfication and regression as two separate networks\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import torchvision\n",
    "from IPython.display import display  # to display images\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import io\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "# In[2]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_X, IMG_Y = 200, 200\n",
    "# length and width of blocks (fixed for now)\n",
    "block_l, block_w = 20, 30\n",
    "\n",
    "\n",
    "# Make dataset -------------------------------------------------------\n",
    "\n",
    "def makeRectangle(l, w, theta, offset=(0, 0)):\n",
    "    c, s = math.cos(theta), math.sin(theta)\n",
    "    rectCoords = [(l / 2.0, w / 2.0), (l / 2.0, -w / 2.0),\n",
    "                  (-l / 2.0, -w / 2.0), (-l / 2.0, w / 2.0), ]\n",
    "    return [(c * x - s * y + offset[0],\n",
    "             s * x + c * y + offset[1]) for (x, y) in rectCoords]\n",
    "\n",
    "\n",
    "# ---- Make depth images ---\n",
    "def make_dataset(dirname, num_images):\n",
    "    true_coords = []\n",
    "    newpath = \"./\" + dirname\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "        print(newpath)\n",
    "    for i in range(num_images):\n",
    "        # orient = 0 # degrees\n",
    "        img = Image.new(\"RGB\", (IMG_X, IMG_Y), \"black\")\n",
    "\n",
    "        # block_l and _w offset so blocks don't run off edge of image\n",
    "        rand_x = int(np.random.rand() * (IMG_X - 2 * block_l)) + block_l\n",
    "        rand_y = int(np.random.rand() * (IMG_Y - 2 * block_w)) + block_w\n",
    "        orient = int(np.random.rand() * 180)  # .random() is range [0.0, 1.0).\n",
    "        orient = math.radians(orient)  # math.cos takes radians!\n",
    "        \n",
    "        rect_vertices = makeRectangle(\n",
    "            block_l, block_w, orient, offset=(rand_x, rand_y))\n",
    "\n",
    "       # true_coords.append(np.array((rand_x, rand_y, math.degrees(orient))))\n",
    "        true_coords.append(np.array((rand_y, rand_x, math.degrees(orient))))\n",
    "\n",
    "        idraw = ImageDraw.Draw(img)\n",
    "        idraw.polygon(rect_vertices, fill=\"white\")\n",
    "\n",
    "        img.save(newpath + \"/rect\" + str(i) + \".png\")\n",
    "    return true_coords\n",
    "\n",
    "\n",
    "# NOTE Define size of dataset\n",
    "train_truth = make_dataset(\"data\", 600)\n",
    "# print(len(train_truth))\n",
    "test_truth = make_dataset(\"./data/test\", 300)\n",
    "\n",
    "np.save(\"train_truth.npy\", train_truth)\n",
    "np.save(\"test_truth.npy\", test_truth)\n",
    "\n",
    "\n",
    "train_truth = np.load(\"train_truth.npy\")\n",
    "test_truth = np.load(\"test_truth.npy\")  # loading the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataloader -------------------------------------------------------\n",
    "\n",
    "class RectDepthImgsDataset(Dataset):\n",
    "    \"\"\"Artificially generated depth images dataset\"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, coords, transform=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.true_coords = coords\n",
    "        self.transform = transform\n",
    "\n",
    "        STEPSIZE = 50  # todo make this input arguments\n",
    "        WINDOWSIZE = (100, 100)\n",
    "        MARGIN_PX = 10\n",
    "        self.step = STEPSIZE\n",
    "        self.cropSize = WINDOWSIZE\n",
    "        self.detectMargin = MARGIN_PX\n",
    "\n",
    "    def __len__(self):\n",
    "        # print('true coord len', len(self.true_coords))\n",
    "        return len(self.true_coords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # image = self.images[idx]\n",
    "        image = io.imread(self.img_dir + \"/rect\" +\n",
    "                          str(idx) + \".png\", as_gray=True)\n",
    "        # image = torch.FloatTensor(image).permute(2, 0, 1)  # PIL and torch expect difft orders\n",
    "        coords = torch.FloatTensor(self.true_coords[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        crops, labels, cropCoords = self.makeCrops(\n",
    "            image, self.step, self.cropSize, coords, self.detectMargin)\n",
    "\n",
    "        sample = image, torch.FloatTensor(labels), torch.FloatTensor(cropCoords)\n",
    "        return sample\n",
    "\n",
    "    def makeCrops(self, image, stepSize, windowSize, rectCenter, detectMargin):\n",
    "        \"\"\"\n",
    "        Returns image crops, as well as T/F for those crops\n",
    "        \"\"\"\n",
    "        crops = []\n",
    "        # c_x, c_y, theta = rectCenter\n",
    "        c_y, c_x, theta = rectCenter\n",
    "        margin = detectMargin\n",
    "        hasRects = []\n",
    "        rectCoords = []\n",
    "        for y in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "            for x in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "                end_x, end_y = x + windowSize[1], y + windowSize[0]\n",
    "                hasRect = (x + margin < c_x < end_x - margin) and (\n",
    "                    y + margin < c_y < end_y - margin)\n",
    "                crops.append(image[y:end_y, x:end_x])\n",
    "                hasRects.append(hasRect)\n",
    "                if hasRect:\n",
    "                    rectCoords.append((c_y, c_x, theta))\n",
    "                else:\n",
    "                    # NOTE: Return empty label, when not hasRect\n",
    "                    rectCoords.append((0, 0, 0))\n",
    "        # print('length of truths in makeCrops', len(truths))\n",
    "        return crops, hasRects, rectCoords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Define NN -------------------------------------------------------\n",
    "\n",
    "class regrNet(nn.Module):\n",
    "    def __init__(self, cropSize, numOutputs):\n",
    "        \"\"\"\n",
    "        We need the image width and height to determine CNN layer sizes\n",
    "        \"\"\"\n",
    "        super(regrNet, self).__init__()\n",
    "        _pool = 2\n",
    "        _stride = 5\n",
    "        _outputlayers = 16\n",
    "\n",
    "        STEPSIZE = 50  # todo make this input arguments\n",
    "        WINDOWSIZE = (100, 100)\n",
    "        self.step = STEPSIZE\n",
    "        self.cropSize = cropSize\n",
    "        self.numOutputs = numOutputs\n",
    "\n",
    "        self.numCrops = 0\n",
    "        # calculate number of crops\n",
    "        for y in range(0, IMG_Y - WINDOWSIZE[0] + 1, STEPSIZE):\n",
    "            for x in range(0, IMG_X - WINDOWSIZE[1] + 1, STEPSIZE):\n",
    "                self.numCrops += 1\n",
    "\n",
    "        def _calc(val):  # use to calculate layer sizes\n",
    "            layer_size = (val - (_stride - 1)) / _pool\n",
    "            return layer_size\n",
    "\n",
    "        self._const = _calc(_calc(self.cropSize[0]))\n",
    "        self._const *= _calc(_calc(self.cropSize[1]))\n",
    "        self._const *= _outputlayers\n",
    "        self._const = int(self._const)\n",
    "\n",
    "        # --- LOCATION OF RECTANGLE\n",
    "        # NOTE: only one channel for now (black/white)\n",
    "        self.conv1 = nn.Conv2d(self.numCrops, 6, _stride).to(device)\n",
    "        self.bn = nn.BatchNorm2d(6)\n",
    "        self.pool = nn.MaxPool2d(_pool, _pool).to(device)\n",
    "        self.conv2 = nn.Conv2d(6, _outputlayers, _stride).to(device)\n",
    "        self.fc1 = nn.Linear(self._const, 120).to(device)\n",
    "        self.fc2 = nn.Linear(120, 84).to(device)\n",
    "        self.fc3 = nn.Linear(84, self.numOutputs * self.numCrops).to(device)\n",
    "\n",
    "    def forward(self, crops, labels):\n",
    "        \"\"\"\n",
    "        Forward propogation\n",
    "        : param image: images, a tensor of dimensions(N, 3, IMG_X, IMG_Y)\n",
    "        : return: (x, y, theta) and T/F for each window\n",
    "        \"\"\"\n",
    "        # TODO: presumably by doing this i lose some of the multithread goodness\n",
    "        # print('!--label ', labels)\n",
    "        # print('!--label size', labels.size)\n",
    "        # print('!--crops size', crops.size)\n",
    "        crops = crops.to(device)\n",
    "\n",
    "        crops = self.zeroCrops(crops, labels)\n",
    "\n",
    "        # LOCALIZATION\n",
    "        regr_crops = self.pool(F.relu((self.bn(self.conv1(crops)))))\n",
    "        regr_crops = self.pool(F.relu(self.conv2(regr_crops)))\n",
    "        regr_crops = regr_crops.view(-1, self._const)\n",
    "        regr_crops = F.relu(self.fc1(regr_crops))\n",
    "        regr_crops = F.relu(self.fc2(regr_crops))\n",
    "        regr_crops = self.fc3(regr_crops)\n",
    "\n",
    "        objCoords = regr_crops\n",
    "        \n",
    "        # reshape to batchsize x number of crops x 3\n",
    "        objCoords = objCoords.reshape(-1, self.numCrops, self.numOutputs)\n",
    "        return objCoords\n",
    "\n",
    "    def zeroCrops(self, crops, labels):\n",
    "        # 15 x 9\n",
    "        # 15 x 9 x 100 x 100\n",
    "        labels = torch.FloatTensor(labels).to(device)\n",
    "        mask = labels.unsqueeze(2)\n",
    "        mask.unsqueeze_(3)\n",
    "        crops = crops * mask\n",
    "        return crops\n",
    "\n",
    "\n",
    "class classifNet(nn.Module):  # CIFAR is 32x32x3, MNIST is 28x28x1)\n",
    "    def __init__(self, IMG_X, IMG_Y):\n",
    "        \"\"\"\n",
    "        We need the image width and height to determine CNN layer sizes\n",
    "        \"\"\"\n",
    "        super(classifNet, self).__init__()\n",
    "        self._imgx = IMG_X\n",
    "        self._imgy = IMG_Y\n",
    "        _pool = 2\n",
    "        _stride = 5\n",
    "        _outputlayers = 16\n",
    "\n",
    "        STEPSIZE = 50  # todo make this input arguments\n",
    "        WINDOWSIZE = (100, 100)\n",
    "        self.step = STEPSIZE\n",
    "        self.cropSize = WINDOWSIZE\n",
    "        self.numCrops = 0\n",
    "        # T/F for now\n",
    "\n",
    "        # calculate number of crops\n",
    "        for y in range(0, IMG_Y - WINDOWSIZE[0] + 1, STEPSIZE):\n",
    "            for x in range(0, IMG_X - WINDOWSIZE[1] + 1, STEPSIZE):\n",
    "                self.numCrops += 1\n",
    "\n",
    "        def _calc(val):  # use to calculate layer sizes\n",
    "            layer_size = (val - (_stride - 1)) / _pool\n",
    "            return layer_size\n",
    "\n",
    "        # print(self._imgx)\n",
    "        # self._const = _calc(_calc(self._imgx))\n",
    "        # self._const *= _calc(_calc(self._imgy))\n",
    "        self._const = _calc(_calc(self.cropSize[0]))\n",
    "        self._const *= _calc(_calc(self.cropSize[1]))\n",
    "        self._const *= _outputlayers\n",
    "        self._const = int(self._const)\n",
    "\n",
    "        # --- CLASSIFICATION OF WINDOWS\n",
    "        # batch, 3 input image channels (RGB), 6 output channels, 5x5 square convolution\n",
    "        # NOTE: we switched to 1 input channel\n",
    "        self.conv1 = nn.Conv2d(self.numCrops, 6, _stride).to(device)\n",
    "        self.bn = nn.BatchNorm2d(6)\n",
    "        self.pool = nn.MaxPool2d(_pool, _pool).to(device)\n",
    "        self.conv2 = nn.Conv2d(6, _outputlayers, _stride).to(device)\n",
    "        self.fc1 = nn.Linear(self._const, 120).to(device)\n",
    "        self.fc2 = nn.Linear(120, 84).to(device)\n",
    "        self.fc3 = nn.Linear(84, self.numCrops).to(device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # TODO: batch normalization  self.bn = nn.BatchNorm2d()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propogation\n",
    "        : param image: images, a tensor of dimensions(N, 3, IMG_X, IMG_Y)\n",
    "        : return: (x, y, theta) and T/F for each window\n",
    "        \"\"\"\n",
    "        x = x.to(device)\n",
    "        batch_images = x\n",
    "\n",
    "        all_crops = []\n",
    "        for img in batch_images:\n",
    "            crops = self.makeCrops(img, self.step, self.cropSize)\n",
    "            all_crops.append(crops)\n",
    "            # all_crops.append(crops)\n",
    "        all_crops = torch.stack(all_crops)\n",
    "        feats = all_crops.view(-1, self.numCrops, self.cropSize[0],\n",
    "                               self.cropSize[1]).to(device)\n",
    "\n",
    "        # CLASSIFICATION of the windows\n",
    "        c_crops = self.pool(F.relu((self.bn(self.conv1(feats)))))\n",
    "        c_crops = self.pool(F.relu(self.conv2(c_crops)))\n",
    "        c_crops = c_crops.view(-1, self._const)\n",
    "        c_crops = F.relu(self.fc1(c_crops))\n",
    "        c_crops = F.relu(self.fc2(c_crops))\n",
    "        c_crops = self.fc3(c_crops)\n",
    "        c_crops = self.sigmoid(c_crops)\n",
    "\n",
    "        containsObj = c_crops\n",
    "\n",
    "        return containsObj, all_crops\n",
    "\n",
    "    def makeCrops(self, image, stepSize, windowSize):\n",
    "        \"\"\"\n",
    "        Returns a generator of cropped boxes(the top left x, y, the image data)\n",
    "        \"\"\"\n",
    "        image = image.type(torch.FloatTensor).to(device)\n",
    "        crops = []\n",
    "\n",
    "        # TODO: look into ordering, why it's y,x !\n",
    "        for y in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "            for x in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "                end_x, end_y = x + windowSize[1], y + windowSize[0]\n",
    "                # print('This is the x and y used: ', x, '; ', y)\n",
    "                crops.append(image[y:end_y, x:end_x])\n",
    "        crops = torch.stack(crops)\n",
    "        # self.numCrops=len(crops)\n",
    "        return crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Utility fxn -------------------------------------------------------\n",
    "# Source: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/train.py\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Keeps track of most recent, average, sum, and count of a metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "    epoch, epochs_since_improvement, model1, model2, optimizer1, optimizer2, loss, loss2, best_loss, is_best\n",
    "):\n",
    "    \"\"\"\n",
    "    Save model checkpoint.\n",
    "    :param epoch: epoch number\n",
    "    :param epochs_since_improvement: number of epochs since last improvement\n",
    "    :param model: model\n",
    "    :param optimizer: optimizer\n",
    "    :param loss: validation loss in this epoch\n",
    "    :param best_loss: best validation loss achieved so far (not necessarily in this checkpoint)\n",
    "    :param is_best: is this checkpoint the best so far?\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"epochs_since_improvement\": epochs_since_improvement,\n",
    "        \"loss\": loss,\n",
    "        \"loss2\": loss2,\n",
    "        \"best_loss\": best_loss,\n",
    "        \"model1\": model1,\n",
    "        \"model2\": model2,\n",
    "        \"optimizer1\": optimizer1,\n",
    "        \"optimizer2\": optimizer2,\n",
    "    }\n",
    "    filename = \"checkpoint_v2sliding.pth.tar\"\n",
    "    torch.save(state, filename)\n",
    "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
    "    if is_best:\n",
    "        torch.save(state, \"BEST_\" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Define Train and Valid fxn -------------------------------------------------------\n",
    "\n",
    "def train(train_loader, classifModel, regrModel, classifCriterion,\n",
    "          regrCriterion, optimizer1, optimizer2, epoch):\n",
    "    \"\"\"\n",
    "    One epoch's training.\n",
    "    : param train_loader: DataLoader for training data\n",
    "    : param model: model\n",
    "    : param criterion: for classification (crop contains an Obj, t/f)\n",
    "    : param criterion: for regresion (of the x,y, theta)\n",
    "    : param optimizer: optimizer\n",
    "    : param epoch: epoch number\n",
    "    \"\"\"\n",
    "\n",
    "    classifModel.train()  # training mode enables dropout\n",
    "    regrModel.train()  # training mode enables dropout\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
    "    data_time = AverageMeter()  # data loading time\n",
    "    losses = AverageMeter()  # loss\n",
    "    start = time.time()\n",
    "    \n",
    "    plt_loss_list = []\n",
    "    plt_loss2_list = []\n",
    "\n",
    "    for i_batch, (images, labels, coords) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predicted_class, all_crops = classifModel(images)\n",
    "\n",
    "        loss1 = classifCriterion(predicted_class, labels)\n",
    "        optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "\n",
    "        # Update model\n",
    "        optimizer1.step()\n",
    "        losses.update(loss1.item())\n",
    "\n",
    "        batch_time.update(time.time() - start)\n",
    "        start = time.time()\n",
    "\n",
    "        # Print status\n",
    "        if i_batch % print_freq == 0:\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}]\\t\"\n",
    "                \"Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(\n",
    "                    epoch,\n",
    "                    i_batch,\n",
    "                    len(train_loader),\n",
    "                    batch_time=batch_time,\n",
    "                    loss=losses,\n",
    "                )\n",
    "            )\n",
    "        # free some memory since their histories may be stored\n",
    "        plt_loss_list.append(loss1.item())\n",
    "        del predicted_class, images, labels, all_crops\n",
    "\n",
    "    losses2 = AverageMeter()  # loss\n",
    "\n",
    "    for i_batch, (images, labels, coords) in enumerate(train_loader):\n",
    "        #print('\\n!--- \\n training regression!')\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        images = images.to(device)\n",
    "        coords = coords.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predicted_class, all_crops = classifModel(images)\n",
    "\n",
    "        # Forward pass\n",
    "        labelly = predicted_class.detach().cpu().numpy()\n",
    "        predicted_coords = regrModel(all_crops, labelly)\n",
    "\n",
    "        loss2 = regrCriterion(predicted_coords, coords)\n",
    "        optimizer2.zero_grad()\n",
    "        loss2.backward()\n",
    "\n",
    "        optimizer2.step()\n",
    "\n",
    "        batch_time.update(time.time() - start)\n",
    "        start = time.time()\n",
    "\n",
    "        losses2.update(loss2.item())\n",
    "\n",
    "        # Print status\n",
    "        if i_batch % print_freq == 0:\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}]\\t\"\n",
    "                \"Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(\n",
    "                    epoch,\n",
    "                    i_batch,\n",
    "                    len(train_loader),\n",
    "                    batch_time=batch_time,\n",
    "                    loss=losses,\n",
    "                )\n",
    "            )\n",
    "        # free some memory since their histories may be stored\n",
    "        \n",
    "        plt_loss2_list.append(loss2.item())\n",
    "        del predicted_class, predicted_coords, images, coords, all_crops\n",
    "\n",
    "\n",
    "def validate(val_loader, c_model, r_model, c_criterion, r_criterion):\n",
    "    \"\"\"\n",
    "    One epoch's validation.\n",
    "    : param val_loader: DataLoader for validation data\n",
    "    : param model: model\n",
    "    : param criterion: MultiBox loss\n",
    "    : return: average validation loss\n",
    "    \"\"\"\n",
    "    c_model.eval()  # eval mode disables dropout\n",
    "    r_model.eval()  # eval mode disables dropout\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses2 = AverageMeter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Prohibit gradient computation explicity because I had some problems with memory\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for i_batch, (images, labels, coords) in enumerate(train_loader):\n",
    "            # Move to default device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            coords = coords.to(device)\n",
    "\n",
    "            predicted_class, all_crops = c_model(images)\n",
    "            loss = c_criterion(predicted_class, labels)\n",
    "\n",
    "            labelly = predicted_class.detach().cpu().numpy()\n",
    "            predicted_coords = r_model(all_crops, labelly)\n",
    "\n",
    "            loss2 = r_criterion(predicted_coords, coords)\n",
    "\n",
    "            losses.update(loss.item())\n",
    "            losses2.update(loss2.item())\n",
    "\n",
    "            batch_time.update(time.time() - start)\n",
    "            start = time.time()\n",
    "\n",
    "            # Print status\n",
    "            if i_batch % print_freq == 0:\n",
    "                print(\n",
    "                    \"[{0}/{1}]\\t\"\n",
    "                    \"Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                    \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(\n",
    "                        i_batch, len(val_loader), batch_time=batch_time, loss=losses\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    print(\"\\n * LOSS - {loss.avg:.3f}\\n\".format(loss=losses))\n",
    "\n",
    "    return losses.avg, losses2.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available? device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# -- Load data -------------------------------------------------------\n",
    "batch_size = 15\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA available? device: \", device)\n",
    "\n",
    "\n",
    "# Dataset is depth images of rectangular blocks\n",
    "train_dataset = RectDepthImgsDataset(img_dir=\"./data\", coords=train_truth)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = RectDepthImgsDataset(img_dir=\"./data/test\", coords=test_truth)\n",
    "\n",
    "# Data loader\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Hyperparamaters -------------------------\n",
    "\n",
    "num_epochs = 50  # number of epochs to run without early-stopping\n",
    "learning_rate = 0.001\n",
    "\n",
    "start_epoch = 0  # start at this epoch\n",
    "# number of epochs since there was an improvement in the validation metric\n",
    "epochs_since_improvement = 0\n",
    "best_loss = 1000.0  # assume a high loss at first\n",
    "workers = 4  # number of workers for loading data in the DataLoader\n",
    "\n",
    "classifModel = classifNet(IMG_X, IMG_Y)\n",
    "classifModel = classifModel.to(device)\n",
    "\n",
    "regrModel = regrNet((100, 100), 3)  # crop size in pixels; output x,y, theta\n",
    "regrModel = regrModel.to(device)\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "classifCriterion = nn.BCELoss()\n",
    "regrCriterion = nn.MSELoss()\n",
    "optimizer1 = torch.optim.Adam(classifModel.parameters(), lr=learning_rate)\n",
    "optimizer2 = torch.optim.Adam(regrModel.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,\n",
    "#                             momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "print_freq = 25  # print training or validation status every __ batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Training and validation.\n",
    "    \"\"\"\n",
    "\n",
    "    global epochs_since_improvement, start_epoch, best_loss, epoch, checkpoint\n",
    "\n",
    "    print(\"Training model now...\")\n",
    "\n",
    "    # -- Begin training -------------------------\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train(\n",
    "            train_loader=train_loader,\n",
    "            classifModel=classifModel,\n",
    "            regrModel=regrModel,\n",
    "            classifCriterion=classifCriterion,\n",
    "            regrCriterion=regrCriterion,\n",
    "            optimizer1=optimizer1,\n",
    "            optimizer2=optimizer2,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "\n",
    "        # One epoch's validation\n",
    "        val_loss, regr_loss = validate(val_loader=test_loader,\n",
    "                                       c_model=classifModel, r_model=regrModel,\n",
    "                                       c_criterion=classifCriterion,\n",
    "                                       r_criterion=regrCriterion)\n",
    "\n",
    "        # Did validation loss improve?\n",
    "        is_best = val_loss < best_loss\n",
    "        best_loss = min(val_loss, best_loss)\n",
    "\n",
    "        if not is_best:\n",
    "            epochs_since_improvement += 1\n",
    "            print(\"\\nEpochs since last improvement: %d\\n\" %\n",
    "                  (epochs_since_improvement,))\n",
    "\n",
    "        else:\n",
    "            epochs_since_improvement = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(epoch, epochs_since_improvement, classifModel, regrModel, optimizer1,\n",
    "                        optimizer2, val_loss, regr_loss, best_loss, is_best)\n",
    "\n",
    "    print(\"All ready!\")\n",
    "\n",
    "    # -- Check the results -------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running main!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model now...\n",
      "Epoch: [0][0/40]\tBatch Time 0.030 (0.030)\tLoss 0.7002 (0.7002)\t\n",
      "Epoch: [0][25/40]\tBatch Time 0.024 (0.025)\tLoss 0.2763 (0.4843)\t\n",
      "Epoch: [0][0/40]\tBatch Time 0.025 (0.024)\tLoss 0.1043 (0.3817)\t\n",
      "Epoch: [0][25/40]\tBatch Time 0.025 (0.025)\tLoss 0.1043 (0.3817)\t\n",
      "[0/20]\tBatch Time 0.023 (0.023)\tLoss 0.1030 (0.1030)\t\n",
      "[25/20]\tBatch Time 0.023 (0.023)\tLoss 0.1154 (0.1140)\t\n",
      "\n",
      " * LOSS - 0.112\n",
      "\n",
      "Epoch: [1][0/40]\tBatch Time 0.026 (0.026)\tLoss 0.1009 (0.1009)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rui/mlenv/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type classifNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/rui/mlenv/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type regrNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][25/40]\tBatch Time 0.024 (0.024)\tLoss 0.0644 (0.1143)\t\n",
      "Epoch: [1][0/40]\tBatch Time 0.025 (0.024)\tLoss 0.0531 (0.1005)\t\n",
      "Epoch: [1][25/40]\tBatch Time 0.025 (0.025)\tLoss 0.0531 (0.1005)\t\n",
      "[0/20]\tBatch Time 0.023 (0.023)\tLoss 0.0576 (0.0576)\t\n",
      "[25/20]\tBatch Time 0.023 (0.023)\tLoss 0.0688 (0.0607)\t\n",
      "\n",
      " * LOSS - 0.060\n",
      "\n",
      "Epoch: [2][0/40]\tBatch Time 0.025 (0.025)\tLoss 0.0408 (0.0408)\t\n",
      "Epoch: [2][25/40]\tBatch Time 0.024 (0.024)\tLoss 0.0234 (0.0579)\t\n",
      "Epoch: [2][0/40]\tBatch Time 0.024 (0.024)\tLoss 0.0469 (0.0565)\t\n",
      "Epoch: [2][25/40]\tBatch Time 0.025 (0.024)\tLoss 0.0469 (0.0565)\t\n",
      "[0/20]\tBatch Time 0.023 (0.023)\tLoss 0.0931 (0.0931)\t\n",
      "[25/20]\tBatch Time 0.023 (0.023)\tLoss 0.0335 (0.0474)\t\n",
      "\n",
      " * LOSS - 0.050\n",
      "\n",
      "Epoch: [3][0/40]\tBatch Time 0.026 (0.026)\tLoss 0.1083 (0.1083)\t\n",
      "Epoch: [3][25/40]\tBatch Time 0.024 (0.024)\tLoss 0.0511 (0.0524)\t\n",
      "Epoch: [3][0/40]\tBatch Time 0.024 (0.024)\tLoss 0.0104 (0.0468)\t\n",
      "Epoch: [3][25/40]\tBatch Time 0.025 (0.025)\tLoss 0.0104 (0.0468)\t\n",
      "[0/20]\tBatch Time 0.023 (0.023)\tLoss 0.0313 (0.0313)\t\n",
      "[25/20]\tBatch Time 0.023 (0.023)\tLoss 0.0100 (0.0217)\t\n",
      "\n",
      " * LOSS - 0.029\n",
      "\n",
      "Epoch: [4][0/40]\tBatch Time 0.025 (0.025)\tLoss 0.0418 (0.0418)\t\n",
      "Epoch: [4][25/40]\tBatch Time 0.024 (0.024)\tLoss 0.0452 (0.0291)\t\n",
      "Epoch: [4][0/40]\tBatch Time 0.024 (0.024)\tLoss 0.0139 (0.0315)\t\n",
      "Epoch: [4][25/40]\tBatch Time 0.025 (0.025)\tLoss 0.0139 (0.0315)\t\n",
      "[0/20]\tBatch Time 0.023 (0.023)\tLoss 0.0076 (0.0076)\t\n",
      "[25/20]\tBatch Time 0.023 (0.023)\tLoss 0.0089 (0.0262)\t\n",
      "\n",
      " * LOSS - 0.026\n",
      "\n",
      "Epoch: [5][0/40]\tBatch Time 0.024 (0.024)\tLoss 0.0112 (0.0112)\t\n",
      "Epoch: [5][25/40]\tBatch Time 0.024 (0.024)\tLoss 0.0030 (0.0220)\t\n",
      "Epoch: [5][0/40]\tBatch Time 0.024 (0.024)\tLoss 0.0216 (0.0273)\t\n",
      "Epoch: [5][25/40]\tBatch Time 0.025 (0.025)\tLoss 0.0216 (0.0273)\t\n",
      "[0/20]\tBatch Time 0.023 (0.023)\tLoss 0.0160 (0.0160)\t\n",
      "[25/20]\tBatch Time 0.023 (0.023)\tLoss 0.0419 (0.0261)\t\n",
      "\n",
      " * LOSS - 0.027\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [6][0/40]\tBatch Time 0.024 (0.024)\tLoss 0.0195 (0.0195)\t\n",
      "Epoch: [6][25/40]\tBatch Time 0.024 (0.024)\tLoss 0.0227 (0.0301)\t\n",
      "Epoch: [6][0/40]\tBatch Time 0.024 (0.024)\tLoss 0.0197 (0.0282)\t\n",
      "Epoch: [6][25/40]\tBatch Time 0.025 (0.025)\tLoss 0.0197 (0.0282)\t\n",
      "[0/20]\tBatch Time 0.023 (0.023)\tLoss 0.0634 (0.0634)\t\n",
      "[25/20]\tBatch Time 0.023 (0.023)\tLoss 0.0240 (0.0133)\t\n",
      "\n",
      " * LOSS - 0.013\n",
      "\n",
      "Epoch: [7][0/40]\tBatch Time 0.024 (0.024)\tLoss 0.0330 (0.0330)\t\n",
      "Epoch: [7][25/40]\tBatch Time 0.024 (0.024)\tLoss 0.0054 (0.0150)\t\n",
      "Epoch: [7][0/40]\tBatch Time 0.025 (0.024)\tLoss 0.0145 (0.0207)\t\n",
      "Epoch: [7][25/40]\tBatch Time 0.025 (0.025)\tLoss 0.0145 (0.0207)\t\n",
      "[0/20]\tBatch Time 0.023 (0.023)\tLoss 0.0107 (0.0107)\t\n",
      "[25/20]\tBatch Time 0.023 (0.023)\tLoss 0.0055 (0.0179)\t\n",
      "\n",
      " * LOSS - 0.016\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [8][0/40]\tBatch Time 0.024 (0.024)\tLoss 0.0242 (0.0242)\t\n",
      "Epoch: [8][25/40]\tBatch Time 0.024 (0.024)\tLoss 0.0585 (0.0181)\t\n",
      "Epoch: [8][0/40]\tBatch Time 0.024 (0.024)\tLoss 0.0136 (0.0197)\t\n",
      "Epoch: [8][25/40]\tBatch Time 0.025 (0.024)\tLoss 0.0136 (0.0197)\t\n",
      "[0/20]\tBatch Time 0.023 (0.023)\tLoss 0.0107 (0.0107)\t\n",
      "[25/20]\tBatch Time 0.023 (0.023)\tLoss 0.0041 (0.0113)\t\n",
      "\n",
      " * LOSS - 0.012\n",
      "\n",
      "Epoch: [9][0/40]\tBatch Time 0.025 (0.025)\tLoss 0.0062 (0.0062)\t\n",
      "Epoch: [9][25/40]\tBatch Time 0.024 (0.024)\tLoss 0.0054 (0.0112)\t\n",
      "Epoch: [9][0/40]\tBatch Time 0.024 (0.024)\tLoss 0.0057 (0.0128)\t\n",
      "Epoch: [9][25/40]\tBatch Time 0.025 (0.025)\tLoss 0.0057 (0.0128)\t\n",
      "[0/20]\tBatch Time 0.023 (0.023)\tLoss 0.0018 (0.0018)\t\n",
      "[25/20]\tBatch Time 0.023 (0.023)\tLoss 0.0032 (0.0112)\t\n",
      "\n",
      " * LOSS - 0.011\n",
      "\n",
      "Epoch: [10][0/40]\tBatch Time 0.025 (0.025)\tLoss 0.0098 (0.0098)\t\n",
      "Epoch: [10][25/40]\tBatch Time 0.024 (0.024)\tLoss 0.0012 (0.0087)\t\n",
      "Epoch: [10][0/40]\tBatch Time 0.025 (0.024)\tLoss 0.0260 (0.0105)\t\n",
      "Epoch: [10][25/40]\tBatch Time 0.025 (0.025)\tLoss 0.0260 (0.0105)\t\n",
      "[0/20]\tBatch Time 0.023 (0.023)\tLoss 0.0047 (0.0047)\t\n",
      "[25/20]\tBatch Time 0.023 (0.023)\tLoss 0.0092 (0.0161)\t\n",
      "\n",
      " * LOSS - 0.012\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [11][0/40]\tBatch Time 0.024 (0.024)\tLoss 0.0149 (0.0149)\t\n",
      "Epoch: [11][25/40]\tBatch Time 0.025 (0.025)\tLoss 0.0651 (0.0214)\t\n",
      "Epoch: [11][0/40]\tBatch Time 0.024 (0.025)\tLoss 0.0806 (0.0211)\t\n",
      "Epoch: [11][25/40]\tBatch Time 0.025 (0.025)\tLoss 0.0806 (0.0211)\t\n",
      "[0/20]\tBatch Time 0.023 (0.023)\tLoss 0.0024 (0.0024)\t\n",
      "[25/20]\tBatch Time 0.023 (0.023)\tLoss 0.0260 (0.0077)\t\n",
      "\n",
      " * LOSS - 0.012\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [12][0/40]\tBatch Time 0.024 (0.024)\tLoss 0.0311 (0.0311)\t\n"
     ]
    }
   ],
   "source": [
    "main()\n",
    "\n",
    "# alert when training is done\n",
    "sound_file = '/home/rui/Downloads/newyear.ogg'\n",
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All Ready!')\n",
    "\n",
    "\n",
    "filename = \"checkpoint_v2sliding.pth.tar\"\n",
    "checkpoint = torch.load(filename)\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "best_loss = checkpoint['best_loss']\n",
    "print('\\nLoaded checkpoint from epoch %d. Best loss so far is %.3f.\\n' % (start_epoch, best_loss))\n",
    "classifModel = checkpoint['model1']\n",
    "regrModel = checkpoint['model2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_loader)\n",
    "images, labels, coords = dataiter.next()\n",
    "\n",
    "n = 1\n",
    "print(\"\\n\\nFOR N = \", n)\n",
    "imgSample = images[n]\n",
    "max_idx = torch.argmax(coords[n][:,0])\n",
    "x, y, theta = coords[n][max_idx]\n",
    "print(labels[n])\n",
    "print([float(zed) for zed in (x,y,theta)])\n",
    "#print(x,y,theta)\n",
    "#crops, __ = makeCrops(imgSample, 50, (100, 100), (x,y, theta))\n",
    "\n",
    "plt.imshow(imgSample)\n",
    "#fig, axess = plt.subplots(3,3, figsize=(10,10))\n",
    "#axess = np.array(axess).flatten()#order='F') #'F' means to flatten in column-major (Fortran- style) order.\n",
    "#for i in range(9):\n",
    "#    axess[i].imshow(crops[i])\n",
    "#    axess[i].set_ylabel('# '+str(i))\n",
    "plt.suptitle('Window')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Check the results -------------------------------------------------------\n",
    "\n",
    "# -- Utility ---------------------------------------------\n",
    "def makeCrops(image, stepSize, windowSize, true_center):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    image = image.type(torch.FloatTensor)\n",
    "    crops = []\n",
    "    truths = []\n",
    "    c_x, c_y, orient = true_center\n",
    "    # TODO: look into otdering, why it's y,x !\n",
    "    margin = 10 \n",
    "    # --> is x, but is the column\n",
    "    # to slide horizontally, y must come first\n",
    "    for y in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "        for x in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "            end_x, end_y = x + windowSize[1], y + windowSize[0]\n",
    "            hasRect = (x + margin < c_x < end_x - margin) and (\n",
    "                y + margin < c_y < end_y - margin\n",
    "            )\n",
    "            truths.append(hasRect)\n",
    "            crops.append(image[y:end_y, x:end_x])\n",
    "    crops = torch.stack(crops)\n",
    "    print(\"shape of crops\", crops.shape)\n",
    "    return crops, truths\n",
    "\n",
    "\n",
    "losses = AverageMeter()  # loss\n",
    "losses2 = AverageMeter()  # loss\n",
    "\n",
    "\n",
    "# -- Get some validation results ---------------------------------------------\n",
    "classifModel.to(device).eval()\n",
    "regrModel.to(device).eval()\n",
    "\n",
    "c_criterion = nn.BCELoss()\n",
    "r_criterion = nn.MSELoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    dataiter = iter(test_loader)\n",
    "    images, labels, coords = dataiter.next()\n",
    "    \n",
    "    # Move to default device\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    coords = coords.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    predicted_class, all_crops = classifModel(images)\n",
    "    # Loss\n",
    "    loss = c_criterion(predicted_class, labels)\n",
    "    all_crops = all_crops.to(device)\n",
    "    \n",
    "    loss1 = classifCriterion(predicted_class, labels)\n",
    "\n",
    "    # Forward pass\n",
    "    labelly = predicted_class.detach().cpu().numpy()\n",
    "    predicted_coords = regrModel(all_crops, labelly)\n",
    "\n",
    "    loss2 = regrCriterion(predicted_coords, coords)\n",
    "\n",
    "\n",
    "\n",
    "    losses2.update(loss2.item())\n",
    "\n",
    "        # Print status\n",
    "    \n",
    "print(\"loss across batch size of \", labels.size()[0], 'is: ', loss1, loss2)\n",
    "#print(labels)\n",
    "print('!-- labels size', labels.size())\n",
    "#print(torch.round(predicted_class))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(losses)\n",
    "plt.plot(range(len(losses), losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Plot windows and print labels -----------------------------------\n",
    "\n",
    "n = np.random.randint(10)\n",
    "imgSample = images[n]\n",
    "crops, __ = makeCrops(imgSample, 50, (100, 100), (x,y, theta))\n",
    "# -- Show original image, and the sliding windo crops -------\n",
    "plt.imshow(imgSample)\n",
    "# ---------------------------------------------------\n",
    "foo_label = labels[n]\n",
    "\n",
    "foo_coord = coords[n] \n",
    "predicted_locs = predicted_coords.view(-1,\n",
    "                                     coords.size(1), coords.size(2))\n",
    "foo_coord_est = predicted_locs[n] # 3 per window\n",
    "foo_label_est = predicted_class[n]\n",
    "print('!-- coords', foo_coord)\n",
    "\n",
    "max_idx = torch.argmax(foo_coord[:,0])\n",
    "x, y, theta = foo_coord[max_idx]\n",
    "est_max_idx = torch.argmax(foo_coord_est[:,0])\n",
    "x_est, y_est, theta_est = foo_coord_est[est_max_idx]\n",
    "\n",
    "# -- Print x,y for one result -------\n",
    "# -- Print window t/f for one result -------\n",
    "print(\"!-- FOR N = \", n)\n",
    "print(\"y (crops) \\n\\t\", [int(l) for l in foo_label])\n",
    "print(\"!-- yhat (crops) \\n\\t\", [int(np.round(p,0)) for p in foo_label_est] )\n",
    "# -------------------------------------------------\n",
    "sns.set(rc={\"figure.figsize\": (8, 6)})\n",
    "\n",
    "print(\"\\n\\nFOR N = \", n)\n",
    "print(\"!-- center y \\n\\t\", [float(zed) for zed in (x, y, theta)])\n",
    "print(\"!-- center y est \\n\\t \", [float(zed) for zed in (x_est, y_est, theta_est)])\n",
    "print(foo_coord_est)\n",
    "\n",
    "fig, axess = plt.subplots(3,3, figsize=(10,10))\n",
    "axess = np.array(axess).flatten()#order='F') #'F' means to flatten in column-major (Fortran- style) order.\n",
    "for i in range(9):\n",
    "    axess[i].imshow(crops[i])\n",
    "    axess[i].set_ylabel('# '+str(i))\n",
    "plt.suptitle('Window')\n",
    "plt.show()\n",
    "\n",
    "#for (i, crop) in enumerate(crops):\n",
    "    # print(\"1-index number of window: \", i+1, 'x', x, 'y', y, 'has rectangle?', hasRect)\n",
    "    #plt.figure()\n",
    "    #plt.suptitle(\"numero: %d\" % (i))\n",
    "    #plt.imshow(crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
