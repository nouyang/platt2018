{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sliding Window\n",
    "# Feb 2019\n",
    "#\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import torchvision\n",
    "from IPython.display import display  # to display images\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import io\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataset -------------------------------------------------------\n",
    "IMG_X, IMG_Y = 200, 200\n",
    "# length and width of blocks (fixed for now)\n",
    "block_l, block_w = 20, 30\n",
    "\n",
    "\n",
    "def makeRectangle(l, w, theta, offset=(0, 0)):\n",
    "    c, s = math.cos(theta), math.sin(theta)\n",
    "    rectCoords = [(l / 2.0, w / 2.0), (l / 2.0, -w / 2.0),\n",
    "                  (-l / 2.0, -w / 2.0), (-l / 2.0, w / 2.0), ]\n",
    "    return [(c * x - s * y + offset[0],\n",
    "             s * x + c * y + offset[1]) for (x, y) in rectCoords]\n",
    "\n",
    "\n",
    "# ---- Make depth images ---\n",
    "def make_dataset(dirname, num_images):\n",
    "    true_coords = []\n",
    "    newpath = \"./\" + dirname\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "        print(newpath)\n",
    "    for i in range(num_images):\n",
    "        # orient = 0 # degrees\n",
    "        img = Image.new(\"RGB\", (IMG_X, IMG_Y), \"black\")\n",
    "\n",
    "        # block_l and _w offset so blocks don't run off edge of image\n",
    "        rand_x = int(np.random.rand() * (IMG_X - 2 * block_l)) + block_l\n",
    "        rand_y = int(np.random.rand() * (IMG_Y - 2 * block_w)) + block_w\n",
    "        orient = int(np.random.rand() * 180)  # .random() is range [0.0, 1.0).\n",
    "        orient = math.radians(orient)  # math.cos takes radians!\n",
    "\n",
    "        true_coords.append(np.array((rand_x, rand_y, orient)))\n",
    "\n",
    "        rect_vertices = makeRectangle(\n",
    "            block_l, block_w, orient, offset=(rand_x, rand_y))\n",
    "\n",
    "        idraw = ImageDraw.Draw(img)\n",
    "        idraw.polygon(rect_vertices, fill=\"white\")\n",
    "\n",
    "        img.save(newpath + \"/rect\" + str(i) + \".png\")\n",
    "    return true_coords\n",
    "\n",
    "\n",
    "# NOTE Define size of dataset\n",
    "train_truth = make_dataset(\"data\", 1000)\n",
    "# print(len(train_truth))\n",
    "test_truth = make_dataset(\"./data/test\", 300)\n",
    "\n",
    "np.save(\"train_truth.npy\", train_truth)\n",
    "np.save(\"test_truth.npy\", test_truth)\n",
    "\n",
    "\n",
    "train_truth = np.load(\"train_truth.npy\")\n",
    "test_truth = np.load(\"test_truth.npy\")  # loading the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Dataloader -------------------------------------------------------\n",
    "class RectDepthImgsDataset(Dataset):\n",
    "    \"\"\"Artificially generated depth images dataset\"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, coords, transform=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.true_coords = coords\n",
    "        self.transform = transform\n",
    "\n",
    "        STEPSIZE = 50  # todo make this input arguments\n",
    "        WINDOWSIZE = (100, 100)\n",
    "        MARGIN_PX = 15\n",
    "        self.step = STEPSIZE\n",
    "        self.cropSize = WINDOWSIZE\n",
    "        self.detectMargin = MARGIN_PX\n",
    "        \n",
    "        # go ahead and read size of the images (assume all same size)\n",
    "        image = io.imread(self.img_dir + \"/rect\" +\n",
    "                          \"1\" + \".png\", as_gray=True)\n",
    "        self._imgx, self._imgy = image.shape[1], image.shape[0]\n",
    "        del image\n",
    "\n",
    "    def __len__(self):\n",
    "        # print('true coord len', len(self.true_coords))\n",
    "        return len(self.true_coords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # image = self.images[idx]\n",
    "        image = io.imread(self.img_dir + \"/rect\" +\n",
    "                          str(idx) + \".png\", as_gray=True)\n",
    "        # image = torch.FloatTensor(image).permute(2, 0, 1)  # PIL and torch expect difft orders\n",
    "        coords = torch.FloatTensor(self.true_coords[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        labels, coordsPerCrop = self.makeCrops(self.step, self.cropSize, coords, self.detectMargin)\n",
    "\n",
    "        sample = image, torch.FloatTensor(labels), torch.FloatTensor(coordsPerCrop)\n",
    "        return sample\n",
    "\n",
    "    def makeCrops(self, stepSize, windowSize, rectCenter, detectMargin):\n",
    "        \"\"\"\n",
    "        Returns image crops, as well as T/F for those crops\n",
    "        \"\"\"\n",
    "        c_x, c_y, theta = rectCenter\n",
    "        margin = detectMargin\n",
    "        hasObjs = []\n",
    "        objCoords = []\n",
    "        for y in range(0, self._imgy - windowSize[0] + 1, stepSize):\n",
    "            for x in range(0, self._imgx - windowSize[1] + 1, stepSize):\n",
    "                end_x, end_y = x + windowSize[1], y + windowSize[0]\n",
    "                hasRect = (x + margin < c_x < end_x - margin) and (\n",
    "                    y + margin < c_y < end_y - margin)\n",
    "                hasObjs.append(hasRect)\n",
    "                if hasRect:\n",
    "                    objCoords.append((c_x, c_y, theta))\n",
    "                else:\n",
    "                    objCoords.append((-0, -0, -0))\n",
    "        # print('length of truths in makeCrops', len(truths))\n",
    "        return hasObjs, objCoords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Define Loss -------------------------------------------------------\n",
    "\n",
    "import copy \n",
    "\n",
    "\n",
    "class MultiObjLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The MultiBox loss, a loss function for object detection.\n",
    "    This is a combination of:\n",
    "    (1) a localization loss for the predicted locations of the boxes, and\n",
    "    (2) a confidence loss for the predicted class scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MultiObjLoss, self).__init__()\n",
    "        #self.smooth_l1 = nn.L1Loss()\n",
    "        self.smooth_l1 = nn.SmoothL1Loss()\n",
    "        self.binary_cross_entropy = nn.BCELoss()\n",
    "        #self.MSE = nn.MSELoss()\n",
    "        self.alpha = 1  # some weighting between class vs regr loss\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def forward(self, predicted_locs, predicted_classes, locs, labels):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param predicted_locs: predicted locations/boxes w.r.t the 8732 prior boxes, a tensor of dimensions (N, 8732, 4)\n",
    "        :param predicted_scores: class scores for each of the encoded locations/boxes, a tensor of dimensions (N, 8732, n_classes)\n",
    "        :param boxes: true object bounding boxes in boundary coordinates, a list of N tensors\n",
    "        :param labels: true object labels, a list of N tensors\n",
    "        :return: multibox loss, a scalar\n",
    "        \"\"\"\n",
    "        #batch_size = predicted_locs.size(0)\n",
    "        #print('\\n<<--- before shaping, locs size', locs.size(), '--->>>')\n",
    "        #print('locs', locs)\n",
    "        #print('\\n<<--- Before shaping, Predicted locs size', predicted_locs.size(), '--->>>')\n",
    "        #print('beofre shaping, plocs', predicted_locs)\n",
    "        # TODO: Is this correct way to resize?? -------------------------->><<<<>>><<<\n",
    "        predicted_locs = predicted_locs.view(-1,\n",
    "                                             locs.size(1), locs.size(2))\n",
    "        #print('\\n<<--- After, Predicted locs size', predicted_locs.size(), '--->>>')\n",
    "        #print('after shaping, plocs', predicted_locs)\n",
    "\n",
    "        #masked = predicted_classes.numpy() * predicted_locs.numpy()\n",
    "        #mask = predicted_classes.detach().numpy()\n",
    "        num_classes = 3 #x,y, theta\n",
    "        mask = np.repeat(mask[:, :, np.newaxis], num_classes, axis=2)\n",
    "        #loc_loss = self.smooth_l1(predicted_locs, locs)\n",
    "        masked = mask * predicted_locs.detach().numpy()\n",
    "        #print(masked)\n",
    "        #print(mask)\n",
    "        masked = masked.torch()\n",
    "        locs = locs\n",
    "        labels = labels\n",
    "        \n",
    "        loc_loss = self.smooth_l1(masked, locs)\n",
    "        conf_loss = self.binary_cross_entropy(predicted_classes, labels)\n",
    "\n",
    "        return conf_loss + self.alpha * loc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f22dec6d3b3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcrit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiObjLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrueloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrueclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/mlenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-3905715ad9d3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, predicted_locs, predicted_classes, locs, labels)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m#mask = np.repeat(mask[:, :, np.newaxis], num_classes, axis=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#loc_loss = self.smooth_l1(predicted_locs, locs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mmasked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpredicted_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;31m#print(masked)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m#print(mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mask' is not defined"
     ]
    }
   ],
   "source": [
    "# two batchs, each batch of 2 images, with 2 sliding windows each\n",
    "predloc = torch.FloatTensor([[1,1,1], [2,2,2]])\n",
    "predloc = torch.cat((predloc, predloc))\n",
    "trueloc = torch.FloatTensor([[1,1,2], [2,2,1]])\n",
    "trueloc = torch.stack((trueloc, trueloc))\n",
    "\n",
    "predclass = torch.FloatTensor([[0, 1], [1, 1]])\n",
    "trueclass = torch.FloatTensor([[0, 0], [0, 1]])\n",
    "\n",
    "crit = MultiObjLoss()\n",
    "crit(predloc, predclass, trueloc, trueclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Define NN -------------------------------------------------------\n",
    "\n",
    "class myNet(nn.Module):  # CIFAR is 32x32x3, MNIST is 28x28x1)\n",
    "    def __init__(self, IMG_X, IMG_Y):\n",
    "        \"\"\" We need the image width and height to determine CNN layer sizes\n",
    "        \"\"\"\n",
    "        super(myNet, self).__init__()\n",
    "        self._imgx = IMG_X\n",
    "        self._imgy = IMG_Y\n",
    "        _pool = 2\n",
    "        _stride = 5\n",
    "        _outputlayers = 16\n",
    "\n",
    "        STEPSIZE = 50  # todo make this input arguments\n",
    "        WINDOWSIZE = (100, 100)\n",
    "        self.step = STEPSIZE\n",
    "        self.cropSize = WINDOWSIZE\n",
    "        self.numCrops = 0\n",
    "        # T/F for now\n",
    "\n",
    "        # calculate number of crops\n",
    "        for x in range(0, IMG_Y - WINDOWSIZE[0] + 1, STEPSIZE):\n",
    "            for y in range(0, IMG_X - WINDOWSIZE[1] + 1, STEPSIZE):\n",
    "                self.numCrops += 1\n",
    "        # TODO: but our net wants everything to be the same size... pad with 0s?\n",
    "\n",
    "        def _calc(val):  # use to calculate layer sizes\n",
    "            layer_size = (val - (_stride - 1)) / _pool\n",
    "            return layer_size\n",
    "\n",
    "        # print(self._imgx)\n",
    "        # self._const = _calc(_calc(self._imgx))\n",
    "        # self._const *= _calc(_calc(self._imgy))\n",
    "        self._const = _calc(_calc(self.cropSize[0]))\n",
    "        self._const *= _calc(_calc(self.cropSize[1]))\n",
    "        self._const *= _outputlayers\n",
    "        self._const = int(self._const)\n",
    "\n",
    "        # --- CLASSIFICATION OF WINDOWS\n",
    "        # batch, 3 input image channels (RGB), 6 output channels, 5x5 square convolution\n",
    "        # NOTE: we switched to 1 input channel\n",
    "        self.c_conv1 = nn.Conv2d(self.numCrops, 6, _stride).to(device)\n",
    "        self.c_pool = nn.MaxPool2d(_pool, _pool).to(device)\n",
    "        self.c_conv2 = nn.Conv2d(6, _outputlayers, _stride).to(device)\n",
    "        self.c_fc1 = nn.Linear(self._const, 120).to(device)\n",
    "        self.c_fc2 = nn.Linear(120, 84).to(device)\n",
    "        self.c_fc3 = nn.Linear(84, self.numCrops).to(device)\n",
    "        self.c_sigmoid = nn.Sigmoid()\n",
    "        # TODO: batch normalization  self.bn = nn.BatchNorm2d()\n",
    "\n",
    "        # --- LOCATION OF RECTANGLE\n",
    "        self.l_conv1 = nn.Conv2d(self.numCrops, 6, _stride).to(device)\n",
    "        self.l_pool = nn.MaxPool2d(_pool, _pool).to(device)\n",
    "        self.l_conv2 = nn.Conv2d(6, _outputlayers, _stride).to(device)\n",
    "        self.l_fc1 = nn.Linear(self._const, 120).to(device)\n",
    "        self.l_fc2 = nn.Linear(120, 84).to(device)\n",
    "        self.l_fc3 = nn.Linear(84, 3 * self.numCrops).to(device)\n",
    "        # TODO: what if there are multiple rectnagles?????\n",
    "        # I thought that was the whole point of the sliding windows\n",
    "        # How to deal with potential unknown number of objects\n",
    "        # for now... let's just assume there's at most one rectangle per sliding\n",
    "        # window... = 3 (x,y, theta) * self.numCrops\n",
    "        #self.l_sigmoid = nn.Sigmoid() # NO BUG ! \n",
    "\n",
    "    # Localization prediction convolutions (predict offsets w.r.t prior-boxes)\n",
    "    # self.loc_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * 4, kernel_size=3, padding=1)\n",
    "    # Class prediction convolutions (predict classes in localization boxes)\n",
    "    # self.cl_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * n_classes, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propogation\n",
    "        :param image: images, a tensor of dimensions (N, 3, IMG_X, IMG_Y)\n",
    "        :return: (x,y, theta) and T/F for each window \n",
    "        \"\"\"\n",
    "        # TODO: Wait, if we're going to return a regression for each window,\n",
    "        # do we even need classif? We can just say (-99,-99) to denote no rect\n",
    "        # NOTE: Let (No Rect) = (-1,-1,-1) for localization\n",
    "\n",
    "        x = x.to(device)\n",
    "        # TODO: presumably by doing this i lose some of the multithread goodness\n",
    "        batch_images = x\n",
    "\n",
    "        all_crops = []\n",
    "        for img in batch_images:\n",
    "            crops = self.makeCrops(img, self.step, self.cropSize)\n",
    "            all_crops.append(crops)\n",
    "            # all_crops.append(crops)\n",
    "        all_crops = torch.stack(all_crops)\n",
    "        # gray=1\n",
    "        # we want, batch_size, [100x100]*numcrops 4 dims\n",
    "        # TODO is this even right # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! <<<<-----------------------\n",
    "        feats = all_crops.view(-1, self.numCrops, self.cropSize[0],\n",
    "                               self.cropSize[1]).to(device)\n",
    "\n",
    "        # CLASSIFICATION of the windows\n",
    "        c_crops = self.c_pool(F.relu((self.c_conv1(feats))))\n",
    "        c_crops = self.c_pool(F.relu(self.c_conv2(c_crops)))\n",
    "        c_crops = c_crops.view(-1, self._const)\n",
    "        c_crops = F.relu(self.c_fc1(c_crops))\n",
    "        c_crops = F.relu(self.c_fc2(c_crops))\n",
    "        c_crops = self.c_fc3(c_crops)\n",
    "        c_crops = self.c_sigmoid(c_crops)\n",
    "\n",
    "        # LOCALIZATION\n",
    "        regr_crops = self.l_pool(F.relu((self.l_conv1(feats))))\n",
    "        regr_crops = self.l_pool(F.relu(self.l_conv2(regr_crops)))\n",
    "        regr_crops = regr_crops.view(-1, self._const)\n",
    "        regr_crops = F.relu(self.l_fc1(regr_crops))\n",
    "        regr_crops = F.relu(self.l_fc2(regr_crops))\n",
    "        regr_crops = self.l_fc3(regr_crops)\n",
    "        #regr_crops = self.l_sigmoid(regr_crops) # NO BUG!\n",
    "\n",
    "        containsRect = c_crops\n",
    "        rectCoords = regr_crops\n",
    "        return containsRect, rectCoords\n",
    "\n",
    "    def makeCrops(self, image, stepSize, windowSize):\n",
    "        \"\"\"\n",
    "        Returns a generator of cropped boxes(the top left x, y, the image data)\n",
    "        \"\"\"\n",
    "        image = image.type(torch.FloatTensor).to(device)\n",
    "        crops = []\n",
    "\n",
    "        # TODO: look into ordering, why it's y,x !\n",
    "        for y in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "            for x in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "                end_x, end_y = x + windowSize[1], y + windowSize[0]\n",
    "                # print('This is the x and y used: ', x, '; ', y)\n",
    "                crops.append(image[y:end_y, x:end_x])\n",
    "        crops = torch.stack(crops)\n",
    "        # self.numCrops=len(crops)\n",
    "        return crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Utility fxn -------------------------------------------------------\n",
    "# Source: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/train.py\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Keeps track of most recent, average, sum, and count of a metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "    epoch, epochs_since_improvement, model, optimizer, loss, best_loss, is_best\n",
    "):\n",
    "    \"\"\"\n",
    "    Save model checkpoint.\n",
    "    :param epoch: epoch number\n",
    "    :param epochs_since_improvement: number of epochs since last improvement\n",
    "    :param model: model\n",
    "    :param optimizer: optimizer\n",
    "    :param loss: validation loss in this epoch\n",
    "    :param best_loss: best validation loss achieved so far (not necessarily in this checkpoint)\n",
    "    :param is_best: is this checkpoint the best so far?\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"epochs_since_improvement\": epochs_since_improvement,\n",
    "        \"loss\": loss,\n",
    "        \"best_loss\": best_loss,\n",
    "        \"model\": model,\n",
    "        \"optimizer\": optimizer,\n",
    "    }\n",
    "    filename = \"checkpoint_v2sliding.pth.tar\"\n",
    "    torch.save(state, filename)\n",
    "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
    "    if is_best:\n",
    "        torch.save(state, \"BEST_\" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -- Define Train and Valid fxn -------------------------------------------------------\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    One epoch's training.\n",
    "    :param train_loader: DataLoader for training data\n",
    "    :param model: model\n",
    "    :param criterion: MultiBox loss\n",
    "    :param optimizer: optimizer\n",
    "    :param epoch: epoch number\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()  # training mode enables dropout\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
    "    data_time = AverageMeter()  # data loading time\n",
    "    losses = AverageMeter()  # loss\n",
    "    start = time.time()\n",
    "\n",
    "    for i_batch, (images, labels, coords) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        coords = coords.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predicted_class, predicted_locs = model(images)\n",
    "        loss = criterion(predicted_locs, predicted_class, coords, labels)\n",
    "\n",
    "        # Backward prop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        batch_time.update(time.time() - start)\n",
    "        start = time.time()\n",
    "\n",
    "        # Print status\n",
    "        if i_batch % print_freq == 0:\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}]\\t\"\n",
    "                \"Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(\n",
    "                    epoch,\n",
    "                    i_batch,\n",
    "                    len(train_loader),\n",
    "                    batch_time=batch_time,\n",
    "                    loss=losses,\n",
    "                )\n",
    "            )\n",
    "        # free some memory since their histories may be stored\n",
    "        del predicted_locs, predicted_class, images, labels, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    \"\"\"\n",
    "    One epoch's validation.\n",
    "    :param val_loader: DataLoader for validation data\n",
    "    :param model: model\n",
    "    :param criterion: MultiBox loss\n",
    "    :return: average validation loss\n",
    "    \"\"\"\n",
    "    model.eval()  # eval mode disables dropout\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Prohibit gradient computation explicity because I had some problems with memory\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for i_batch, (images, labels, coords) in enumerate(train_loader):\n",
    "\n",
    "            # Move to default device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            coords = coords.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            predicted_class, predicted_locs = model(images)\n",
    "\n",
    "            # Loss\n",
    "            loss = criterion(predicted_locs, predicted_class, coords, labels)\n",
    "\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            batch_time.update(time.time() - start)\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            # Print status\n",
    "            if i_batch % print_freq == 0:\n",
    "                print(\n",
    "                    \"[{0}/{1}]\\t\"\n",
    "                    \"Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                    \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(\n",
    "                        i_batch, len(val_loader), batch_time=batch_time, loss=losses\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    print(\"\\n * LOSS - {loss.avg:.3f}\\n\".format(loss=losses))\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Load data -------------------------------------------------------\n",
    "batch_size = 32 \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA available? device: \", device)\n",
    "\n",
    "\n",
    "# Dataset is depth images of rectangular blocks\n",
    "train_dataset = RectDepthImgsDataset(img_dir=\"./data\", coords=train_truth)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = RectDepthImgsDataset(img_dir=\"./data/test\", coords=test_truth)\n",
    "\n",
    "# Data loader\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Hyperparamaters -------------------------\n",
    "\n",
    "num_epochs = 30 # number of epochs to run without early-stopping\n",
    "learning_rate = 0.001\n",
    "\n",
    "start_epoch = 0  # start at this epoch\n",
    "# number of epochs since there was an improvement in the validation metric\n",
    "epochs_since_improvement = 0\n",
    "best_loss = 100.0  # assume a high loss at first\n",
    "workers = 4  # number of workers for loading data in the DataLoader\n",
    "\n",
    "model = myNet(IMG_X, IMG_Y)\n",
    "model = model.to(device)\n",
    "\n",
    "#criterion = nn.BCELoss()\n",
    "criterion = MultiObjLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,\n",
    "#                             momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "print_freq = 25  # print training or validation status every __ batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Training and validation.\n",
    "    \"\"\"\n",
    "\n",
    "    global epochs_since_improvement, start_epoch, best_loss, epoch, checkpoint\n",
    "\n",
    "    print(\"Training model now...\")\n",
    "\n",
    "    # -- Begin training -------------------------\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train(\n",
    "            train_loader=train_loader,\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "\n",
    "        # One epoch's validation\n",
    "        val_loss = validate(val_loader=test_loader,\n",
    "                            model=model, criterion=criterion)\n",
    "\n",
    "        # Did validation loss improve?\n",
    "        is_best = val_loss < best_loss\n",
    "        best_loss = min(val_loss, best_loss)\n",
    "\n",
    "        if not is_best:\n",
    "            epochs_since_improvement += 1\n",
    "            print(\"\\nEpochs since last improvement: %d\\n\" %\n",
    "                  (epochs_since_improvement,))\n",
    "\n",
    "        else:\n",
    "            epochs_since_improvement = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(epoch, epochs_since_improvement, model, optimizer,\n",
    "                        val_loss, best_loss, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main()\n",
    "\n",
    "# alert when training is done\n",
    "sound_file = '/home/rui/Downloads/newyear.ogg'\n",
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All Ready!')\n",
    "\n",
    "\n",
    "filename = \"checkpoint_v2sliding.pth.tar\"\n",
    "checkpoint = torch.load(filename)\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "best_loss = checkpoint['best_loss']\n",
    "print('\\nLoaded checkpoint from epoch %d. Best loss so far is %.3f.\\n' % (start_epoch, best_loss))\n",
    "model = checkpoint['model']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_loader)\n",
    "images, labels, coords = dataiter.next()\n",
    "\n",
    "n = 1\n",
    "print(\"\\n\\nFOR N = \", n)\n",
    "imgSample = images[n]\n",
    "max_idx = torch.argmax(coords[n][:,0])\n",
    "x, y, theta = coords[n][max_idx]\n",
    "print(labels[n])\n",
    "print([float(zed) for zed in (x,y,theta)])\n",
    "#print(x,y,theta)\n",
    "#crops, __ = makeCrops(imgSample, 50, (100, 100), (x,y, theta))\n",
    "\n",
    "plt.imshow(imgSample)\n",
    "#fig, axess = plt.subplots(3,3, figsize=(10,10))\n",
    "#axess = np.array(axess).flatten()#order='F') #'F' means to flatten in column-major (Fortran- style) order.\n",
    "#for i in range(9):\n",
    "#    axess[i].imshow(crops[i])\n",
    "#    axess[i].set_ylabel('# '+str(i))\n",
    "plt.suptitle('Window')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Check the results -------------------------------------------------------\n",
    "\n",
    "# -- Utility ---------------------------------------------\n",
    "def makeCrops(image, stepSize, windowSize, true_center):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    image = image.type(torch.FloatTensor)\n",
    "    crops = []\n",
    "    truths = []\n",
    "    c_x, c_y, orient = true_center\n",
    "    # TODO: look into otdering, why it's y,x !\n",
    "    margin = 15\n",
    "    # --> is x, but is the column\n",
    "    # to slide horizontally, y must come first\n",
    "    for y in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "        for x in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "            end_x, end_y = x + windowSize[1], y + windowSize[0]\n",
    "            hasRect = (x + margin < c_x < end_x - margin) and (\n",
    "                y + margin < c_y < end_y - margin\n",
    "            )\n",
    "            truths.append(hasRect)\n",
    "            crops.append(image[y:end_y, x:end_x])\n",
    "    crops = torch.stack(crops)\n",
    "    print(\"shape of crops\", crops.shape)\n",
    "    return crops, truths\n",
    "\n",
    "\n",
    "# -- Get some validation results ---------------------------------------------\n",
    "model.to(device).eval()\n",
    "with torch.no_grad():\n",
    "    dataiter = iter(test_loader)\n",
    "    images, labels, coords = dataiter.next()\n",
    "    \n",
    "    # Move to default device\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    coords = coords.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    predicted_class, predicted_locs = model(images)\n",
    "\n",
    "    # Loss\n",
    "    loss = criterion(predicted_locs, predicted_class, coords, labels)\n",
    "\n",
    "print(\"loss across batch size of \", labels.size()[0], 'is: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Plot windows and print labels -----------------------------------\n",
    "n = 5\n",
    "imgSample = images[n]\n",
    "crops, __ = makeCrops(imgSample, 50, (100, 100), (x,y, theta))\n",
    "# -- Show original image, and the sliding windo crops -------\n",
    "plt.imshow(imgSample)\n",
    "# ---------------------------------------------------\n",
    "foo_label = labels[n]\n",
    "\n",
    "foo_coord = coords[n] \n",
    "predicted_locs = predicted_locs.view(-1,\n",
    "                                     coords.size(1), coords.size(2))\n",
    "foo_coord_est = predicted_locs[n] # 3 per window\n",
    "foo_label_est = predicted_class[n]\n",
    "\n",
    "max_idx = torch.argmax(foo_coord[:,0])\n",
    "x, y, theta = foo_coord[max_idx]\n",
    "est_max_idx = torch.argmax(foo_coord_est[:,0])\n",
    "x_est, y_est, theta_est = foo_coord_est[est_max_idx]\n",
    "\n",
    "# -- Print x,y for one result -------\n",
    "# -- Print window t/f for one result -------\n",
    "print(\"!-- FOR N = \", n)\n",
    "print(\"y (crops) \\n\\t\", [int(l) for l in foo_label])\n",
    "print(\"!-- yhat (crops) \\n\\t\", [int(np.round(p,0)) for p in foo_label_est] )\n",
    "# -------------------------------------------------\n",
    "sns.set(rc={\"figure.figsize\": (8, 6)})\n",
    "\n",
    "print(\"\\n\\nFOR N = \", n)\n",
    "print(\"!-- center y \\n\\t\", [float(zed) for zed in (x, y, theta)])\n",
    "print(\"!-- center y est \\n\\t \", [float(zed) for zed in (x_est, y_est, theta_est)])\n",
    "print(foo_coord_est)\n",
    "\n",
    "fig, axess = plt.subplots(3,3, figsize=(10,10))\n",
    "axess = np.array(axess).flatten()#order='F') #'F' means to flatten in column-major (Fortran- style) order.\n",
    "for i in range(9):\n",
    "    axess[i].imshow(crops[i])\n",
    "    axess[i].set_ylabel('# '+str(i))\n",
    "plt.suptitle('Window')\n",
    "plt.show()\n",
    "\n",
    "#for (i, crop) in enumerate(crops):\n",
    "    # print(\"1-index number of window: \", i+1, 'x', x, 'y', y, 'has rectangle?', hasRect)\n",
    "    #plt.figure()\n",
    "    #plt.suptitle(\"numero: %d\" % (i))\n",
    "    #plt.imshow(crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(\"!-- yhat \\n\\t\", [int(round(o, 0)) for o in outputs[n].cpu().numpy()])    \n",
    "print(\"!-- \")\n",
    "print('True locations, defined for each crop\\n', foo_coord)\n",
    "print(\"!-- \")\n",
    "print('Full predicted locations (3 per crop)\\n', foo_coord_est)\n",
    "#print(\"\\n ------ x,y center + orient: \", coords[n], \"\\n\"))\n",
    "print(np.argwhere(foo_coord_est > 0) )\n",
    "\n",
    "# -- Print outputs for multiple results -------\n",
    "for ix in range(5):\n",
    "    print('\\nSAMPLE ', ix)\n",
    "    print(\"!-- y (crops) \\n\\t\", [int(l) for l in labels[ix]])\n",
    "    print(\"!-- yhat (crops) \\n\\t\", [int(np.round(p, 0)) for p in predicted_class[ix]] )\n",
    "\n",
    "# -- Main ---------------------------------------------\n",
    "#if __name__ == '__main__':\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
