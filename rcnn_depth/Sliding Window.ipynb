{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sliding Window\n",
    "# Feb 2019\n",
    "#\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import torchvision\n",
    "from IPython.display import display  # to display images\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import io\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataset -------------------------------------------------------\n",
    "IMG_X, IMG_Y = 200, 200\n",
    "# length and width of blocks (fixed for now)\n",
    "block_l, block_w = 20, 30\n",
    "\n",
    "\n",
    "def makeRectangle(l, w, theta, offset=(0, 0)):\n",
    "    c, s = math.cos(theta), math.sin(theta)\n",
    "    rectCoords = [(l / 2.0, w / 2.0), (l / 2.0, -w / 2.0),\n",
    "                  (-l / 2.0, -w / 2.0), (-l / 2.0, w / 2.0), ]\n",
    "    return [(c * x - s * y + offset[0],\n",
    "             s * x + c * y + offset[1]) for (x, y) in rectCoords]\n",
    "\n",
    "\n",
    "# ---- Make depth images ---\n",
    "def make_dataset(dirname, num_images):\n",
    "    true_coords = []\n",
    "    newpath = \"./\" + dirname\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "        print(newpath)\n",
    "    for i in range(num_images):\n",
    "        # orient = 0 # degrees\n",
    "        img = Image.new(\"RGB\", (IMG_X, IMG_Y), \"black\")\n",
    "\n",
    "        # block_l and _w offset so blocks don't run off edge of image\n",
    "        rand_x = int(np.random.rand() * (IMG_X - 2 * block_l)) + block_l\n",
    "        rand_y = int(np.random.rand() * (IMG_Y - 2 * block_w)) + block_w\n",
    "        orient = int(np.random.rand() * 180)  # .random() is range [0.0, 1.0).\n",
    "        orient = math.radians(orient)  # math.cos takes radians!\n",
    "\n",
    "        true_coords.append(np.array((rand_x, rand_y, orient)))\n",
    "\n",
    "        rect_vertices = makeRectangle(\n",
    "            block_l, block_w, orient, offset=(rand_x, rand_y))\n",
    "\n",
    "        idraw = ImageDraw.Draw(img)\n",
    "        idraw.polygon(rect_vertices, fill=\"white\")\n",
    "\n",
    "        img.save(newpath + \"/rect\" + str(i) + \".png\")\n",
    "    return true_coords\n",
    "\n",
    "\n",
    "# NOTE Define size of dataset\n",
    "# train_truth = make_dataset(\"data\", 1500)\n",
    "# print(len(train_truth))\n",
    "# test_truth = make_dataset(\"./data/test\", 300)\n",
    "\n",
    "#np.save(\"train_truth.npy\", train_truth)\n",
    "#np.save(\"test_truth.npy\", test_truth)\n",
    "\n",
    "\n",
    "train_truth = np.load(\"train_truth.npy\")\n",
    "test_truth = np.load(\"test_truth.npy\")  # loading the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Dataloader -------------------------------------------------------\n",
    "class RectDepthImgsDataset(Dataset):\n",
    "    \"\"\"Artificially generated depth images dataset\"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, coords, transform=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.true_coords = coords\n",
    "        self.transform = transform\n",
    "\n",
    "        STEPSIZE = 50  # todo make this input arguments\n",
    "        WINDOWSIZE = (100, 100)\n",
    "        MARGIN_PX = 15\n",
    "        self.step = STEPSIZE\n",
    "        self.cropSize = WINDOWSIZE\n",
    "        self.detectMargin = MARGIN_PX\n",
    "\n",
    "    def __len__(self):\n",
    "        # print('true coord len', len(self.true_coords))\n",
    "        return len(self.true_coords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # image = self.images[idx]\n",
    "        image = io.imread(self.img_dir + \"/rect\" +\n",
    "                          str(idx) + \".png\", as_grey=True)\n",
    "        # image = torch.FloatTensor(image).permute(2, 0, 1)  # PIL and torch expect difft orders\n",
    "        coords = torch.FloatTensor(self.true_coords[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        crops, labels, cropCoords = self.makeCrops(\n",
    "            image, self.step, self.cropSize, coords, self.detectMargin)\n",
    "\n",
    "        sample = image, torch.FloatTensor(\n",
    "            labels), torch.FloatTensor(cropCoords)\n",
    "        return sample\n",
    "\n",
    "    def makeCrops(self, image, stepSize, windowSize, rectCenter, detectMargin):\n",
    "        \"\"\"\n",
    "        Returns image crops, as well as T/F for those crops\n",
    "        \"\"\"\n",
    "        crops = []\n",
    "        c_x, c_y, theta = rectCenter\n",
    "        margin = detectMargin\n",
    "        hasRects = []\n",
    "        rectCoords = []\n",
    "        for y in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "            for x in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "                end_x, end_y = x + windowSize[1], y + windowSize[0]\n",
    "                hasRect = (x + margin < c_x < end_x - margin) and (\n",
    "                    y + margin < c_y < end_y - margin)\n",
    "                crops.append(image[y:end_y, x:end_x])\n",
    "                hasRects.append(hasRect)\n",
    "                if hasRect:\n",
    "                    rectCoords.append((c_x, c_y, theta))\n",
    "                else:\n",
    "                    rectCoords.append((-1, -1, -1))\n",
    "        # print('length of truths in makeCrops', len(truths))\n",
    "        return crops, hasRects, rectCoords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Define Loss -------------------------------------------------------\n",
    "\n",
    "class MultiObjLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The MultiBox loss, a loss function for object detection.\n",
    "    This is a combination of:\n",
    "    (1) a localization loss for the predicted locations of the boxes, and\n",
    "    (2) a confidence loss for the predicted class scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MultiObjLoss, self).__init__()\n",
    "        #self.smooth_l1 = nn.L1Loss()\n",
    "        self.smooth_l1 = nn.SmoothL1Loss()\n",
    "        self.cross_entropy = nn.BCELoss()\n",
    "        self.alpha = 1  # some weighting between class vs regr loss\n",
    "\n",
    "    def forward(self, predicted_locs, predicted_classes, coords, labels):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param predicted_locs: predicted locations/boxes w.r.t the 8732 prior boxes, a tensor of dimensions (N, 8732, 4)\n",
    "        :param predicted_scores: class scores for each of the encoded locations/boxes, a tensor of dimensions (N, 8732, n_classes)\n",
    "        :param boxes: true object bounding boxes in boundary coordinates, a list of N tensors\n",
    "        :param labels: true object labels, a list of N tensors\n",
    "        :return: multibox loss, a scalar\n",
    "        \"\"\"\n",
    "        #batch_size = predicted_locs.size(0)\n",
    "        #print('\\n<<--- locs size', coords.size(), '--->>>')\n",
    "        predicted_locs = predicted_locs.view(-1,\n",
    "                                             coords.size(1), coords.size(2))\n",
    "        #print('\\n<<--- Predicted locs size', predicted_locs.size(), '--->>>')\n",
    "\n",
    "        loc_loss = self.smooth_l1(predicted_locs, coords)\n",
    "        conf_loss = self.cross_entropy(predicted_classes, labels)\n",
    "\n",
    "        return conf_loss + self.alpha * loc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Define NN -------------------------------------------------------\n",
    "\n",
    "\n",
    "class myNet(nn.Module):  # CIFAR is 32x32x3, MNIST is 28x28x1)\n",
    "    def __init__(self, IMG_X, IMG_Y):\n",
    "        \"\"\" We need the image width and height to determine CNN layer sizes\n",
    "        \"\"\"\n",
    "        super(myNet, self).__init__()\n",
    "        self._imgx = IMG_X\n",
    "        self._imgy = IMG_Y\n",
    "        _pool = 2\n",
    "        _stride = 5\n",
    "        _outputlayers = 16\n",
    "\n",
    "        STEPSIZE = 50  # todo make this input arguments\n",
    "        WINDOWSIZE = (100, 100)\n",
    "        self.step = STEPSIZE\n",
    "        self.cropSize = WINDOWSIZE\n",
    "        self.numCrops = 0\n",
    "        # T/F for now\n",
    "\n",
    "        # calculate number of crops\n",
    "        for x in range(0, IMG_Y - WINDOWSIZE[0] + 1, STEPSIZE):\n",
    "            for y in range(0, IMG_X - WINDOWSIZE[1] + 1, STEPSIZE):\n",
    "                self.numCrops += 1\n",
    "        # TODO: but our net wants everything to be the same size... pad with 0s?\n",
    "\n",
    "        def _calc(val):  # use to calculate layer sizes\n",
    "            layer_size = (val - (_stride - 1)) / _pool\n",
    "            return layer_size\n",
    "\n",
    "        # print(self._imgx)\n",
    "        # self._const = _calc(_calc(self._imgx))\n",
    "        # self._const *= _calc(_calc(self._imgy))\n",
    "        self._const = _calc(_calc(self.cropSize[0]))\n",
    "        self._const *= _calc(_calc(self.cropSize[1]))\n",
    "        self._const *= _outputlayers\n",
    "        self._const = int(self._const)\n",
    "\n",
    "        # --- CLASSIFICATION OF WINDOWS\n",
    "        # batch, 3 input image channels (RGB), 6 output channels, 5x5 square convolution\n",
    "        # NOTE: we switched to 1 input channel\n",
    "        self.c_conv1 = nn.Conv2d(self.numCrops, 6, _stride).to(device)\n",
    "        self.c_pool = nn.MaxPool2d(_pool, _pool).to(device)\n",
    "        self.c_conv2 = nn.Conv2d(6, _outputlayers, _stride).to(device)\n",
    "        self.c_fc1 = nn.Linear(self._const, 120).to(device)\n",
    "        self.c_fc2 = nn.Linear(120, 84).to(device)\n",
    "        self.c_fc3 = nn.Linear(84, self.numCrops).to(device)\n",
    "        self.c_sigmoid = nn.Sigmoid()\n",
    "        # TODO: batch normalization  self.bn = nn.BatchNorm2d()\n",
    "\n",
    "        # --- LOCATION OF RECTANGLE\n",
    "        self.l_conv1 = nn.Conv2d(self.numCrops, 6, _stride).to(device)\n",
    "        self.l_pool = nn.MaxPool2d(_pool, _pool).to(device)\n",
    "        self.l_conv2 = nn.Conv2d(6, _outputlayers, _stride).to(device)\n",
    "        self.l_fc1 = nn.Linear(self._const, 120).to(device)\n",
    "        self.l_fc2 = nn.Linear(120, 84).to(device)\n",
    "        self.l_fc3 = nn.Linear(84, 3 * self.numCrops).to(device)\n",
    "        # TODO: what if there are multiple rectnagles?????\n",
    "        # I thought that was the whole point of the sliding windows\n",
    "        # How to deal with potential unknown number of objects\n",
    "        # for now... let's just assume there's at most one rectangle per sliding\n",
    "        # window... = 3 (x,y, theta) * self.numCrops\n",
    "        self.l_sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # Localization prediction convolutions (predict offsets w.r.t prior-boxes)\n",
    "    # self.loc_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * 4, kernel_size=3, padding=1)\n",
    "    # Class prediction convolutions (predict classes in localization boxes)\n",
    "    # self.cl_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * n_classes, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propogation\n",
    "        :param image: images, a tensor of dimensions (N, 3, IMG_X, IMG_Y)\n",
    "        :return: (x,y, theta) and T/F for each window \n",
    "        \"\"\"\n",
    "        # TODO: Wait, if we're going to return a regression for each window,\n",
    "        # do we even need classif? We can just say (-99,-99) to denote no rect\n",
    "        # NOTE: Let (No Rect) = (-1,-1,-1) for localization\n",
    "\n",
    "        x = x.to(device)\n",
    "        # TODO: presumably by doing this i lose some of the multithread goodness\n",
    "        batch_images = x\n",
    "\n",
    "        all_crops = []\n",
    "        for img in batch_images:\n",
    "            crops = self.makeCrops(img, self.step, self.cropSize)\n",
    "            all_crops.append(crops)\n",
    "            # all_crops.append(crops)\n",
    "        all_crops = torch.stack(all_crops)\n",
    "        # gray=1\n",
    "        # we want, batch_size, [100x100]*numcrops 4 dims\n",
    "        # TODO is this even right # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! <<<<-----------------------\n",
    "        feats = all_crops.view(-1, self.numCrops, self.cropSize[0],\n",
    "                               self.cropSize[1]).to(device)\n",
    "\n",
    "        # CLASSIFICATION of the windows\n",
    "        c_crops = self.c_pool(F.relu((self.c_conv1(feats))))\n",
    "        c_crops = self.c_pool(F.relu(self.c_conv2(c_crops)))\n",
    "        c_crops = c_crops.view(-1, self._const)\n",
    "        c_crops = F.relu(self.c_fc1(c_crops))\n",
    "        c_crops = F.relu(self.c_fc2(c_crops))\n",
    "        c_crops = self.c_fc3(c_crops)\n",
    "        c_crops = self.c_sigmoid(c_crops)\n",
    "\n",
    "        # LOCALIZATION\n",
    "        regr_crops = self.l_pool(F.relu((self.l_conv1(feats))))\n",
    "        regr_crops = self.l_pool(F.relu(self.l_conv2(regr_crops)))\n",
    "        regr_crops = regr_crops.view(-1, self._const)\n",
    "        regr_crops = F.relu(self.l_fc1(regr_crops))\n",
    "        regr_crops = F.relu(self.l_fc2(regr_crops))\n",
    "        regr_crops = self.l_fc3(regr_crops)\n",
    "        regr_crops = self.l_sigmoid(regr_crops)\n",
    "\n",
    "        containsRect = c_crops\n",
    "        rectCoords = regr_crops\n",
    "        return containsRect, rectCoords\n",
    "\n",
    "    def makeCrops(self, image, stepSize, windowSize):\n",
    "        \"\"\"\n",
    "        Returns a generator of cropped boxes(the top left x, y, the image data)\n",
    "        \"\"\"\n",
    "        image = image.type(torch.FloatTensor).to(device)\n",
    "        crops = []\n",
    "\n",
    "        # TODO: look into ordering, why it's y,x !\n",
    "        for y in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "            for x in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "                end_x, end_y = x + windowSize[1], y + windowSize[0]\n",
    "                # print('This is the x and y used: ', x, '; ', y)\n",
    "                crops.append(image[y:end_y, x:end_x])\n",
    "        crops = torch.stack(crops)\n",
    "        # self.numCrops=len(crops)\n",
    "        return crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Utility fxn -------------------------------------------------------\n",
    "# Source: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/train.py\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Keeps track of most recent, average, sum, and count of a metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "    epoch, epochs_since_improvement, model, optimizer, loss, best_loss, is_best\n",
    "):\n",
    "    \"\"\"\n",
    "    Save model checkpoint.\n",
    "    :param epoch: epoch number\n",
    "    :param epochs_since_improvement: number of epochs since last improvement\n",
    "    :param model: model\n",
    "    :param optimizer: optimizer\n",
    "    :param loss: validation loss in this epoch\n",
    "    :param best_loss: best validation loss achieved so far (not necessarily in this checkpoint)\n",
    "    :param is_best: is this checkpoint the best so far?\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"epochs_since_improvement\": epochs_since_improvement,\n",
    "        \"loss\": loss,\n",
    "        \"best_loss\": best_loss,\n",
    "        \"model\": model,\n",
    "        \"optimizer\": optimizer,\n",
    "    }\n",
    "    filename = \"checkpoint_v2sliding.pth.tar\"\n",
    "    torch.save(state, filename)\n",
    "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
    "    if is_best:\n",
    "        torch.save(state, \"BEST_\" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -- Define Train and Valid fxn -------------------------------------------------------\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    One epoch's training.\n",
    "    :param train_loader: DataLoader for training data\n",
    "    :param model: model\n",
    "    :param criterion: MultiBox loss\n",
    "    :param optimizer: optimizer\n",
    "    :param epoch: epoch number\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()  # training mode enables dropout\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
    "    data_time = AverageMeter()  # data loading time\n",
    "    losses = AverageMeter()  # loss\n",
    "    start = time.time()\n",
    "\n",
    "    for i_batch, (images, labels, coords) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        coords = coords.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predicted_class, predicted_locs = model(images)\n",
    "        loss = criterion(predicted_locs, predicted_class, coords, labels)\n",
    "\n",
    "        # Backward prop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        batch_time.update(time.time() - start)\n",
    "        start = time.time()\n",
    "\n",
    "        # Print status\n",
    "        if i_batch % print_freq == 0:\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}]\\t\"\n",
    "                \"Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(\n",
    "                    epoch,\n",
    "                    i_batch,\n",
    "                    len(train_loader),\n",
    "                    batch_time=batch_time,\n",
    "                    loss=losses,\n",
    "                )\n",
    "            )\n",
    "        # free some memory since their histories may be stored\n",
    "        del predicted_locs, predicted_class, images, labels, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    \"\"\"\n",
    "    One epoch's validation.\n",
    "    :param val_loader: DataLoader for validation data\n",
    "    :param model: model\n",
    "    :param criterion: MultiBox loss\n",
    "    :return: average validation loss\n",
    "    \"\"\"\n",
    "    model.eval()  # eval mode disables dropout\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Prohibit gradient computation explicity because I had some problems with memory\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for i_batch, (images, labels, coords) in enumerate(train_loader):\n",
    "\n",
    "            # Move to default device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            coords = coords.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            predicted_class, predicted_locs = model(images)\n",
    "\n",
    "            # Loss\n",
    "            loss = criterion(predicted_locs, predicted_class, coords, labels)\n",
    "\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            batch_time.update(time.time() - start)\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            # Print status\n",
    "            if i_batch % print_freq == 0:\n",
    "                print(\n",
    "                    \"[{0}/{1}]\\t\"\n",
    "                    \"Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                    \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(\n",
    "                        i_batch, len(val_loader), batch_time=batch_time, loss=losses\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    print(\"\\n * LOSS - {loss.avg:.3f}\\n\".format(loss=losses))\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available? device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# -- Load data -------------------------------------------------------\n",
    "batch_size = 15\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA available? device: \", device)\n",
    "\n",
    "\n",
    "# Dataset is depth images of rectangular blocks\n",
    "train_dataset = RectDepthImgsDataset(img_dir=\"./data\", coords=train_truth)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = RectDepthImgsDataset(img_dir=\"./data/test\", coords=test_truth)\n",
    "\n",
    "# Data loader\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Hyperparamaters -------------------------\n",
    "\n",
    "num_epochs = 30  # number of epochs to run without early-stopping\n",
    "learning_rate = 0.001\n",
    "\n",
    "start_epoch = 0  # start at this epoch\n",
    "# number of epochs since there was an improvement in the validation metric\n",
    "epochs_since_improvement = 0\n",
    "best_loss = 1.0  # assume a high loss at first\n",
    "workers = 4  # number of workers for loading data in the DataLoader\n",
    "\n",
    "model = myNet(IMG_X, IMG_Y)\n",
    "model = model.to(device)\n",
    "\n",
    "#criterion = nn.BCELoss()\n",
    "criterion = MultiObjLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,\n",
    "#                             momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "print_freq = 25  # print training or validation status every __ batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Training and validation.\n",
    "    \"\"\"\n",
    "\n",
    "    global epochs_since_improvement, start_epoch, best_loss, epoch, checkpoint\n",
    "\n",
    "    print(\"Training model now...\")\n",
    "\n",
    "    # -- Begin training -------------------------\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train(\n",
    "            train_loader=train_loader,\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "\n",
    "        # One epoch's validation\n",
    "        val_loss = validate(val_loader=test_loader,\n",
    "                            model=model, criterion=criterion)\n",
    "\n",
    "        # Did validation loss improve?\n",
    "        is_best = val_loss < best_loss\n",
    "        best_loss = min(val_loss, best_loss)\n",
    "\n",
    "        if not is_best:\n",
    "            epochs_since_improvement += 1\n",
    "            print(\"\\nEpochs since last improvement: %d\\n\" %\n",
    "                  (epochs_since_improvement,))\n",
    "\n",
    "        else:\n",
    "            epochs_since_improvement = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(epoch, epochs_since_improvement, model, optimizer,\n",
    "                        val_loss, best_loss, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model now...\n",
      "Epoch: [0][0/100]\tBatch Time 0.035 (0.035)\tLoss 12.3854 (12.3854)\t\n",
      "Epoch: [0][25/100]\tBatch Time 0.026 (0.026)\tLoss 13.1305 (12.7470)\t\n",
      "Epoch: [0][50/100]\tBatch Time 0.025 (0.026)\tLoss 12.0056 (12.6487)\t\n",
      "Epoch: [0][75/100]\tBatch Time 0.026 (0.026)\tLoss 10.4740 (12.5036)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 11.6165 (11.6165)\t\n",
      "[25/20]\tBatch Time 0.021 (0.022)\tLoss 9.6554 (12.7498)\t\n",
      "[50/20]\tBatch Time 0.022 (0.022)\tLoss 11.2586 (12.5421)\t\n",
      "[75/20]\tBatch Time 0.021 (0.022)\tLoss 12.6947 (12.4226)\t\n",
      "\n",
      " * LOSS - 12.333\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [1][0/100]\tBatch Time 0.026 (0.026)\tLoss 9.9534 (9.9534)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rui/mlenv/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type myNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][25/100]\tBatch Time 0.026 (0.025)\tLoss 10.6182 (12.2178)\t\n",
      "Epoch: [1][50/100]\tBatch Time 0.026 (0.025)\tLoss 14.3911 (12.1469)\t\n",
      "Epoch: [1][75/100]\tBatch Time 0.025 (0.025)\tLoss 10.2480 (12.3251)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 11.2898 (11.2898)\t\n",
      "[25/20]\tBatch Time 0.021 (0.022)\tLoss 16.4157 (12.1690)\t\n",
      "[50/20]\tBatch Time 0.021 (0.021)\tLoss 12.4762 (12.3738)\t\n",
      "[75/20]\tBatch Time 0.021 (0.021)\tLoss 13.8769 (12.3885)\t\n",
      "\n",
      " * LOSS - 12.304\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [2][0/100]\tBatch Time 0.026 (0.026)\tLoss 12.4951 (12.4951)\t\n",
      "Epoch: [2][25/100]\tBatch Time 0.025 (0.026)\tLoss 11.7237 (12.0932)\t\n",
      "Epoch: [2][50/100]\tBatch Time 0.026 (0.026)\tLoss 14.2731 (12.1042)\t\n",
      "Epoch: [2][75/100]\tBatch Time 0.026 (0.026)\tLoss 12.3322 (12.1974)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 15.4728 (15.4728)\t\n",
      "[25/20]\tBatch Time 0.021 (0.021)\tLoss 12.2583 (12.2908)\t\n",
      "[50/20]\tBatch Time 0.021 (0.022)\tLoss 14.2956 (12.2494)\t\n",
      "[75/20]\tBatch Time 0.021 (0.022)\tLoss 12.0510 (12.2652)\t\n",
      "\n",
      " * LOSS - 12.298\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [3][0/100]\tBatch Time 0.026 (0.026)\tLoss 12.4309 (12.4309)\t\n",
      "Epoch: [3][25/100]\tBatch Time 0.026 (0.026)\tLoss 13.6503 (12.8522)\t\n",
      "Epoch: [3][50/100]\tBatch Time 0.025 (0.026)\tLoss 12.7077 (12.2760)\t\n",
      "Epoch: [3][75/100]\tBatch Time 0.026 (0.026)\tLoss 11.2633 (12.2584)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 11.5957 (11.5957)\t\n",
      "[25/20]\tBatch Time 0.021 (0.022)\tLoss 11.8447 (12.2427)\t\n",
      "[50/20]\tBatch Time 0.021 (0.022)\tLoss 12.0258 (12.2088)\t\n",
      "[75/20]\tBatch Time 0.021 (0.021)\tLoss 14.2179 (12.2822)\t\n",
      "\n",
      " * LOSS - 12.280\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n",
      "Epoch: [4][0/100]\tBatch Time 0.026 (0.026)\tLoss 10.2461 (10.2461)\t\n",
      "Epoch: [4][25/100]\tBatch Time 0.025 (0.026)\tLoss 12.0673 (12.3228)\t\n",
      "Epoch: [4][50/100]\tBatch Time 0.025 (0.026)\tLoss 10.8899 (12.0104)\t\n",
      "Epoch: [4][75/100]\tBatch Time 0.025 (0.026)\tLoss 11.7801 (12.0381)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 16.9313 (16.9313)\t\n",
      "[25/20]\tBatch Time 0.021 (0.022)\tLoss 13.5703 (12.6764)\t\n",
      "[50/20]\tBatch Time 0.021 (0.022)\tLoss 11.6345 (12.4694)\t\n",
      "[75/20]\tBatch Time 0.021 (0.021)\tLoss 11.0604 (12.2121)\t\n",
      "\n",
      " * LOSS - 12.269\n",
      "\n",
      "\n",
      "Epochs since last improvement: 5\n",
      "\n",
      "Epoch: [5][0/100]\tBatch Time 0.026 (0.026)\tLoss 10.8252 (10.8252)\t\n",
      "Epoch: [5][25/100]\tBatch Time 0.026 (0.026)\tLoss 9.9732 (12.2712)\t\n",
      "Epoch: [5][50/100]\tBatch Time 0.026 (0.026)\tLoss 13.7024 (12.0841)\t\n",
      "Epoch: [5][75/100]\tBatch Time 0.026 (0.026)\tLoss 15.5615 (12.3440)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 10.8580 (10.8580)\t\n",
      "[25/20]\tBatch Time 0.021 (0.021)\tLoss 13.0345 (12.3835)\t\n",
      "[50/20]\tBatch Time 0.022 (0.022)\tLoss 14.7006 (12.2144)\t\n",
      "[75/20]\tBatch Time 0.021 (0.021)\tLoss 12.2003 (12.1723)\t\n",
      "\n",
      " * LOSS - 12.257\n",
      "\n",
      "\n",
      "Epochs since last improvement: 6\n",
      "\n",
      "Epoch: [6][0/100]\tBatch Time 0.026 (0.026)\tLoss 11.7370 (11.7370)\t\n",
      "Epoch: [6][25/100]\tBatch Time 0.025 (0.026)\tLoss 13.3600 (12.0291)\t\n",
      "Epoch: [6][50/100]\tBatch Time 0.026 (0.026)\tLoss 12.3503 (12.0960)\t\n",
      "Epoch: [6][75/100]\tBatch Time 0.025 (0.026)\tLoss 10.0829 (12.1993)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 13.8680 (13.8680)\t\n",
      "[25/20]\tBatch Time 0.021 (0.021)\tLoss 12.4007 (12.3638)\t\n",
      "[50/20]\tBatch Time 0.021 (0.021)\tLoss 11.5253 (12.0190)\t\n",
      "[75/20]\tBatch Time 0.022 (0.022)\tLoss 9.0135 (12.1416)\t\n",
      "\n",
      " * LOSS - 12.249\n",
      "\n",
      "\n",
      "Epochs since last improvement: 7\n",
      "\n",
      "Epoch: [7][0/100]\tBatch Time 0.026 (0.026)\tLoss 11.8080 (11.8080)\t\n",
      "Epoch: [7][25/100]\tBatch Time 0.025 (0.025)\tLoss 12.0072 (12.0307)\t\n",
      "Epoch: [7][50/100]\tBatch Time 0.026 (0.025)\tLoss 13.3385 (11.9941)\t\n",
      "Epoch: [7][75/100]\tBatch Time 0.026 (0.025)\tLoss 12.9198 (12.2159)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 12.7598 (12.7598)\t\n",
      "[25/20]\tBatch Time 0.022 (0.022)\tLoss 11.7174 (12.1040)\t\n",
      "[50/20]\tBatch Time 0.021 (0.022)\tLoss 11.8745 (12.0555)\t\n",
      "[75/20]\tBatch Time 0.021 (0.022)\tLoss 12.3506 (12.2198)\t\n",
      "\n",
      " * LOSS - 12.230\n",
      "\n",
      "\n",
      "Epochs since last improvement: 8\n",
      "\n",
      "Epoch: [8][0/100]\tBatch Time 0.026 (0.026)\tLoss 13.9729 (13.9729)\t\n",
      "Epoch: [8][25/100]\tBatch Time 0.025 (0.025)\tLoss 12.6459 (12.3644)\t\n",
      "Epoch: [8][50/100]\tBatch Time 0.025 (0.025)\tLoss 9.2711 (12.2705)\t\n",
      "Epoch: [8][75/100]\tBatch Time 0.025 (0.025)\tLoss 11.3662 (12.2205)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 10.8152 (10.8152)\t\n",
      "[25/20]\tBatch Time 0.021 (0.021)\tLoss 12.1578 (11.6370)\t\n",
      "[50/20]\tBatch Time 0.021 (0.021)\tLoss 10.9672 (11.9960)\t\n",
      "[75/20]\tBatch Time 0.021 (0.021)\tLoss 13.7335 (12.2896)\t\n",
      "\n",
      " * LOSS - 12.203\n",
      "\n",
      "\n",
      "Epochs since last improvement: 9\n",
      "\n",
      "Epoch: [9][0/100]\tBatch Time 0.026 (0.026)\tLoss 14.0778 (14.0778)\t\n",
      "Epoch: [9][25/100]\tBatch Time 0.026 (0.026)\tLoss 10.7932 (12.4316)\t\n",
      "Epoch: [9][50/100]\tBatch Time 0.026 (0.026)\tLoss 11.4408 (12.3564)\t\n",
      "Epoch: [9][75/100]\tBatch Time 0.025 (0.026)\tLoss 13.1849 (12.4655)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 11.8425 (11.8425)\t\n",
      "[25/20]\tBatch Time 0.021 (0.021)\tLoss 11.9010 (12.3314)\t\n",
      "[50/20]\tBatch Time 0.021 (0.021)\tLoss 12.3404 (12.4179)\t\n",
      "[75/20]\tBatch Time 0.022 (0.021)\tLoss 10.7613 (12.3671)\t\n",
      "\n",
      " * LOSS - 12.179\n",
      "\n",
      "\n",
      "Epochs since last improvement: 10\n",
      "\n",
      "Epoch: [10][0/100]\tBatch Time 0.026 (0.026)\tLoss 11.3497 (11.3497)\t\n",
      "Epoch: [10][25/100]\tBatch Time 0.025 (0.026)\tLoss 12.9704 (12.1344)\t\n",
      "Epoch: [10][50/100]\tBatch Time 0.025 (0.025)\tLoss 13.7953 (12.3316)\t\n",
      "Epoch: [10][75/100]\tBatch Time 0.025 (0.025)\tLoss 12.8580 (12.2688)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 13.0317 (13.0317)\t\n",
      "[25/20]\tBatch Time 0.021 (0.022)\tLoss 10.4671 (12.3747)\t\n",
      "[50/20]\tBatch Time 0.026 (0.022)\tLoss 15.3514 (12.3411)\t\n",
      "[75/20]\tBatch Time 0.022 (0.022)\tLoss 13.6514 (12.1693)\t\n",
      "\n",
      " * LOSS - 12.154\n",
      "\n",
      "\n",
      "Epochs since last improvement: 11\n",
      "\n",
      "Epoch: [11][0/100]\tBatch Time 0.026 (0.026)\tLoss 9.9976 (9.9976)\t\n",
      "Epoch: [11][25/100]\tBatch Time 0.027 (0.025)\tLoss 13.0572 (11.6784)\t\n",
      "Epoch: [11][50/100]\tBatch Time 0.025 (0.025)\tLoss 11.5224 (12.1869)\t\n",
      "Epoch: [11][75/100]\tBatch Time 0.025 (0.025)\tLoss 10.1974 (12.0630)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 11.3052 (11.3052)\t\n",
      "[25/20]\tBatch Time 0.021 (0.021)\tLoss 11.6786 (11.9018)\t\n",
      "[50/20]\tBatch Time 0.021 (0.021)\tLoss 9.4995 (12.1288)\t\n",
      "[75/20]\tBatch Time 0.021 (0.021)\tLoss 11.7901 (12.2006)\t\n",
      "\n",
      " * LOSS - 12.133\n",
      "\n",
      "\n",
      "Epochs since last improvement: 12\n",
      "\n",
      "Epoch: [12][0/100]\tBatch Time 0.026 (0.026)\tLoss 17.5416 (17.5416)\t\n",
      "Epoch: [12][25/100]\tBatch Time 0.025 (0.025)\tLoss 11.3115 (12.2621)\t\n",
      "Epoch: [12][50/100]\tBatch Time 0.025 (0.025)\tLoss 12.0723 (12.1191)\t\n",
      "Epoch: [12][75/100]\tBatch Time 0.026 (0.025)\tLoss 14.5624 (12.1356)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 12.9696 (12.9696)\t\n",
      "[25/20]\tBatch Time 0.023 (0.022)\tLoss 15.7487 (12.2994)\t\n",
      "[50/20]\tBatch Time 0.021 (0.022)\tLoss 11.0301 (12.2549)\t\n",
      "[75/20]\tBatch Time 0.022 (0.022)\tLoss 13.2229 (12.3001)\t\n",
      "\n",
      " * LOSS - 12.105\n",
      "\n",
      "\n",
      "Epochs since last improvement: 13\n",
      "\n",
      "Epoch: [13][0/100]\tBatch Time 0.026 (0.026)\tLoss 9.6345 (9.6345)\t\n",
      "Epoch: [13][25/100]\tBatch Time 0.026 (0.026)\tLoss 11.2500 (12.0245)\t\n",
      "Epoch: [13][50/100]\tBatch Time 0.025 (0.026)\tLoss 11.5269 (12.0876)\t\n",
      "Epoch: [13][75/100]\tBatch Time 0.025 (0.025)\tLoss 12.6343 (12.1085)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 13.9990 (13.9990)\t\n",
      "[25/20]\tBatch Time 0.021 (0.022)\tLoss 10.9014 (12.4894)\t\n",
      "[50/20]\tBatch Time 0.021 (0.022)\tLoss 11.2565 (12.2217)\t\n",
      "[75/20]\tBatch Time 0.021 (0.022)\tLoss 11.3459 (12.0548)\t\n",
      "\n",
      " * LOSS - 12.093\n",
      "\n",
      "\n",
      "Epochs since last improvement: 14\n",
      "\n",
      "Epoch: [14][0/100]\tBatch Time 0.026 (0.026)\tLoss 13.5541 (13.5541)\t\n",
      "Epoch: [14][25/100]\tBatch Time 0.026 (0.026)\tLoss 10.0023 (12.0791)\t\n",
      "Epoch: [14][50/100]\tBatch Time 0.026 (0.026)\tLoss 10.4446 (12.0638)\t\n",
      "Epoch: [14][75/100]\tBatch Time 0.025 (0.025)\tLoss 12.5569 (12.0152)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 11.4881 (11.4881)\t\n",
      "[25/20]\tBatch Time 0.022 (0.022)\tLoss 14.3552 (12.0838)\t\n",
      "[50/20]\tBatch Time 0.021 (0.022)\tLoss 12.5470 (11.9312)\t\n",
      "[75/20]\tBatch Time 0.022 (0.022)\tLoss 13.3766 (12.1588)\t\n",
      "\n",
      " * LOSS - 12.071\n",
      "\n",
      "\n",
      "Epochs since last improvement: 15\n",
      "\n",
      "Epoch: [15][0/100]\tBatch Time 0.026 (0.026)\tLoss 15.2356 (15.2356)\t\n",
      "Epoch: [15][25/100]\tBatch Time 0.025 (0.026)\tLoss 9.0426 (11.7382)\t\n",
      "Epoch: [15][50/100]\tBatch Time 0.025 (0.025)\tLoss 13.1456 (12.0269)\t\n",
      "Epoch: [15][75/100]\tBatch Time 0.025 (0.026)\tLoss 11.7519 (12.0348)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 14.0839 (14.0839)\t\n",
      "[25/20]\tBatch Time 0.021 (0.021)\tLoss 13.2609 (12.4215)\t\n",
      "[50/20]\tBatch Time 0.022 (0.021)\tLoss 11.5295 (12.2586)\t\n",
      "[75/20]\tBatch Time 0.021 (0.021)\tLoss 10.3260 (12.0544)\t\n",
      "\n",
      " * LOSS - 12.059\n",
      "\n",
      "\n",
      "Epochs since last improvement: 16\n",
      "\n",
      "Epoch: [16][0/100]\tBatch Time 0.026 (0.026)\tLoss 13.7421 (13.7421)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][25/100]\tBatch Time 0.025 (0.026)\tLoss 12.9281 (12.4545)\t\n",
      "Epoch: [16][50/100]\tBatch Time 0.025 (0.025)\tLoss 10.6570 (12.1292)\t\n",
      "Epoch: [16][75/100]\tBatch Time 0.025 (0.026)\tLoss 11.7554 (12.1559)\t\n",
      "[0/20]\tBatch Time 0.021 (0.021)\tLoss 11.0764 (11.0764)\t\n",
      "[25/20]\tBatch Time 0.021 (0.021)\tLoss 14.9869 (12.0345)\t\n",
      "[50/20]\tBatch Time 0.021 (0.021)\tLoss 10.1872 (12.0868)\t\n",
      "[75/20]\tBatch Time 0.022 (0.021)\tLoss 12.3615 (12.1692)\t\n",
      "\n",
      " * LOSS - 12.055\n",
      "\n",
      "\n",
      "Epochs since last improvement: 17\n",
      "\n",
      "Epoch: [17][0/100]\tBatch Time 0.026 (0.026)\tLoss 14.5162 (14.5162)\t\n",
      "Epoch: [17][25/100]\tBatch Time 0.025 (0.026)\tLoss 12.6217 (12.2623)\t\n",
      "Epoch: [17][50/100]\tBatch Time 0.025 (0.025)\tLoss 14.5120 (11.9603)\t\n",
      "Epoch: [17][75/100]\tBatch Time 0.025 (0.025)\tLoss 13.6587 (12.0144)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 10.4114 (10.4114)\t\n",
      "[25/20]\tBatch Time 0.021 (0.021)\tLoss 13.5938 (11.5628)\t\n",
      "[50/20]\tBatch Time 0.021 (0.021)\tLoss 11.0739 (11.7500)\t\n",
      "[75/20]\tBatch Time 0.021 (0.021)\tLoss 11.4339 (11.9367)\t\n",
      "\n",
      " * LOSS - 12.050\n",
      "\n",
      "\n",
      "Epochs since last improvement: 18\n",
      "\n",
      "Epoch: [18][0/100]\tBatch Time 0.026 (0.026)\tLoss 14.1561 (14.1561)\t\n",
      "Epoch: [18][25/100]\tBatch Time 0.025 (0.025)\tLoss 9.3499 (12.0912)\t\n",
      "Epoch: [18][50/100]\tBatch Time 0.025 (0.025)\tLoss 13.1770 (12.1707)\t\n",
      "Epoch: [18][75/100]\tBatch Time 0.025 (0.025)\tLoss 13.7676 (11.9455)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 9.7196 (9.7196)\t\n",
      "[25/20]\tBatch Time 0.021 (0.021)\tLoss 11.7695 (12.1562)\t\n",
      "[50/20]\tBatch Time 0.021 (0.021)\tLoss 11.6063 (11.8632)\t\n",
      "[75/20]\tBatch Time 0.021 (0.021)\tLoss 13.6421 (11.8831)\t\n",
      "\n",
      " * LOSS - 12.040\n",
      "\n",
      "\n",
      "Epochs since last improvement: 19\n",
      "\n",
      "Epoch: [19][0/100]\tBatch Time 0.026 (0.026)\tLoss 10.1780 (10.1780)\t\n",
      "Epoch: [19][25/100]\tBatch Time 0.026 (0.025)\tLoss 14.2330 (12.1263)\t\n",
      "Epoch: [19][50/100]\tBatch Time 0.025 (0.025)\tLoss 13.3411 (12.1924)\t\n",
      "Epoch: [19][75/100]\tBatch Time 0.026 (0.025)\tLoss 10.9831 (11.9914)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 11.3040 (11.3040)\t\n",
      "[25/20]\tBatch Time 0.021 (0.021)\tLoss 13.8526 (12.1429)\t\n",
      "[50/20]\tBatch Time 0.021 (0.021)\tLoss 12.8754 (12.0121)\t\n",
      "[75/20]\tBatch Time 0.021 (0.021)\tLoss 10.5753 (11.8487)\t\n",
      "\n",
      " * LOSS - 12.034\n",
      "\n",
      "\n",
      "Epochs since last improvement: 20\n",
      "\n",
      "Epoch: [20][0/100]\tBatch Time 0.026 (0.026)\tLoss 12.3814 (12.3814)\t\n",
      "Epoch: [20][25/100]\tBatch Time 0.025 (0.025)\tLoss 12.2963 (12.3289)\t\n",
      "Epoch: [20][50/100]\tBatch Time 0.025 (0.025)\tLoss 14.7269 (12.1828)\t\n",
      "Epoch: [20][75/100]\tBatch Time 0.025 (0.025)\tLoss 10.9724 (12.1717)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 12.8200 (12.8200)\t\n",
      "[25/20]\tBatch Time 0.021 (0.021)\tLoss 12.0565 (11.6906)\t\n",
      "[50/20]\tBatch Time 0.022 (0.022)\tLoss 10.5790 (12.1394)\t\n",
      "[75/20]\tBatch Time 0.021 (0.022)\tLoss 13.1644 (12.1044)\t\n",
      "\n",
      " * LOSS - 12.029\n",
      "\n",
      "\n",
      "Epochs since last improvement: 21\n",
      "\n",
      "Epoch: [21][0/100]\tBatch Time 0.026 (0.026)\tLoss 12.9353 (12.9353)\t\n",
      "Epoch: [21][25/100]\tBatch Time 0.025 (0.025)\tLoss 9.6529 (11.9371)\t\n",
      "Epoch: [21][50/100]\tBatch Time 0.025 (0.025)\tLoss 10.8452 (12.0106)\t\n",
      "Epoch: [21][75/100]\tBatch Time 0.026 (0.025)\tLoss 12.1848 (11.9847)\t\n",
      "[0/20]\tBatch Time 0.021 (0.021)\tLoss 10.5127 (10.5127)\t\n",
      "[25/20]\tBatch Time 0.021 (0.022)\tLoss 11.6550 (12.2824)\t\n",
      "[50/20]\tBatch Time 0.021 (0.021)\tLoss 13.3576 (12.1024)\t\n",
      "[75/20]\tBatch Time 0.021 (0.021)\tLoss 10.4895 (11.9996)\t\n",
      "\n",
      " * LOSS - 12.034\n",
      "\n",
      "\n",
      "Epochs since last improvement: 22\n",
      "\n",
      "Epoch: [22][0/100]\tBatch Time 0.026 (0.026)\tLoss 11.5507 (11.5507)\t\n",
      "Epoch: [22][25/100]\tBatch Time 0.025 (0.025)\tLoss 11.7574 (12.6477)\t\n",
      "Epoch: [22][50/100]\tBatch Time 0.025 (0.025)\tLoss 12.1071 (12.3127)\t\n",
      "Epoch: [22][75/100]\tBatch Time 0.025 (0.025)\tLoss 14.0424 (12.1362)\t\n",
      "[0/20]\tBatch Time 0.021 (0.021)\tLoss 11.9213 (11.9213)\t\n",
      "[25/20]\tBatch Time 0.021 (0.022)\tLoss 11.6160 (12.2891)\t\n",
      "[50/20]\tBatch Time 0.021 (0.022)\tLoss 10.0901 (12.0683)\t\n",
      "[75/20]\tBatch Time 0.021 (0.021)\tLoss 10.5140 (11.9959)\t\n",
      "\n",
      " * LOSS - 12.032\n",
      "\n",
      "\n",
      "Epochs since last improvement: 23\n",
      "\n",
      "Epoch: [23][0/100]\tBatch Time 0.026 (0.026)\tLoss 13.1038 (13.1038)\t\n",
      "Epoch: [23][25/100]\tBatch Time 0.025 (0.025)\tLoss 14.3831 (12.3462)\t\n",
      "Epoch: [23][50/100]\tBatch Time 0.025 (0.025)\tLoss 13.1267 (12.2682)\t\n",
      "Epoch: [23][75/100]\tBatch Time 0.025 (0.025)\tLoss 12.7985 (12.0738)\t\n",
      "[0/20]\tBatch Time 0.021 (0.021)\tLoss 13.6554 (13.6554)\t\n",
      "[25/20]\tBatch Time 0.021 (0.021)\tLoss 12.5729 (12.3989)\t\n",
      "[50/20]\tBatch Time 0.021 (0.021)\tLoss 9.3346 (12.3036)\t\n",
      "[75/20]\tBatch Time 0.022 (0.021)\tLoss 11.8796 (12.1294)\t\n",
      "\n",
      " * LOSS - 12.035\n",
      "\n",
      "\n",
      "Epochs since last improvement: 24\n",
      "\n",
      "Epoch: [24][0/100]\tBatch Time 0.026 (0.026)\tLoss 16.8674 (16.8674)\t\n",
      "Epoch: [24][25/100]\tBatch Time 0.025 (0.025)\tLoss 11.5911 (11.9460)\t\n",
      "Epoch: [24][50/100]\tBatch Time 0.025 (0.025)\tLoss 14.3632 (11.8291)\t\n",
      "Epoch: [24][75/100]\tBatch Time 0.025 (0.025)\tLoss 13.3759 (12.0628)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 9.8368 (9.8368)\t\n",
      "[25/20]\tBatch Time 0.021 (0.022)\tLoss 10.1567 (11.8813)\t\n",
      "[50/20]\tBatch Time 0.021 (0.022)\tLoss 13.1590 (11.9526)\t\n",
      "[75/20]\tBatch Time 0.021 (0.021)\tLoss 12.9093 (11.8613)\t\n",
      "\n",
      " * LOSS - 12.027\n",
      "\n",
      "\n",
      "Epochs since last improvement: 25\n",
      "\n",
      "Epoch: [25][0/100]\tBatch Time 0.026 (0.026)\tLoss 12.4527 (12.4527)\t\n",
      "Epoch: [25][25/100]\tBatch Time 0.025 (0.025)\tLoss 11.3974 (11.5809)\t\n",
      "Epoch: [25][50/100]\tBatch Time 0.025 (0.025)\tLoss 11.9892 (11.8831)\t\n",
      "Epoch: [25][75/100]\tBatch Time 0.025 (0.025)\tLoss 11.9369 (12.0493)\t\n",
      "[0/20]\tBatch Time 0.021 (0.021)\tLoss 13.1262 (13.1262)\t\n",
      "[25/20]\tBatch Time 0.021 (0.021)\tLoss 10.9111 (11.9463)\t\n",
      "[50/20]\tBatch Time 0.021 (0.021)\tLoss 11.4427 (11.9250)\t\n",
      "[75/20]\tBatch Time 0.021 (0.021)\tLoss 12.5521 (12.0510)\t\n",
      "\n",
      " * LOSS - 12.024\n",
      "\n",
      "\n",
      "Epochs since last improvement: 26\n",
      "\n",
      "Epoch: [26][0/100]\tBatch Time 0.026 (0.026)\tLoss 13.5936 (13.5936)\t\n",
      "Epoch: [26][25/100]\tBatch Time 0.025 (0.025)\tLoss 14.2969 (11.6551)\t\n",
      "Epoch: [26][50/100]\tBatch Time 0.025 (0.025)\tLoss 12.1651 (11.8173)\t\n",
      "Epoch: [26][75/100]\tBatch Time 0.025 (0.025)\tLoss 10.4013 (12.0047)\t\n",
      "[0/20]\tBatch Time 0.021 (0.021)\tLoss 13.1032 (13.1032)\t\n",
      "[25/20]\tBatch Time 0.021 (0.021)\tLoss 10.4568 (11.7751)\t\n",
      "[50/20]\tBatch Time 0.021 (0.021)\tLoss 11.5071 (11.9481)\t\n",
      "[75/20]\tBatch Time 0.022 (0.021)\tLoss 12.5592 (12.0121)\t\n",
      "\n",
      " * LOSS - 12.022\n",
      "\n",
      "\n",
      "Epochs since last improvement: 27\n",
      "\n",
      "Epoch: [27][0/100]\tBatch Time 0.026 (0.026)\tLoss 13.9810 (13.9810)\t\n",
      "Epoch: [27][25/100]\tBatch Time 0.025 (0.025)\tLoss 10.9993 (11.1315)\t\n",
      "Epoch: [27][50/100]\tBatch Time 0.025 (0.025)\tLoss 12.8847 (11.6717)\t\n",
      "Epoch: [27][75/100]\tBatch Time 0.025 (0.025)\tLoss 13.4710 (11.6926)\t\n",
      "[0/20]\tBatch Time 0.021 (0.021)\tLoss 9.3317 (9.3317)\t\n",
      "[25/20]\tBatch Time 0.021 (0.021)\tLoss 13.9523 (11.9213)\t\n",
      "[50/20]\tBatch Time 0.021 (0.021)\tLoss 14.7250 (12.0888)\t\n",
      "[75/20]\tBatch Time 0.022 (0.021)\tLoss 13.6931 (12.1587)\t\n",
      "\n",
      " * LOSS - 12.023\n",
      "\n",
      "\n",
      "Epochs since last improvement: 28\n",
      "\n",
      "Epoch: [28][0/100]\tBatch Time 0.026 (0.026)\tLoss 9.8560 (9.8560)\t\n",
      "Epoch: [28][25/100]\tBatch Time 0.025 (0.025)\tLoss 9.7692 (11.7675)\t\n",
      "Epoch: [28][50/100]\tBatch Time 0.025 (0.025)\tLoss 12.7762 (11.9273)\t\n",
      "Epoch: [28][75/100]\tBatch Time 0.025 (0.025)\tLoss 9.6815 (11.9856)\t\n",
      "[0/20]\tBatch Time 0.022 (0.022)\tLoss 11.9306 (11.9306)\t\n",
      "[25/20]\tBatch Time 0.022 (0.022)\tLoss 12.3230 (12.4574)\t\n",
      "[50/20]\tBatch Time 0.021 (0.022)\tLoss 11.3175 (12.3072)\t\n",
      "[75/20]\tBatch Time 0.021 (0.022)\tLoss 8.9641 (12.0722)\t\n",
      "\n",
      " * LOSS - 12.021\n",
      "\n",
      "\n",
      "Epochs since last improvement: 29\n",
      "\n",
      "Epoch: [29][0/100]\tBatch Time 0.026 (0.026)\tLoss 11.8698 (11.8698)\t\n",
      "Epoch: [29][25/100]\tBatch Time 0.025 (0.025)\tLoss 12.9596 (11.7631)\t\n",
      "Epoch: [29][50/100]\tBatch Time 0.025 (0.025)\tLoss 14.6553 (12.0223)\t\n",
      "Epoch: [29][75/100]\tBatch Time 0.025 (0.025)\tLoss 14.9544 (11.9958)\t\n",
      "[0/20]\tBatch Time 0.021 (0.021)\tLoss 11.1971 (11.1971)\t\n",
      "[25/20]\tBatch Time 0.021 (0.022)\tLoss 8.7827 (11.9839)\t\n",
      "[50/20]\tBatch Time 0.021 (0.021)\tLoss 10.6320 (11.9536)\t\n",
      "[75/20]\tBatch Time 0.026 (0.022)\tLoss 11.6564 (11.9714)\t\n",
      "\n",
      " * LOSS - 12.021\n",
      "\n",
      "\n",
      "Epochs since last improvement: 30\n",
      "\n",
      "All ready!\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Ready!\n"
     ]
    }
   ],
   "source": [
    "print('All Ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images length 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rui/mlenv/lib/python3.6/site-packages/skimage/io/_io.py:49: UserWarning: `as_grey` has been deprecated in favor of `as_gray`\n",
      "  warn('`as_grey` has been deprecated in favor of `as_gray`')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-536a2b37c005>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"images length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# PREDICT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# -- Check the results -------------------------------------------------------\n",
    "\n",
    "# -- Utility ---------------------------------------------\n",
    "def makeCrops(image, stepSize, windowSize, true_center):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    image = image.type(torch.FloatTensor)\n",
    "    crops = []\n",
    "    truths = []\n",
    "    c_x, c_y, orient = true_center\n",
    "    # TODO: look into otdering, why it's y,x !\n",
    "    margin = 15\n",
    "    # --> is x, but is the column\n",
    "    # to slide horizontally, y must come first\n",
    "    for y in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "        for x in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "            end_x, end_y = x + windowSize[1], y + windowSize[0]\n",
    "            hasRect = (x + margin < c_x < end_x - margin) and (\n",
    "                y + margin < c_y < end_y - margin\n",
    "            )\n",
    "            truths.append(hasRect)\n",
    "            crops.append(image[y:end_y, x:end_x])\n",
    "    crops = torch.stack(crops)\n",
    "    print(\"shape of crops\", crops.shape)\n",
    "    return crops, truths\n",
    "\n",
    "\n",
    "# -- Get some validation results ---------------------------------------------\n",
    "model.to(device).eval()\n",
    "with torch.no_grad():\n",
    "    dataiter = iter(test_loader)\n",
    "    images, labels, coords = dataiter.next()\n",
    "    print(\"images length\", len(images))\n",
    "    outputs = model(images)  # PREDICT\n",
    "    loss = criterion(outputs.to(device), labels.to(device))\n",
    "    print(loss)\n",
    "\n",
    "\n",
    "# -- Plot windows and print labels -----------------------------------\n",
    "n = 1\n",
    "imgSample = images[n]\n",
    "crops, truths = makeCrops(imgSample, 50, (100, 100), coords[n])\n",
    "\n",
    "# -- Print x,y for one result -------\n",
    "print(\"!-- FOR N = \", n)\n",
    "print(\"y \\n\\t\", [int(v) for v in labels[n]])\n",
    "print(\"!-- y \\n\\t\", [int(n) for n in truths])\n",
    "print(\"!-- yhat \\n\\t\", [int(round(o, 0)) for o in outputs[n].cpu().numpy()])\n",
    "print(\"\\n ------ x,y center + orient: \", coords[n], \"\\n\")\n",
    "\n",
    "# -- Print outputs for multiple results -------\n",
    "for n in range(5):\n",
    "    print(\"!-- y \\t\", [int(n) for n in labels[n]],\n",
    "          \"!-- yhat \\t\", [int(round(o, 0)) for o in outputs[n].cpu().numpy()],\n",
    "          \"\\t loss: %.5f\" % (criterion(outputs[n], labels[n].to(device))),)\n",
    "\n",
    "# -- Show original image, and the sliding windo crops -------\n",
    "plt.imshow(imgSample)\n",
    "\n",
    "for (i, crop) in enumerate(crops):\n",
    "    # print(\"1-index number of window: \", i+1, 'x', x, 'y', y, 'has rectangle?', hasRect)\n",
    "    plt.figure()\n",
    "    plt.suptitle(\"numero: %d\" % (i))\n",
    "    plt.imshow(crop)\n",
    "\n",
    "\n",
    "# -- Main ---------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
