{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import io\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "# gen images\n",
    "IMG_X, IMG_Y = 200,200 \n",
    "# length and width of blocks (fixed for now)\n",
    "block_l, block_w = 20, 30\n",
    "\n",
    "# img_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "def makeRectangle(l, w, theta, offset=(0, 0)):\n",
    "    c, s = math.cos(theta), math.sin(theta)\n",
    "    rectCoords = [(l/2.0, w/2.0), (l/2.0, -w/2.0), (-l/2.0, -w/2.0), (-l/2.0, w/2.0)]\n",
    "    return [(c*x-s*y+offset[0], s*x+c*y+offset[1]) for (x, y) in rectCoords]\n",
    "\n",
    "\n",
    "# ---- Make depth images ---\n",
    "def make_dataset(dirname, num_images):\n",
    "    true_coords = []\n",
    "    newpath = './' + dirname  \n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "        print(newpath)\n",
    "    for i in range(num_images):\n",
    "        #orient = 0 # degrees\n",
    "        img = Image.new('RGB', (IMG_X, IMG_Y), 'black')\n",
    "\n",
    "        # block_l and _w offset so blocks don't run off edge of image \n",
    "        rand_x = int(np.random.rand() * (IMG_X-2*block_l)) + block_l\n",
    "        rand_y = int(np.random.rand() * (IMG_Y-2*block_w)) + block_w\n",
    "        orient = int(np.random.rand() * 180)  # .random() is range [0.0, 1.0).\n",
    "        orient = math.radians(orient) # math.cos takes radians!\n",
    "\n",
    "        true_coords.append(np.array((rand_x, rand_y, orient)))\n",
    "\n",
    "        rect_vertices = makeRectangle(block_l, block_w, orient, offset=(rand_x,\n",
    "                                                                        rand_y))\n",
    "\n",
    "        idraw = ImageDraw.Draw(img)\n",
    "        idraw.polygon(rect_vertices, fill='white')\n",
    "\n",
    "        # use a truetype font\n",
    "        #font = imagefont.truetype(\"dejavusans.ttf\", 15)\n",
    "        #font = imagefont.truetype(\"arial.ttf\",14)\n",
    "        #idraw.text((10, 25), '('+ str(rand_x) + ', ' + str(rand_y) +')')\n",
    "        img.save(newpath + '/rect'+str(i)+'.png')\n",
    "    return true_coords\n",
    "\n",
    "train_truth = make_dataset('data', 500)\n",
    "print(len(train_truth))\n",
    "test_truth = make_dataset('./data/test', 300)\n",
    "\n",
    "np.save('train_truth.npy', train_truth)\n",
    "np.save('test_truth.npy', test_truth)\n",
    "\n",
    "\n",
    "class RectDepthImgsDataset(Dataset):\n",
    "    \"\"\"Artificially generated depth images dataset\"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, coords, transform=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.true_coords = coords\n",
    "        self.transform = transform\n",
    "        STEPSIZE = 50  # todo make this input arguments\n",
    "        WINDOWSIZE = (100, 100)\n",
    "        MARGIN_PX = 10\n",
    "        self.step = STEPSIZE\n",
    "        self.cropSize = WINDOWSIZE\n",
    "        self.detectMargin = MARGIN_PX\n",
    "\n",
    "    def __len__(self):\n",
    "        #print('true coord len', len(self.true_coords))\n",
    "        return len(self.true_coords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # image = self.images[idx]\n",
    "        image = io.imread(self.img_dir + '/rect'+str(idx)+'.png')\n",
    "        image = torch.FloatTensor(image).permute(\n",
    "            2, 0, 1)  # PIL and torch expect difft orders\n",
    "        coords = torch.FloatTensor(self.true_coords[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        crops, labels = self.makeCrops(image, self.step, self.cropSize, coords,\n",
    "                                       self.detectMargin)\n",
    "\n",
    "        #sample = {'crops': crops, 'labels': labels}\n",
    "        return crops, labels\n",
    "\n",
    "    def makeCrops(self, image, stepSize, windowSize, rectCenter, detectMargin):\n",
    "        \"\"\"\n",
    "        Returns image crops, as well as T/F for those crops \n",
    "        \"\"\"\n",
    "        crops = []\n",
    "        #print(rectCenter)\n",
    "        c_x, c_y, theta = rectCenter\n",
    "        margin = detectMargin\n",
    "        truths = []\n",
    "        # TODO : look into why it's y,x !\n",
    "        print('image type in crops', type(image))\n",
    "        for x in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "            for y in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "                end_x, end_y = x + windowSize[0], y + windowSize[1]\n",
    "                hasRect = (x + margin < c_x < end_x - margin) and \\\n",
    "                    (y + margin < c_y < end_y - margin)\n",
    "                crops.append(image[y:end_y, x:end_x])\n",
    "                truths.append(hasRect)\n",
    "        return crops, truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 3)\n",
      "[ 56.        131.          0.2443461]\n",
      "200 200\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read in Images\n",
    "train_truth = np.load('train_truth.npy')# loading the training and testing data\n",
    "test_truth = np.load('test_truth.npy')# loading the training and testing data\n",
    "print(train_truth.shape)\n",
    "print(train_truth[1])\n",
    "\n",
    "#from IPython.display import Image, display\n",
    "foo_image = io.imread('./data'+ '/rect'+str(1)+'.png')\n",
    "plt.imshow(foo_image)\n",
    "print(IMG_X, IMG_Y)\n",
    "#plt.imshow(foo_image[100:200, 100:200]o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available? device:  cuda:0\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 10 \n",
    "num_classes = 3 # predicting x,y,orientation\n",
    "learning_rate = 0.001\n",
    "batch_size = 15 \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA available? device: \", device)\n",
    "\n",
    "# Dataset is depth images of rectangular blocks\n",
    "train_dataset = RectDepthImgsDataset(img_dir='./data', coords=train_truth)\n",
    "print(len(train_dataset))\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_dataset = RectDepthImgsDataset(img_dir='./data/test', coords=test_truth)\n",
    "\n",
    "# Data loader\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):  # CIFAR is 32x32x3, MNIST is 28x28x1)\n",
    "    def __init__(self, IMG_X, IMG_Y):\n",
    "        \"\"\" We need the image width and height to determine CNN layer sizes\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        self._imgx = IMG_X\n",
    "        self._imgy = IMG_Y\n",
    "        _pool = 2\n",
    "        _stride = 5\n",
    "        _outputlayers = 16\n",
    "\n",
    "        STEPSIZE = 50  # todo make this input arguments\n",
    "        WINDOWSIZE = (100, 100)\n",
    "        self.step = STEPSIZE\n",
    "        self.cropSize = WINDOWSIZE\n",
    "        # self.numCrops = 0\n",
    "        # T/F for now\n",
    "\n",
    "        # calculate number of crops\n",
    "        # for y in range(0, IMG_Y, STEPSIZE):\n",
    "        #    for x in range(0, IMG_X, STEPSIZE):\n",
    "        #        self.numCrops += 1\n",
    "        # TODO: but our net wants everything to be the same size... pad with 0s?\n",
    "\n",
    "        def _calc(val):  # use to calculate layer sizes\n",
    "            layer_size = (val - (_stride - 1)) / _pool\n",
    "            return layer_size\n",
    "\n",
    "        # print(self._imgx)\n",
    "        self._const = _calc(_calc(self._imgx))\n",
    "        self._const *= _calc(_calc(self._imgy))\n",
    "        self._const *= _outputlayers\n",
    "        # print(self._const)\n",
    "        self._const = int(self._const)\n",
    "\n",
    "        # 3 input image channels (RGB)  .6 output channels, 5x5 square convolution\n",
    "        self.conv1 = nn.Conv2d(3, 6, _stride).to(device)\n",
    "        self.pool = nn.MaxPool2d(_pool, _pool).to(device)\n",
    "        self.conv2 = nn.Conv2d(6, _outputlayers, _stride).to(device)\n",
    "        self.fc1 = nn.Linear(self._const, 120).to(device)\n",
    "        self.fc2 = nn.Linear(120, 84).to(device)\n",
    "        self.fc3 = nn.Linear(84, 1).to(device)\n",
    "        # (* N crops, if we wanted output for all # windows at once)\n",
    "        # TODO: batch normalization  self.bn = nn.BatchNorm2d()\n",
    "\n",
    "    #\n",
    "        # Localization prediction convolutions (predict offsets w.r.t prior-boxes)\n",
    "        # self.loc_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * 4, kernel_size=3, padding=1)\n",
    "        # Class prediction convolutions (predict classes in localization boxes)\n",
    "        # self.cl_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * n_classes, kernel_size=3, padding=1)\n",
    "    def forward(self, x):\n",
    "        # print(x.size())\n",
    "        #x = x.to(device)\n",
    "        crops = self.makeCrops(x, self.step, self.cropSize)\n",
    "        box_class_scores = []\n",
    "        for img in crops:\n",
    "            #print(img.shape)\n",
    "            x = img.view(-1, 3, self.cropSize[0], self.cropSize[1])\n",
    "            x = self.pool(F.relu((self.conv1(x))))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            x = x.view(-1, self._const)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            box_class_scores.append(x)\n",
    "        return box_class_scores\n",
    "        # TODO: maybe the last layer should have ReLu or softmax for binary?  # vs multiple classes\n",
    "        # TODO: should we run the windows through as a concat, or let the net\n",
    "        # learn from every individual window? prolly the latter\n",
    "\n",
    "    def makeCrops(self, image, stepSize, windowSize):\n",
    "        \"\"\"\n",
    "        Returns a generator of cropped boxes (the top left x,y, the image data)\n",
    "        \"\"\"\n",
    "        crops = []\n",
    "        # TODO: look into ordering, why it's y,x !\n",
    "        for x in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "            for y in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "                end_x, end_y = x + windowSize[0], y + windowSize[1]\n",
    "                crops.append(image[x:end_x, y:end_y])\n",
    "        # self.numCrops=len(crops)\n",
    "        return crops\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "model = Net(IMG_X, IMG_Y)\n",
    "model = model.to(device)\n",
    "\n",
    "# ONLY FOR DEBUGGING (check if code runs at all)\n",
    "#images = iter(train_loader)\n",
    "##outputs = model(images.next()[0])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model now...\n",
      "image type i ncrops <class 'torch.Tensor'>\n",
      "image type i ncrops <class 'torch.Tensor'>\n",
      "image type i ncrops <class 'torch.Tensor'>\n",
      "image type i ncrops <class 'torch.Tensor'>\n",
      "image type i ncrops <class 'torch.Tensor'>\n",
      "image type i ncrops <class 'torch.Tensor'>\n",
      "image type i ncrops <class 'torch.Tensor'>\n",
      "image type i ncrops <class 'torch.Tensor'>\n",
      "image type i ncrops <class 'torch.Tensor'>\n",
      "image type i ncrops <class 'torch.Tensor'>\n",
      "image type i ncrops <class 'torch.Tensor'>\n",
      "image type i ncrops <class 'torch.Tensor'>\n",
      "image type i ncrops <class 'torch.Tensor'>\n",
      "image type i ncrops <class 'torch.Tensor'>\n",
      "image type i ncrops <class 'torch.Tensor'>\n",
      "labels <class 'list'> []\n",
      "images <class 'list'> []\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-a827fd21ade4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-c7c117019836>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# print(x.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m#x = x.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mcrops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakeCrops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcropSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mbox_class_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-c7c117019836>\u001b[0m in \u001b[0;36mmakeCrops\u001b[0;34m(self, image, stepSize, windowSize)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mcrops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# TODO: look into ordering, why it's y,x !\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwindowSize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstepSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwindowSize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstepSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mend_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwindowSize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwindowSize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "losses_list = []\n",
    "ct = 0\n",
    "print('Training model now...')\n",
    "total_step = len(train_loader)\n",
    "tstart = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for i_batch, (images, labels) in enumerate(train_loader):\n",
    "        #slidow_coords(images)\n",
    "        \n",
    "        #prediction = blah\n",
    "\n",
    "        print('labels', type(labels), labels)\n",
    "        print('images', type(images), images)\n",
    "        images = [img.to(device) for img in images]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images).to(device)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #if (i_batch+1) % 1 == 0:\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Time since start: ', time.time() - tstart)\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1,\n",
    "                                                                      num_epochs,\n",
    "                                                                      i_batch+1,\n",
    "                                                                      total_step,\n",
    "                                                                      loss.item()))\n",
    "        losses_list.append(loss.item())\n",
    "        \n",
    "#windowGenerator = slidow_coords(foo_image, 50, (100,100), (45,101), MARGIN_PX)\n",
    "\n",
    "#MARGIN_PX = 10\n",
    "#for i, foo in enumerate(windowGenerator):\n",
    "    #x,y,img, hasRect = foo\n",
    "    #print(\"1-index number of window: \", i+1, 'x', x, 'y', y, 'has rectangle?', hasRect)\n",
    "    #plt.figure()\n",
    "    #plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
