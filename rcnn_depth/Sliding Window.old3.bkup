{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Sliding Window\n",
    "# Feb 2019\n",
    "#\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import torchvision\n",
    "from IPython.display import display  # to display images\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import io\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sliding Window\n",
    "# Feb 2019\n",
    "# Implement classfication and regression as two separate networks\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import torchvision\n",
    "from IPython.display import display  # to display images\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import io\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataset -------------------------------------------------------\n",
    "IMG_X, IMG_Y = 200, 200\n",
    "# length and width of blocks (fixed for now)\n",
    "block_l, block_w = 20, 30\n",
    "\n",
    "\n",
    "def makeRectangle(l, w, theta, offset=(0, 0)):\n",
    "    c, s = math.cos(theta), math.sin(theta)\n",
    "    rectCoords = [(l / 2.0, w / 2.0), (l / 2.0, -w / 2.0),\n",
    "                  (-l / 2.0, -w / 2.0), (-l / 2.0, w / 2.0), ]\n",
    "    return [(c * x - s * y + offset[0],\n",
    "             s * x + c * y + offset[1]) for (x, y) in rectCoords]\n",
    "\n",
    "\n",
    "# ---- Make depth images ---\n",
    "def make_dataset(dirname, num_images):\n",
    "    true_coords = []\n",
    "    newpath = \"./\" + dirname\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "        print(newpath)\n",
    "    for i in range(num_images):\n",
    "        # orient = 0 # degrees\n",
    "        img = Image.new(\"RGB\", (IMG_X, IMG_Y), \"black\")\n",
    "\n",
    "        # block_l and _w offset so blocks don't run off edge of image\n",
    "        rand_x = int(np.random.rand() * (IMG_X - 2 * block_l)) + block_l\n",
    "        rand_y = int(np.random.rand() * (IMG_Y - 2 * block_w)) + block_w\n",
    "        orient = int(np.random.rand() * 180)  # .random() is range [0.0, 1.0).\n",
    "        orient = math.radians(orient)  # math.cos takes radians!\n",
    "\n",
    "        # switch to degrees so that we normalize between the x,y,orient for MSELoss\n",
    "        true_coords.append(np.array((rand_x, rand_y, math.degrees(orient))))\n",
    "\n",
    "        rect_vertices = makeRectangle(\n",
    "            block_l, block_w, orient, offset=(rand_x, rand_y))\n",
    "\n",
    "        idraw = ImageDraw.Draw(img)\n",
    "        idraw.polygon(rect_vertices, fill=\"white\")\n",
    "\n",
    "        img.save(newpath + \"/rect\" + str(i) + \".png\")\n",
    "    return true_coords\n",
    "\n",
    "\n",
    "# NOTE Define size of dataset\n",
    "train_truth = make_dataset(\"data\", 5000)\n",
    "# print(len(train_truth))\n",
    "test_truth = make_dataset(\"./data/test\", 300)\n",
    "\n",
    "np.save(\"train_truth.npy\", train_truth)\n",
    "np.save(\"test_truth.npy\", test_truth)\n",
    "\n",
    "\n",
    "train_truth = np.load(\"train_truth.npy\")\n",
    "test_truth = np.load(\"test_truth.npy\")  # loading the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Dataloader -------------------------------------------------------\n",
    "class RectDepthImgsDataset(Dataset):\n",
    "    \"\"\"Artificially generated depth images dataset\"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, coords, transform=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.true_coords = coords\n",
    "        self.transform = transform\n",
    "\n",
    "        STEPSIZE = 50  # todo make this input arguments\n",
    "        WINDOWSIZE = (100, 100)\n",
    "        MARGIN_PX = 5 \n",
    "        self.step = STEPSIZE\n",
    "        self.cropSize = WINDOWSIZE\n",
    "        self.detectMargin = MARGIN_PX\n",
    "\n",
    "    def __len__(self):\n",
    "        # print('true coord len', len(self.true_coords))\n",
    "        return len(self.true_coords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # image = self.images[idx]\n",
    "        image = io.imread(self.img_dir + \"/rect\" +\n",
    "                          str(idx) + \".png\", as_gray=True)\n",
    "        # image = torch.FloatTensor(image).permute(2, 0, 1)  # PIL and torch expect difft orders\n",
    "        coords = torch.FloatTensor(self.true_coords[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        crops, labels, cropCoords = self.makeCrops(\n",
    "            image, self.step, self.cropSize, coords, self.detectMargin)\n",
    "\n",
    "        sample = image, torch.FloatTensor(\n",
    "            labels), torch.FloatTensor(cropCoords)\n",
    "        return sample\n",
    "\n",
    "    def makeCrops(self, image, stepSize, windowSize, rectCenter, detectMargin):\n",
    "        \"\"\"\n",
    "        Returns image crops, as well as T/F for those crops\n",
    "        \"\"\"\n",
    "        crops = []\n",
    "        c_x, c_y, theta = rectCenter\n",
    "        margin = detectMargin\n",
    "        hasRects = []\n",
    "        rectCoords = []\n",
    "        for y in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "            for x in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "                end_x, end_y = x + windowSize[1], y + windowSize[0]\n",
    "                hasRect = (x + margin < c_x < end_x - margin) and (\n",
    "                    y + margin < c_y < end_y - margin)\n",
    "                crops.append(image[y:end_y, x:end_x])\n",
    "                hasRects.append(hasRect)\n",
    "                if hasRect:\n",
    "                    rectCoords.append((c_x, c_y, theta))\n",
    "                else:\n",
    "                    # NOTE: Return empty label, when not hasRect\n",
    "                    rectCoords.append((0, 0, 0))\n",
    "        # print('length of truths in makeCrops', len(truths))\n",
    "        return crops, hasRects, rectCoords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class regrNet(nn.Module):\n",
    "    def __init__(self, cropSize, numOutputs):\n",
    "        \"\"\"\n",
    "        We need the image width and height to determine CNN layer sizes\n",
    "        \"\"\"\n",
    "        super(regrNet, self).__init__()\n",
    "        _pool = 2\n",
    "        _stride = 5\n",
    "        _outputlayers = 16\n",
    "\n",
    "        STEPSIZE = 50  # todo make this input arguments\n",
    "        WINDOWSIZE = (100, 100)\n",
    "        self.step = STEPSIZE\n",
    "        self.cropSize = cropSize\n",
    "        self.numOutputs = numOutputs\n",
    "\n",
    "        self.numCrops = 0\n",
    "        # calculate number of crops\n",
    "        for x in range(0, IMG_Y - WINDOWSIZE[0] + 1, STEPSIZE):\n",
    "            for y in range(0, IMG_X - WINDOWSIZE[1] + 1, STEPSIZE):\n",
    "                self.numCrops += 1\n",
    "\n",
    "        def _calc(val):  # use to calculate layer sizes\n",
    "            layer_size = (val - (_stride - 1)) / _pool\n",
    "            return layer_size\n",
    "\n",
    "        self._const = _calc(_calc(self.cropSize[0]))\n",
    "        self._const *= _calc(_calc(self.cropSize[1]))\n",
    "        self._const *= _outputlayers\n",
    "        self._const = int(self._const)\n",
    "\n",
    "        # --- LOCATION OF RECTANGLE\n",
    "        # NOTE: only one channel for now (black/white)\n",
    "        self.conv1 = nn.Conv2d(self.numCrops, 6, _stride).to(device)\n",
    "        self.pool = nn.MaxPool2d(_pool, _pool).to(device)\n",
    "        self.conv2 = nn.Conv2d(6, _outputlayers, _stride).to(device)\n",
    "        self.fc1 = nn.Linear(self._const, 120).to(device)\n",
    "        self.fc2 = nn.Linear(120, 84).to(device)\n",
    "        self.fc3 = nn.Linear(84, self.numOutputs * self.numCrops).to(device)\n",
    "\n",
    "    def forward(self, crops):\n",
    "        \"\"\"\n",
    "        Forward propogation\n",
    "        : param image: images, a tensor of dimensions(N, 3, IMG_X, IMG_Y)\n",
    "        : return: (x, y, theta) and T/F for each window\n",
    "        \"\"\"\n",
    "        # TODO: presumably by doing this i lose some of the multithread goodness\n",
    "        crops = crops.to(device)\n",
    "\n",
    "        # LOCALIZATION\n",
    "        regr_crops = self.pool(F.relu((self.conv1(crops))))\n",
    "        regr_crops = self.pool(F.relu(self.conv2(regr_crops)))\n",
    "        regr_crops = regr_crops.view(-1, self._const)\n",
    "        regr_crops = F.relu(self.fc1(regr_crops))\n",
    "        regr_crops = F.relu(self.fc2(regr_crops))\n",
    "        regr_crops = self.fc3(regr_crops)\n",
    "\n",
    "        objCoords = regr_crops\n",
    "        # reshape to batchsize x number of crops x 3\n",
    "        objCoords = objCoords.reshape(-1, self.numCrops, self.numOutputs)\n",
    "        return objCoords\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifNet(nn.Module):  # CIFAR is 32x32x3, MNIST is 28x28x1)\n",
    "    def __init__(self, IMG_X, IMG_Y):\n",
    "        \"\"\"\n",
    "        We need the image width and height to determine CNN layer sizes\n",
    "        \"\"\"\n",
    "        super(classifNet, self).__init__()\n",
    "        self._imgx = IMG_X\n",
    "        self._imgy = IMG_Y\n",
    "        _pool = 2\n",
    "        _stride = 5\n",
    "        _outputlayers = 16\n",
    "\n",
    "        STEPSIZE = 50  # todo make this input arguments\n",
    "        WINDOWSIZE = (100, 100)\n",
    "        self.step = STEPSIZE\n",
    "        self.cropSize = WINDOWSIZE\n",
    "        self.numCrops = 0\n",
    "        # T/F for now\n",
    "\n",
    "        # calculate number of crops\n",
    "        for x in range(0, IMG_Y - WINDOWSIZE[0] + 1, STEPSIZE):\n",
    "            for y in range(0, IMG_X - WINDOWSIZE[1] + 1, STEPSIZE):\n",
    "                self.numCrops += 1\n",
    "\n",
    "        def _calc(val):  # use to calculate layer sizes\n",
    "            layer_size = (val - (_stride - 1)) / _pool\n",
    "            return layer_size\n",
    "\n",
    "        # print(self._imgx)\n",
    "        # self._const = _calc(_calc(self._imgx))\n",
    "        # self._const *= _calc(_calc(self._imgy))\n",
    "        self._const = _calc(_calc(self.cropSize[0]))\n",
    "        self._const *= _calc(_calc(self.cropSize[1]))\n",
    "        self._const *= _outputlayers\n",
    "        self._const = int(self._const)\n",
    "\n",
    "        # --- CLASSIFICATION OF WINDOWS\n",
    "        # batch, 3 input image channels (RGB), 6 output channels, 5x5 square convolution\n",
    "        # NOTE: we switched to 1 input channel\n",
    "        self.conv1 = nn.Conv2d(self.numCrops, 6, _stride).to(device)\n",
    "        self.pool = nn.MaxPool2d(_pool, _pool).to(device)\n",
    "        self.conv2 = nn.Conv2d(6, _outputlayers, _stride).to(device)\n",
    "        self.fc1 = nn.Linear(self._const, 120).to(device)\n",
    "        self.fc2 = nn.Linear(120, 84).to(device)\n",
    "        self.fc3 = nn.Linear(84, self.numCrops).to(device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # TODO: batch normalization  self.bn = nn.BatchNorm2d()\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propogation\n",
    "        : param image: images, a tensor of dimensions(N, 3, IMG_X, IMG_Y)\n",
    "        : return: (x, y, theta) and T/F for each window\n",
    "        \"\"\"\n",
    "        x = x.to(device)\n",
    "        batch_images = x\n",
    "\n",
    "        all_crops = []\n",
    "        for img in batch_images:\n",
    "            crops = self.makeCrops(img, self.step, self.cropSize)\n",
    "            all_crops.append(crops)\n",
    "            # all_crops.append(crops)\n",
    "        all_crops = torch.stack(all_crops)\n",
    "        feats = all_crops.view(-1, self.numCrops, self.cropSize[0],\n",
    "                               self.cropSize[1]).to(device)\n",
    "\n",
    "        # CLASSIFICATION of the windows\n",
    "        c_crops = self.pool(F.relu((self.conv1(feats))))\n",
    "        c_crops = self.pool(F.relu(self.conv2(c_crops)))\n",
    "        c_crops = c_crops.view(-1, self._const)\n",
    "        c_crops = F.relu(self.fc1(c_crops))\n",
    "        c_crops = F.relu(self.fc2(c_crops))\n",
    "        c_crops = self.fc3(c_crops)\n",
    "        c_crops = self.sigmoid(c_crops)\n",
    "\n",
    "        containsObj = c_crops\n",
    "        return containsObj, all_crops\n",
    "    \n",
    "    def makeCrops(self, image, stepSize, windowSize):\n",
    "        \"\"\"\n",
    "        Returns a generator of cropped boxes(the top left x, y, the image data)\n",
    "        \"\"\"\n",
    "        image = image.type(torch.FloatTensor).to(device)\n",
    "        crops = []\n",
    "\n",
    "        # TODO: look into ordering, why it's y,x !\n",
    "        for y in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "            for x in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "                end_x, end_y = x + windowSize[1], y + windowSize[0]\n",
    "                # print('This is the x and y used: ', x, '; ', y)\n",
    "                crops.append(image[y:end_y, x:end_x])\n",
    "        crops = torch.stack(crops)\n",
    "        # self.numCrops=len(crops)\n",
    "        return crops\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Utility fxn -------------------------------------------------------\n",
    "# Source: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/train.py\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Keeps track of most recent, average, sum, and count of a metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "    epoch, epochs_since_improvement, model1, model2, optimizer1, optimizer2, loss, loss2, best_loss, is_best\n",
    "):\n",
    "    \"\"\"\n",
    "    Save model checkpoint.\n",
    "    :param epoch: epoch number\n",
    "    :param epochs_since_improvement: number of epochs since last improvement\n",
    "    :param model: model\n",
    "    :param optimizer: optimizer\n",
    "    :param loss: validation loss in this epoch\n",
    "    :param best_loss: best validation loss achieved so far (not necessarily in this checkpoint)\n",
    "    :param is_best: is this checkpoint the best so far?\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"epochs_since_improvement\": epochs_since_improvement,\n",
    "        \"loss\": loss,\n",
    "        \"loss2\": loss2,\n",
    "        \"best_loss\": best_loss,\n",
    "        \"model1\": model1,\n",
    "        \"model2\": model2,\n",
    "        \"optimizer1\": optimizer1,\n",
    "        \"optimizer2\": optimizer2,\n",
    "    }\n",
    "    filename = \"checkpoint_v2sliding.pth.tar\"\n",
    "    torch.save(state, filename)\n",
    "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
    "    if is_best:\n",
    "        torch.save(state, \"BEST_\" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(train_loader, c_model, r_model, classifCriterion,\n",
    "          regrCriterion, optimizer1, optimizer2, epoch):\n",
    "    \"\"\"\n",
    "    One epoch's training.\n",
    "    : param train_loader: DataLoader for training data\n",
    "    : param model: model\n",
    "    : param criterion: for classification (crop contains an Obj, t/f)\n",
    "    : param criterion: for regresion (of the x,y, theta)\n",
    "    : param optimizer: optimizer\n",
    "    : param epoch: epoch number\n",
    "    \"\"\"\n",
    "\n",
    "    c_model.train()  # training mode enables dropout\n",
    "    r_model.train()  # training mode enables dropout\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
    "    data_time = AverageMeter()  # data loading time\n",
    "    losses = AverageMeter()  # loss\n",
    "    losses2 = AverageMeter()  # loss\n",
    "    start = time.time()\n",
    "\n",
    "    for i_batch, (images, labels, coords) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        coords = coords.to(device)\n",
    "\n",
    "        # CLASSIFICATION\n",
    "        # Forward pass\n",
    "        # predicted_class, predicted_locs = model(images)\n",
    "        predicted_class, all_crops = c_model(images)\n",
    "\n",
    "        # print('size of predicted vs labels',\n",
    "        # predicted_class, labels)\n",
    "        # print('size of predicted vs labels',\n",
    "        # predicted_class.size(), labels.size())\n",
    "        loss1 = classifCriterion(predicted_class, labels)\n",
    "        optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "        # Update model\n",
    "        optimizer1.step()\n",
    "\n",
    "\n",
    "        # REGRESSION \n",
    "        # Forward pass\n",
    "        masked_truth = coords\n",
    "\n",
    "        predicted_coords = r_model(all_crops)\n",
    "\n",
    "        # #labels_est = copy.deepcopy(predicted_class)\n",
    "        # #coords_est = copy.deepcopy(predicted_coords)\n",
    "        # labels_est = torch.FloatTensor(predicted_class.detach().cpu().numpy())\n",
    "# # \n",
    "        # mask2 = torch.round(labels_est).type_as(coords)\n",
    "        # mask2.unsqueeze_(2)\n",
    "        # # print('!---- mask2 size', mask2.size(),\n",
    "        # # 'coords2 size', predicted_coords.size())\n",
    "        # mask2 = mask2.repeat(1, 1, 3)\n",
    "        # # print('!---- mask2 size', mask2.size(),\n",
    "        # # 'coords size', predicted_coords.size())\n",
    "        # #masked_est = mask2 * predicted_coords\n",
    "        masked_est = predicted_coords\n",
    "        # print('!---- masked est size', masked_est.size())\n",
    "\n",
    "        # mask = c.reshape(-1, c.size()[1], 1,\n",
    "        #                 1).repeat(1, 1, zee, zed)  # add a dims\n",
    "        # print('mask size', mask.size(), zee, zed)\n",
    "        # goodCrops = mask * crops\n",
    "\n",
    "        #mask_ = predicted_class.nonzero().type_as(all_crops)\n",
    "\n",
    "        # print('!---- truth labels', labels)\n",
    "        # print('!---- truth coords', coords)\n",
    "        # print('!---- masked truth coords', masked_truth)\n",
    "\n",
    "        # print('!---- predicted labels', predicted_class)\n",
    "        # print('!---- predicted coords', predicted_coords)\n",
    "        # print('!---- masked coords est', masked_est)\n",
    "        loss2 = regrCriterion(masked_est, masked_truth)\n",
    "        optimizer2.zero_grad()\n",
    "        loss2.backward()\n",
    "\n",
    "        optimizer2.step()\n",
    "\n",
    "        #losses.update(loss1.item() + loss2.item())\n",
    "        losses.update(loss1.item())\n",
    "        losses2.update(loss2.item())\n",
    "        batch_time.update(time.time() - start)\n",
    "        start = time.time()\n",
    "\n",
    "        # Print status\n",
    "        if i_batch % print_freq == 0:\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}]\\t\"\n",
    "                \"Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(\n",
    "                    epoch, i_batch, len(train_loader), batch_time=batch_time,\n",
    "                    loss=losses,\n",
    "                ),\n",
    "                \"RLoss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(\n",
    "                    epoch, i_batch, len(train_loader), batch_time=batch_time,\n",
    "                    loss=losses2,\n",
    "                ),\n",
    "                \n",
    "            )\n",
    "        # free some memory since their histories may be stored\n",
    "        del predicted_class, predicted_coords, images, labels, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, c_model, r_model, c_criterion, r_criterion):\n",
    "    \"\"\"\n",
    "    One epoch's validation.\n",
    "    : param val_loader: DataLoader for validation data\n",
    "    : param model: model\n",
    "    : param criterion: MultiBox loss\n",
    "    : return: average validation loss\n",
    "    \"\"\"\n",
    "    c_model.eval()  # eval mode disables dropout\n",
    "    r_model.eval()  # eval mode disables dropout\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses2 = AverageMeter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Prohibit gradient computation explicity because I had some problems with memory\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for i_batch, (images, labels, coords) in enumerate(train_loader):\n",
    "            # Move to default device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            coords = coords.to(device)\n",
    "\n",
    "            # CLASSIFICATION Eval\n",
    "            predicted_class, all_crops = c_model(images)\n",
    "            loss1 = c_criterion(predicted_class, labels)\n",
    "\n",
    "            # REGRESSION Eval\n",
    "            predicted_coords = r_model(all_crops)\n",
    "\n",
    "            # labels_est = torch.FloatTensor(\n",
    "                # predicted_class.detach().cpu().numpy())\n",
    "\n",
    "            # mask2 = torch.round(labels_est).type_as(coords)\n",
    "            # mask2.unsqueeze_(2)\n",
    "            # mask2 = mask2.repeat(1, 1, 3)\n",
    "\n",
    "            #masked_est = mask2 * predicted_coords\n",
    "            # For validation, dn't mask\n",
    "            masked_est = predicted_coords\n",
    "            masked_truth = coords\n",
    "\n",
    "            loss2 = r_criterion(masked_est, masked_truth)\n",
    "\n",
    "            #losses.update(loss1.item() + loss2.item(), images.size(0))\n",
    "            losses.update(loss1.item())\n",
    "            losses2.update(loss2.item())\n",
    "\n",
    "            batch_time.update(time.time() - start)\n",
    "            start = time.time()\n",
    "            \n",
    "            # Print status\n",
    "            if i_batch % print_freq == 0:\n",
    "                print(\n",
    "                    \"[{0}/{1}]\\t\"\n",
    "                    \"Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                    \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(i_batch, len(val_loader), \n",
    "                                                                    batch_time=batch_time, loss=losses),\n",
    "                    \"Regr Loss {loss.val:.4f} ({loss.avg:.4f})\\t\".format(i_batch, len(val_loader), \n",
    "                                                                    batch_time=batch_time, loss=losses2)\n",
    "                )\n",
    "\n",
    "    print(\"\\n * LOSS - {loss.avg:.3f}\\n\".format(loss=losses))\n",
    "    print(\" * REGR LOSS - {loss.avg:.3f}\\n\".format(loss=losses2))\n",
    "\n",
    "    return losses.avg, losses2.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available? device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# -- Load data -------------------------------------------------------\n",
    "batch_size = 32 \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA available? device: \", device)\n",
    "\n",
    "\n",
    "# Dataset is depth images of rectangular blocks\n",
    "train_dataset = RectDepthImgsDataset(img_dir=\"./data\", coords=train_truth)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = RectDepthImgsDataset(img_dir=\"./data/test\", coords=test_truth)\n",
    "\n",
    "# Data loader\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Hyperparamaters -------------------------\n",
    "\n",
    "num_epochs = 100  # number of epochs to run without early-stopping\n",
    "learning_rate = 0.001\n",
    "\n",
    "start_epoch = 0  # start at this epoch\n",
    "# number of epochs since there was an improvement in the validation metric\n",
    "epochs_since_improvement = 0\n",
    "best_loss = 1000.0  # assume a high loss at first\n",
    "workers = 4  # number of workers for loading data in the DataLoader\n",
    "\n",
    "classifModel = classifNet(IMG_X, IMG_Y)\n",
    "classifModel = classifModel.to(device)\n",
    "\n",
    "regrModel = regrNet((100, 100), 3)  # crop size in pixels; output x,y, theta\n",
    "regrModel = regrModel.to(device)\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "classifCriterion = nn.BCELoss()\n",
    "regrCriterion = nn.MSELoss()\n",
    "optimizer1 = torch.optim.Adam(classifModel.parameters(), lr=learning_rate)\n",
    "optimizer2 = torch.optim.Adam(regrModel.parameters(), lr=learning_rate)\n",
    "\n",
    "print_freq = 25  # print training or validation status every __ batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Training and validation.\n",
    "    \"\"\"\n",
    "\n",
    "    global epochs_since_improvement, start_epoch, best_loss, epoch, checkpoint\n",
    "\n",
    "    print(\"Training model now...\")\n",
    "\n",
    "    # -- Begin training -------------------------\n",
    "    for epoch in range(num_epochs):\n",
    "        train(\n",
    "            train_loader=train_loader,\n",
    "            c_model=classifModel,\n",
    "            r_model=regrModel,\n",
    "            classifCriterion=classifCriterion,\n",
    "            regrCriterion=regrCriterion,\n",
    "            optimizer1=optimizer1,\n",
    "            optimizer2=optimizer2,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "\n",
    "        # One epoch's validation\n",
    "        val_loss, regr_loss = validate(val_loader=test_loader,\n",
    "                            c_model=classifModel, r_model=regrModel,\n",
    "                            c_criterion=classifCriterion,\n",
    "                            r_criterion=regrCriterion)\n",
    "\n",
    "        # Did validation loss improve?\n",
    "        is_best = val_loss < best_loss\n",
    "        best_loss = min(val_loss, best_loss)\n",
    "\n",
    "        if not is_best:\n",
    "            epochs_since_improvement += 1\n",
    "            print(\"\\nEpochs since last improvement: %d\\n\" %\n",
    "                  (epochs_since_improvement,))\n",
    "\n",
    "        else:\n",
    "            epochs_since_improvement = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(epoch, epochs_since_improvement, classifModel, regrModel, optimizer1,\n",
    "                        optimizer2, val_loss, regr_loss, best_loss, is_best)\n",
    "        #save_checkpoint(epoch, epochs_since_improvement, classifModel, optimizer1,\n",
    "        #                val_loss, best_loss, is_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running main!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model now...\n",
      "Epoch: [0][0/157]\tBatch Time 0.063 (0.063)\tLoss 0.6992 (0.6992)\t RLoss 3567.8286 (3567.8286)\t\n",
      "Epoch: [0][25/157]\tBatch Time 0.055 (0.055)\tLoss 0.4002 (0.5389)\t RLoss 2274.6687 (2619.3861)\t\n",
      "Epoch: [0][50/157]\tBatch Time 0.055 (0.055)\tLoss 0.1260 (0.3830)\t RLoss 1883.5599 (2276.9129)\t\n",
      "Epoch: [0][75/157]\tBatch Time 0.054 (0.055)\tLoss 0.0618 (0.2867)\t RLoss 1471.4506 (2082.6436)\t\n",
      "Epoch: [0][100/157]\tBatch Time 0.054 (0.055)\tLoss 0.0865 (0.2276)\t RLoss 1036.5021 (1852.2799)\t\n",
      "Epoch: [0][125/157]\tBatch Time 0.054 (0.055)\tLoss 0.0458 (0.1913)\t RLoss 785.1042 (1658.3489)\t\n",
      "Epoch: [0][150/157]\tBatch Time 0.055 (0.055)\tLoss 0.0645 (0.1663)\t RLoss 652.7425 (1500.2009)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0312 (0.0312)\t Regr Loss 644.7047 (644.7047)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0122 (0.0281)\t Regr Loss 570.4675 (622.3002)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0636 (0.0339)\t Regr Loss 689.9706 (617.6048)\t\n",
      "[75/10]\tBatch Time 0.047 (0.047)\tLoss 0.0546 (0.0353)\t Regr Loss 567.3899 (620.3825)\t\n",
      "[100/10]\tBatch Time 0.047 (0.048)\tLoss 0.0260 (0.0356)\t Regr Loss 604.2870 (616.3911)\t\n",
      "[125/10]\tBatch Time 0.047 (0.048)\tLoss 0.0217 (0.0362)\t Regr Loss 544.7476 (614.7268)\t\n",
      "[150/10]\tBatch Time 0.047 (0.048)\tLoss 0.0488 (0.0366)\t Regr Loss 615.6316 (612.6974)\t\n",
      "\n",
      " * LOSS - 0.036\n",
      "\n",
      " * REGR LOSS - 610.451\n",
      "\n",
      "Epoch: [1][0/157]\tBatch Time 0.056 (0.056)\tLoss 0.0358 (0.0358)\t RLoss 696.1116 (696.1116)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rui/mlenv/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type classifNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/rui/mlenv/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type regrNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][25/157]\tBatch Time 0.054 (0.055)\tLoss 0.0160 (0.0338)\t RLoss 547.4088 (592.8495)\t\n",
      "Epoch: [1][50/157]\tBatch Time 0.054 (0.055)\tLoss 0.0130 (0.0324)\t RLoss 481.9526 (565.1698)\t\n",
      "Epoch: [1][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0481 (0.0322)\t RLoss 428.2050 (547.8615)\t\n",
      "Epoch: [1][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0167 (0.0308)\t RLoss 409.0266 (524.9853)\t\n",
      "Epoch: [1][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0180 (0.0311)\t RLoss 321.7834 (513.5438)\t\n",
      "Epoch: [1][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0089 (0.0305)\t RLoss 459.9914 (503.1465)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0386 (0.0386)\t Regr Loss 412.4494 (412.4494)\t\n",
      "[25/10]\tBatch Time 0.047 (0.047)\tLoss 0.0163 (0.0293)\t Regr Loss 385.5877 (439.2738)\t\n",
      "[50/10]\tBatch Time 0.047 (0.047)\tLoss 0.0375 (0.0293)\t Regr Loss 442.7816 (426.1478)\t\n",
      "[75/10]\tBatch Time 0.047 (0.047)\tLoss 0.0327 (0.0304)\t Regr Loss 435.2082 (428.2627)\t\n",
      "[100/10]\tBatch Time 0.048 (0.047)\tLoss 0.0119 (0.0304)\t Regr Loss 376.6405 (424.0240)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0557 (0.0315)\t Regr Loss 406.2512 (426.0502)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0391 (0.0319)\t Regr Loss 383.5497 (424.1289)\t\n",
      "\n",
      " * LOSS - 0.032\n",
      "\n",
      " * REGR LOSS - 423.618\n",
      "\n",
      "Epoch: [2][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0299 (0.0299)\t RLoss 401.8694 (401.8694)\t\n",
      "Epoch: [2][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0362 (0.0270)\t RLoss 400.0603 (424.0386)\t\n",
      "Epoch: [2][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0325 (0.0277)\t RLoss 401.0715 (422.4586)\t\n",
      "Epoch: [2][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0256 (0.0264)\t RLoss 538.1508 (435.0115)\t\n",
      "Epoch: [2][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0305 (0.0262)\t RLoss 378.7884 (431.8401)\t\n",
      "Epoch: [2][125/157]\tBatch Time 0.055 (0.054)\tLoss 0.0483 (0.0268)\t RLoss 487.7385 (426.7480)\t\n",
      "Epoch: [2][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0341 (0.0266)\t RLoss 454.9357 (419.5969)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0546 (0.0546)\t Regr Loss 277.2662 (277.2662)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0227 (0.0194)\t Regr Loss 380.4805 (379.6921)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0430 (0.0218)\t Regr Loss 384.0475 (381.2703)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0395 (0.0224)\t Regr Loss 424.1785 (379.0225)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0284 (0.0214)\t Regr Loss 331.5638 (378.9459)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0148 (0.0216)\t Regr Loss 322.4100 (385.2065)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0429 (0.0213)\t Regr Loss 559.9599 (386.0879)\t\n",
      "\n",
      " * LOSS - 0.022\n",
      "\n",
      " * REGR LOSS - 387.104\n",
      "\n",
      "Epoch: [3][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0263 (0.0263)\t RLoss 475.9561 (475.9561)\t\n",
      "Epoch: [3][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0593 (0.0249)\t RLoss 374.1417 (388.3427)\t\n",
      "Epoch: [3][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0152 (0.0238)\t RLoss 273.4207 (377.7321)\t\n",
      "Epoch: [3][75/157]\tBatch Time 0.055 (0.054)\tLoss 0.0097 (0.0227)\t RLoss 341.8206 (379.6342)\t\n",
      "Epoch: [3][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0301 (0.0223)\t RLoss 421.1073 (376.1514)\t\n",
      "Epoch: [3][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0210 (0.0216)\t RLoss 349.9901 (378.7060)\t\n",
      "Epoch: [3][150/157]\tBatch Time 0.055 (0.054)\tLoss 0.0627 (0.0225)\t RLoss 366.5084 (378.1231)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0150 (0.0150)\t Regr Loss 329.6848 (329.6848)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0185 (0.0184)\t Regr Loss 377.9243 (361.5106)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0163 (0.0216)\t Regr Loss 317.8765 (366.7696)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0073 (0.0217)\t Regr Loss 326.8650 (358.6523)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0148 (0.0214)\t Regr Loss 365.7151 (359.3476)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0233 (0.0210)\t Regr Loss 306.8461 (356.4220)\t\n",
      "[150/10]\tBatch Time 0.047 (0.048)\tLoss 0.0143 (0.0207)\t Regr Loss 378.3353 (356.1492)\t\n",
      "\n",
      " * LOSS - 0.021\n",
      "\n",
      " * REGR LOSS - 354.884\n",
      "\n",
      "Epoch: [4][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0251 (0.0251)\t RLoss 360.5580 (360.5580)\t\n",
      "Epoch: [4][25/157]\tBatch Time 0.054 (0.055)\tLoss 0.0298 (0.0176)\t RLoss 402.5209 (363.5581)\t\n",
      "Epoch: [4][50/157]\tBatch Time 0.055 (0.055)\tLoss 0.0145 (0.0168)\t RLoss 285.8842 (363.2424)\t\n",
      "Epoch: [4][75/157]\tBatch Time 0.054 (0.055)\tLoss 0.0116 (0.0173)\t RLoss 288.1632 (361.0417)\t\n",
      "Epoch: [4][100/157]\tBatch Time 0.054 (0.055)\tLoss 0.0185 (0.0181)\t RLoss 288.3995 (362.3665)\t\n",
      "Epoch: [4][125/157]\tBatch Time 0.055 (0.054)\tLoss 0.0236 (0.0187)\t RLoss 425.5744 (362.5442)\t\n",
      "Epoch: [4][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0026 (0.0192)\t RLoss 271.9360 (357.7898)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0122 (0.0122)\t Regr Loss 423.2927 (423.2927)\t\n",
      "[25/10]\tBatch Time 0.047 (0.048)\tLoss 0.0080 (0.0167)\t Regr Loss 307.0566 (384.4915)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0011 (0.0160)\t Regr Loss 425.6656 (380.7829)\t\n",
      "[75/10]\tBatch Time 0.047 (0.048)\tLoss 0.0046 (0.0153)\t Regr Loss 436.6432 (380.3439)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0233 (0.0156)\t Regr Loss 370.4944 (378.8082)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0116 (0.0159)\t Regr Loss 412.0027 (377.8783)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0024 (0.0158)\t Regr Loss 351.0678 (378.6559)\t\n",
      "\n",
      " * LOSS - 0.016\n",
      "\n",
      " * REGR LOSS - 376.839\n",
      "\n",
      "Epoch: [5][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0090 (0.0090)\t RLoss 399.4913 (399.4913)\t\n",
      "Epoch: [5][25/157]\tBatch Time 0.054 (0.055)\tLoss 0.0132 (0.0197)\t RLoss 299.6639 (369.4191)\t\n",
      "Epoch: [5][50/157]\tBatch Time 0.054 (0.055)\tLoss 0.0084 (0.0177)\t RLoss 328.0059 (354.8273)\t\n",
      "Epoch: [5][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0183 (0.0195)\t RLoss 352.5542 (357.4880)\t\n",
      "Epoch: [5][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0068 (0.0190)\t RLoss 363.1261 (353.1916)\t\n",
      "Epoch: [5][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0234 (0.0183)\t RLoss 407.4863 (350.4647)\t\n",
      "Epoch: [5][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0134 (0.0177)\t RLoss 304.8943 (346.8245)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0120 (0.0120)\t Regr Loss 392.4836 (392.4836)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0116 (0.0087)\t Regr Loss 299.3574 (335.7161)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0050 (0.0103)\t Regr Loss 224.4900 (325.2043)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0011 (0.0114)\t Regr Loss 381.6035 (323.2388)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0051 (0.0114)\t Regr Loss 368.4551 (324.9663)\t\n",
      "[125/10]\tBatch Time 0.047 (0.048)\tLoss 0.0131 (0.0118)\t Regr Loss 331.1014 (326.6992)\t\n",
      "[150/10]\tBatch Time 0.047 (0.048)\tLoss 0.0069 (0.0118)\t Regr Loss 273.2652 (326.3744)\t\n",
      "\n",
      " * LOSS - 0.012\n",
      "\n",
      " * REGR LOSS - 326.565\n",
      "\n",
      "Epoch: [6][0/157]\tBatch Time 0.056 (0.056)\tLoss 0.0088 (0.0088)\t RLoss 314.8370 (314.8370)\t\n",
      "Epoch: [6][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0137 (0.0113)\t RLoss 243.6098 (339.4297)\t\n",
      "Epoch: [6][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0129 (0.0134)\t RLoss 389.4535 (351.4905)\t\n",
      "Epoch: [6][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0170 (0.0146)\t RLoss 412.7180 (348.7884)\t\n",
      "Epoch: [6][100/157]\tBatch Time 0.055 (0.054)\tLoss 0.0205 (0.0150)\t RLoss 332.5000 (347.5619)\t\n",
      "Epoch: [6][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0335 (0.0145)\t RLoss 258.0486 (339.3678)\t\n",
      "Epoch: [6][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0278 (0.0150)\t RLoss 357.2034 (335.3651)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0267 (0.0267)\t Regr Loss 273.0985 (273.0985)\t\n",
      "[25/10]\tBatch Time 0.047 (0.048)\tLoss 0.0032 (0.0153)\t Regr Loss 248.4280 (307.8025)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0454 (0.0179)\t Regr Loss 330.7696 (315.0353)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0227 (0.0186)\t Regr Loss 293.9907 (314.2485)\t\n",
      "[100/10]\tBatch Time 0.047 (0.048)\tLoss 0.0133 (0.0184)\t Regr Loss 276.1666 (310.1773)\t\n",
      "[125/10]\tBatch Time 0.047 (0.048)\tLoss 0.0066 (0.0182)\t Regr Loss 326.7328 (310.7528)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0249 (0.0185)\t Regr Loss 424.2475 (313.2713)\t\n",
      "\n",
      " * LOSS - 0.019\n",
      "\n",
      " * REGR LOSS - 314.508\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [7][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0152 (0.0152)\t RLoss 292.3154 (292.3154)\t\n",
      "Epoch: [7][25/157]\tBatch Time 0.055 (0.055)\tLoss 0.0097 (0.0175)\t RLoss 477.9622 (331.8724)\t\n",
      "Epoch: [7][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0097 (0.0160)\t RLoss 285.3161 (330.6250)\t\n",
      "Epoch: [7][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0187 (0.0152)\t RLoss 325.3424 (333.4197)\t\n",
      "Epoch: [7][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0082 (0.0152)\t RLoss 383.2233 (329.9516)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][125/157]\tBatch Time 0.055 (0.054)\tLoss 0.0259 (0.0152)\t RLoss 365.7886 (327.5938)\t\n",
      "Epoch: [7][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0356 (0.0150)\t RLoss 317.9741 (323.6972)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0228 (0.0228)\t Regr Loss 325.0191 (325.0191)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0102 (0.0122)\t Regr Loss 352.2675 (297.3038)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0114 (0.0122)\t Regr Loss 343.5595 (304.9825)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0057 (0.0128)\t Regr Loss 419.4384 (313.3043)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0030 (0.0119)\t Regr Loss 271.2159 (312.5001)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0062 (0.0120)\t Regr Loss 328.0417 (310.0311)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0087 (0.0121)\t Regr Loss 326.1755 (311.3367)\t\n",
      "\n",
      " * LOSS - 0.012\n",
      "\n",
      " * REGR LOSS - 311.242\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [8][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0248 (0.0248)\t RLoss 313.8580 (313.8580)\t\n",
      "Epoch: [8][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0256 (0.0148)\t RLoss 402.8246 (295.0146)\t\n",
      "Epoch: [8][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0488 (0.0148)\t RLoss 372.9812 (304.7873)\t\n",
      "Epoch: [8][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0091 (0.0146)\t RLoss 298.8243 (313.1853)\t\n",
      "Epoch: [8][100/157]\tBatch Time 0.055 (0.054)\tLoss 0.0420 (0.0143)\t RLoss 309.3490 (315.0128)\t\n",
      "Epoch: [8][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0127 (0.0147)\t RLoss 261.1026 (316.1952)\t\n",
      "Epoch: [8][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0236 (0.0147)\t RLoss 356.4735 (315.4671)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0157 (0.0157)\t Regr Loss 385.7567 (385.7567)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0432 (0.0113)\t Regr Loss 315.7592 (292.9049)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0059 (0.0124)\t Regr Loss 304.3474 (293.4428)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0127 (0.0123)\t Regr Loss 325.9417 (297.9281)\t\n",
      "[100/10]\tBatch Time 0.047 (0.048)\tLoss 0.0166 (0.0117)\t Regr Loss 259.0349 (292.8437)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0006 (0.0113)\t Regr Loss 261.6674 (292.7611)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0110 (0.0114)\t Regr Loss 285.4468 (295.2218)\t\n",
      "\n",
      " * LOSS - 0.011\n",
      "\n",
      " * REGR LOSS - 295.266\n",
      "\n",
      "Epoch: [9][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0143 (0.0143)\t RLoss 265.1162 (265.1162)\t\n",
      "Epoch: [9][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0475 (0.0220)\t RLoss 485.8090 (299.9960)\t\n",
      "Epoch: [9][50/157]\tBatch Time 0.055 (0.055)\tLoss 0.0105 (0.0182)\t RLoss 236.5325 (288.0105)\t\n",
      "Epoch: [9][75/157]\tBatch Time 0.054 (0.055)\tLoss 0.0031 (0.0166)\t RLoss 206.5103 (293.1379)\t\n",
      "Epoch: [9][100/157]\tBatch Time 0.056 (0.055)\tLoss 0.0201 (0.0151)\t RLoss 318.7428 (293.4588)\t\n",
      "Epoch: [9][125/157]\tBatch Time 0.054 (0.055)\tLoss 0.0031 (0.0157)\t RLoss 392.9472 (294.4792)\t\n",
      "Epoch: [9][150/157]\tBatch Time 0.055 (0.055)\tLoss 0.0066 (0.0155)\t RLoss 329.4670 (295.7003)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0046 (0.0046)\t Regr Loss 247.2101 (247.2101)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0054 (0.0144)\t Regr Loss 290.0146 (278.1061)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0326 (0.0154)\t Regr Loss 251.4438 (280.9435)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0343 (0.0166)\t Regr Loss 261.5974 (281.4748)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0067 (0.0164)\t Regr Loss 311.9869 (285.8873)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0132 (0.0161)\t Regr Loss 269.3796 (283.4969)\t\n",
      "[150/10]\tBatch Time 0.047 (0.048)\tLoss 0.0027 (0.0159)\t Regr Loss 248.0055 (282.4084)\t\n",
      "\n",
      " * LOSS - 0.016\n",
      "\n",
      " * REGR LOSS - 282.343\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [10][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0140 (0.0140)\t RLoss 310.6115 (310.6115)\t\n",
      "Epoch: [10][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0025 (0.0128)\t RLoss 200.6898 (276.4851)\t\n",
      "Epoch: [10][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0095 (0.0140)\t RLoss 299.9027 (279.9023)\t\n",
      "Epoch: [10][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0072 (0.0130)\t RLoss 351.1516 (281.0403)\t\n",
      "Epoch: [10][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0103 (0.0118)\t RLoss 316.7369 (278.0365)\t\n",
      "Epoch: [10][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0223 (0.0123)\t RLoss 351.1504 (278.0401)\t\n",
      "Epoch: [10][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0147 (0.0131)\t RLoss 273.5786 (277.0638)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0114 (0.0114)\t Regr Loss 263.6053 (263.6053)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0068 (0.0135)\t Regr Loss 276.6281 (255.9030)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0159 (0.0128)\t Regr Loss 215.0359 (247.7026)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0056 (0.0128)\t Regr Loss 230.6311 (246.8944)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0075 (0.0121)\t Regr Loss 212.0067 (245.6315)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0014 (0.0128)\t Regr Loss 185.9952 (248.4005)\t\n",
      "[150/10]\tBatch Time 0.047 (0.048)\tLoss 0.0035 (0.0124)\t Regr Loss 279.9751 (247.3662)\t\n",
      "\n",
      " * LOSS - 0.013\n",
      "\n",
      " * REGR LOSS - 248.280\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [11][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0122 (0.0122)\t RLoss 200.1378 (200.1378)\t\n",
      "Epoch: [11][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0017 (0.0099)\t RLoss 275.8479 (251.6522)\t\n",
      "Epoch: [11][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0088 (0.0088)\t RLoss 303.1819 (255.9087)\t\n",
      "Epoch: [11][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0003 (0.0093)\t RLoss 234.0321 (256.7559)\t\n",
      "Epoch: [11][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0287 (0.0090)\t RLoss 194.9915 (254.7182)\t\n",
      "Epoch: [11][125/157]\tBatch Time 0.055 (0.054)\tLoss 0.0188 (0.0094)\t RLoss 303.7498 (254.4247)\t\n",
      "Epoch: [11][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0011 (0.0095)\t RLoss 225.9798 (252.9568)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0058 (0.0058)\t Regr Loss 290.2338 (290.2338)\t\n",
      "[25/10]\tBatch Time 0.047 (0.047)\tLoss 0.0070 (0.0072)\t Regr Loss 214.4707 (227.9634)\t\n",
      "[50/10]\tBatch Time 0.047 (0.047)\tLoss 0.0201 (0.0071)\t Regr Loss 232.0388 (230.2131)\t\n",
      "[75/10]\tBatch Time 0.048 (0.047)\tLoss 0.0011 (0.0080)\t Regr Loss 177.6379 (229.2324)\t\n",
      "[100/10]\tBatch Time 0.047 (0.047)\tLoss 0.0129 (0.0091)\t Regr Loss 236.4420 (230.4726)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0137 (0.0091)\t Regr Loss 224.4331 (229.2006)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0135 (0.0098)\t Regr Loss 178.2315 (231.5697)\t\n",
      "\n",
      " * LOSS - 0.010\n",
      "\n",
      " * REGR LOSS - 231.291\n",
      "\n",
      "Epoch: [12][0/157]\tBatch Time 0.056 (0.056)\tLoss 0.0044 (0.0044)\t RLoss 200.2316 (200.2316)\t\n",
      "Epoch: [12][25/157]\tBatch Time 0.055 (0.054)\tLoss 0.0047 (0.0103)\t RLoss 269.1636 (244.1774)\t\n",
      "Epoch: [12][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0008 (0.0125)\t RLoss 230.3047 (239.1137)\t\n",
      "Epoch: [12][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0046 (0.0119)\t RLoss 238.0222 (238.3411)\t\n",
      "Epoch: [12][100/157]\tBatch Time 0.055 (0.054)\tLoss 0.0145 (0.0112)\t RLoss 275.5597 (234.4641)\t\n",
      "Epoch: [12][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0062 (0.0111)\t RLoss 324.9516 (234.4901)\t\n",
      "Epoch: [12][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0196 (0.0121)\t RLoss 283.9573 (235.3238)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0043 (0.0043)\t Regr Loss 214.0167 (214.0167)\t\n",
      "[25/10]\tBatch Time 0.047 (0.048)\tLoss 0.0018 (0.0096)\t Regr Loss 212.8980 (223.2020)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0073 (0.0084)\t Regr Loss 138.8876 (215.3140)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0191 (0.0086)\t Regr Loss 262.6898 (218.3998)\t\n",
      "[100/10]\tBatch Time 0.047 (0.048)\tLoss 0.0115 (0.0084)\t Regr Loss 198.6013 (216.4556)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0180 (0.0083)\t Regr Loss 179.8704 (216.5325)\t\n",
      "[150/10]\tBatch Time 0.047 (0.048)\tLoss 0.0206 (0.0087)\t Regr Loss 193.2260 (217.1071)\t\n",
      "\n",
      " * LOSS - 0.009\n",
      "\n",
      " * REGR LOSS - 217.569\n",
      "\n",
      "Epoch: [13][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0031 (0.0031)\t RLoss 236.2390 (236.2390)\t\n",
      "Epoch: [13][25/157]\tBatch Time 0.055 (0.054)\tLoss 0.0014 (0.0088)\t RLoss 211.8921 (217.4368)\t\n",
      "Epoch: [13][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0059 (0.0084)\t RLoss 212.3159 (214.3896)\t\n",
      "Epoch: [13][75/157]\tBatch Time 0.055 (0.054)\tLoss 0.0021 (0.0087)\t RLoss 210.5024 (212.8687)\t\n",
      "Epoch: [13][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0044 (0.0084)\t RLoss 164.8853 (211.9365)\t\n",
      "Epoch: [13][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0093 (0.0080)\t RLoss 262.1409 (213.6871)\t\n",
      "Epoch: [13][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0166 (0.0077)\t RLoss 182.7690 (214.2327)\t\n",
      "[0/10]\tBatch Time 0.047 (0.047)\tLoss 0.0063 (0.0063)\t Regr Loss 244.6597 (244.6597)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25/10]\tBatch Time 0.047 (0.047)\tLoss 0.0077 (0.0054)\t Regr Loss 164.6780 (204.3892)\t\n",
      "[50/10]\tBatch Time 0.047 (0.047)\tLoss 0.0011 (0.0062)\t Regr Loss 171.5393 (196.6101)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0050 (0.0064)\t Regr Loss 159.9202 (199.9059)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0038 (0.0068)\t Regr Loss 182.5118 (202.1897)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0011 (0.0068)\t Regr Loss 166.9960 (203.1330)\t\n",
      "[150/10]\tBatch Time 0.047 (0.048)\tLoss 0.0002 (0.0069)\t Regr Loss 237.7198 (200.9602)\t\n",
      "\n",
      " * LOSS - 0.007\n",
      "\n",
      " * REGR LOSS - 201.282\n",
      "\n",
      "Epoch: [14][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0004 (0.0004)\t RLoss 200.8281 (200.8281)\t\n",
      "Epoch: [14][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0011 (0.0085)\t RLoss 165.1982 (199.0095)\t\n",
      "Epoch: [14][50/157]\tBatch Time 0.055 (0.054)\tLoss 0.0006 (0.0091)\t RLoss 217.5284 (200.1186)\t\n",
      "Epoch: [14][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0028 (0.0084)\t RLoss 174.6395 (200.2225)\t\n",
      "Epoch: [14][100/157]\tBatch Time 0.055 (0.054)\tLoss 0.0141 (0.0082)\t RLoss 234.5379 (198.3629)\t\n",
      "Epoch: [14][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0060 (0.0082)\t RLoss 174.9520 (200.5882)\t\n",
      "Epoch: [14][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0072 (0.0079)\t RLoss 127.3113 (201.5979)\t\n",
      "[0/10]\tBatch Time 0.047 (0.047)\tLoss 0.0068 (0.0068)\t Regr Loss 154.7927 (154.7927)\t\n",
      "[25/10]\tBatch Time 0.047 (0.047)\tLoss 0.0109 (0.0079)\t Regr Loss 191.0563 (197.4482)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0160 (0.0086)\t Regr Loss 169.9819 (196.4390)\t\n",
      "[75/10]\tBatch Time 0.048 (0.047)\tLoss 0.0012 (0.0087)\t Regr Loss 184.6010 (198.2611)\t\n",
      "[100/10]\tBatch Time 0.047 (0.047)\tLoss 0.0015 (0.0084)\t Regr Loss 205.9532 (197.3658)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0142 (0.0078)\t Regr Loss 221.3935 (196.5745)\t\n",
      "[150/10]\tBatch Time 0.049 (0.047)\tLoss 0.0029 (0.0072)\t Regr Loss 187.5917 (195.4528)\t\n",
      "\n",
      " * LOSS - 0.007\n",
      "\n",
      " * REGR LOSS - 195.939\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [15][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0148 (0.0148)\t RLoss 239.5363 (239.5363)\t\n",
      "Epoch: [15][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0281 (0.0059)\t RLoss 168.8338 (198.7219)\t\n",
      "Epoch: [15][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0027 (0.0081)\t RLoss 185.4576 (193.0133)\t\n",
      "Epoch: [15][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0312 (0.0089)\t RLoss 159.9565 (193.5922)\t\n",
      "Epoch: [15][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0005 (0.0090)\t RLoss 136.5829 (190.8032)\t\n",
      "Epoch: [15][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0074 (0.0088)\t RLoss 155.6821 (191.9186)\t\n",
      "Epoch: [15][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0060 (0.0091)\t RLoss 152.1595 (191.2186)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0033 (0.0033)\t Regr Loss 182.9640 (182.9640)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0189 (0.0065)\t Regr Loss 262.1998 (179.1171)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0002 (0.0068)\t Regr Loss 194.2733 (181.5403)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0123 (0.0074)\t Regr Loss 233.4885 (183.7670)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0045 (0.0072)\t Regr Loss 169.3348 (186.8056)\t\n",
      "[125/10]\tBatch Time 0.047 (0.048)\tLoss 0.0167 (0.0082)\t Regr Loss 239.6964 (185.8498)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0020 (0.0082)\t Regr Loss 150.5495 (183.8596)\t\n",
      "\n",
      " * LOSS - 0.008\n",
      "\n",
      " * REGR LOSS - 183.214\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [16][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0064 (0.0064)\t RLoss 156.6109 (156.6109)\t\n",
      "Epoch: [16][25/157]\tBatch Time 0.055 (0.054)\tLoss 0.0107 (0.0089)\t RLoss 197.9807 (176.7318)\t\n",
      "Epoch: [16][50/157]\tBatch Time 0.055 (0.054)\tLoss 0.0085 (0.0116)\t RLoss 148.0884 (182.6796)\t\n",
      "Epoch: [16][75/157]\tBatch Time 0.054 (0.055)\tLoss 0.0011 (0.0102)\t RLoss 146.3086 (180.7392)\t\n",
      "Epoch: [16][100/157]\tBatch Time 0.055 (0.055)\tLoss 0.0069 (0.0094)\t RLoss 184.7086 (183.5446)\t\n",
      "Epoch: [16][125/157]\tBatch Time 0.054 (0.055)\tLoss 0.0061 (0.0093)\t RLoss 198.0610 (183.3831)\t\n",
      "Epoch: [16][150/157]\tBatch Time 0.056 (0.055)\tLoss 0.0049 (0.0094)\t RLoss 164.4159 (181.1106)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0015 (0.0015)\t Regr Loss 142.2523 (142.2523)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0015 (0.0076)\t Regr Loss 153.8100 (167.6203)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0029 (0.0090)\t Regr Loss 118.1352 (170.3562)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0137 (0.0085)\t Regr Loss 123.5748 (169.1153)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0016 (0.0088)\t Regr Loss 154.0649 (168.4868)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0005 (0.0095)\t Regr Loss 194.8406 (169.9415)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0017 (0.0091)\t Regr Loss 123.1311 (167.6788)\t\n",
      "\n",
      " * LOSS - 0.009\n",
      "\n",
      " * REGR LOSS - 166.902\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [17][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0110 (0.0110)\t RLoss 263.4208 (263.4208)\t\n",
      "Epoch: [17][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0015 (0.0087)\t RLoss 164.2680 (168.0985)\t\n",
      "Epoch: [17][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0112 (0.0084)\t RLoss 188.5911 (165.3142)\t\n",
      "Epoch: [17][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0005 (0.0077)\t RLoss 146.4380 (164.4627)\t\n",
      "Epoch: [17][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0056 (0.0068)\t RLoss 184.0015 (166.1152)\t\n",
      "Epoch: [17][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0049 (0.0070)\t RLoss 110.5443 (167.4511)\t\n",
      "Epoch: [17][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0024 (0.0073)\t RLoss 248.5399 (169.4174)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0002 (0.0002)\t Regr Loss 157.9539 (157.9539)\t\n",
      "[25/10]\tBatch Time 0.047 (0.048)\tLoss 0.0018 (0.0067)\t Regr Loss 198.5629 (161.0317)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0010 (0.0053)\t Regr Loss 146.8290 (160.0694)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0087 (0.0055)\t Regr Loss 167.9502 (162.7123)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0028 (0.0063)\t Regr Loss 225.1328 (162.4252)\t\n",
      "[125/10]\tBatch Time 0.047 (0.049)\tLoss 0.0020 (0.0059)\t Regr Loss 212.5928 (162.5661)\t\n",
      "[150/10]\tBatch Time 0.047 (0.049)\tLoss 0.0054 (0.0061)\t Regr Loss 160.4293 (161.2700)\t\n",
      "\n",
      " * LOSS - 0.006\n",
      "\n",
      " * REGR LOSS - 161.178\n",
      "\n",
      "Epoch: [18][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0012 (0.0012)\t RLoss 220.8624 (220.8624)\t\n",
      "Epoch: [18][25/157]\tBatch Time 0.054 (0.055)\tLoss 0.0359 (0.0075)\t RLoss 161.5367 (159.2621)\t\n",
      "Epoch: [18][50/157]\tBatch Time 0.054 (0.055)\tLoss 0.0030 (0.0078)\t RLoss 222.5755 (160.1106)\t\n",
      "Epoch: [18][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0025 (0.0087)\t RLoss 129.2405 (162.6286)\t\n",
      "Epoch: [18][100/157]\tBatch Time 0.055 (0.054)\tLoss 0.0092 (0.0087)\t RLoss 219.3458 (161.6867)\t\n",
      "Epoch: [18][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0026 (0.0089)\t RLoss 211.5281 (163.6272)\t\n",
      "Epoch: [18][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0113 (0.0082)\t RLoss 153.6710 (162.4265)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0207 (0.0207)\t Regr Loss 128.3228 (128.3228)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0181 (0.0058)\t Regr Loss 188.0812 (149.1147)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0117 (0.0061)\t Regr Loss 153.8071 (146.5567)\t\n",
      "[75/10]\tBatch Time 0.047 (0.048)\tLoss 0.0276 (0.0064)\t Regr Loss 162.3199 (146.9851)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0035 (0.0068)\t Regr Loss 177.4706 (146.1768)\t\n",
      "[125/10]\tBatch Time 0.047 (0.048)\tLoss 0.0058 (0.0073)\t Regr Loss 129.9436 (145.7498)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0048 (0.0065)\t Regr Loss 182.7474 (143.6567)\t\n",
      "\n",
      " * LOSS - 0.006\n",
      "\n",
      " * REGR LOSS - 143.468\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [19][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0187 (0.0187)\t RLoss 144.0837 (144.0837)\t\n",
      "Epoch: [19][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0007 (0.0057)\t RLoss 127.3411 (157.1987)\t\n",
      "Epoch: [19][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0044 (0.0055)\t RLoss 148.0314 (155.3677)\t\n",
      "Epoch: [19][75/157]\tBatch Time 0.055 (0.054)\tLoss 0.0003 (0.0049)\t RLoss 144.4779 (153.5405)\t\n",
      "Epoch: [19][100/157]\tBatch Time 0.055 (0.054)\tLoss 0.0085 (0.0054)\t RLoss 152.8805 (150.2868)\t\n",
      "Epoch: [19][125/157]\tBatch Time 0.055 (0.054)\tLoss 0.0015 (0.0056)\t RLoss 150.7009 (151.0988)\t\n",
      "Epoch: [19][150/157]\tBatch Time 0.055 (0.054)\tLoss 0.0135 (0.0057)\t RLoss 177.3880 (151.1905)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0063 (0.0063)\t Regr Loss 177.4085 (177.4085)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0004 (0.0054)\t Regr Loss 98.4145 (151.6030)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0021 (0.0051)\t Regr Loss 164.6244 (151.3815)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0163 (0.0048)\t Regr Loss 124.3882 (146.7645)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0017 (0.0051)\t Regr Loss 184.1787 (144.7618)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0015 (0.0051)\t Regr Loss 159.7530 (143.3430)\t\n",
      "[150/10]\tBatch Time 0.047 (0.048)\tLoss 0.0225 (0.0051)\t Regr Loss 148.9794 (143.3744)\t\n",
      "\n",
      " * LOSS - 0.005\n",
      "\n",
      " * REGR LOSS - 142.794\n",
      "\n",
      "Epoch: [20][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0017 (0.0017)\t RLoss 115.0641 (115.0641)\t\n",
      "Epoch: [20][25/157]\tBatch Time 0.055 (0.055)\tLoss 0.0003 (0.0071)\t RLoss 112.9240 (144.9157)\t\n",
      "Epoch: [20][50/157]\tBatch Time 0.054 (0.055)\tLoss 0.0134 (0.0083)\t RLoss 162.9244 (140.4812)\t\n",
      "Epoch: [20][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0071 (0.0074)\t RLoss 142.2049 (144.4815)\t\n",
      "Epoch: [20][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0023 (0.0069)\t RLoss 117.3763 (141.1972)\t\n",
      "Epoch: [20][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0050 (0.0065)\t RLoss 146.7436 (142.8429)\t\n",
      "Epoch: [20][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0023 (0.0063)\t RLoss 143.2673 (143.1954)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0121 (0.0121)\t Regr Loss 130.1981 (130.1981)\t\n",
      "[25/10]\tBatch Time 0.047 (0.048)\tLoss 0.0146 (0.0042)\t Regr Loss 151.5264 (131.6288)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0014 (0.0033)\t Regr Loss 102.2264 (127.5163)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0033 (0.0030)\t Regr Loss 153.9651 (128.5272)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0106 (0.0032)\t Regr Loss 163.7313 (129.1820)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0003 (0.0033)\t Regr Loss 145.0427 (128.3677)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0005 (0.0033)\t Regr Loss 184.0844 (128.9128)\t\n",
      "\n",
      " * LOSS - 0.003\n",
      "\n",
      " * REGR LOSS - 129.520\n",
      "\n",
      "Epoch: [21][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0061 (0.0061)\t RLoss 129.8438 (129.8438)\t\n",
      "Epoch: [21][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0004 (0.0042)\t RLoss 91.9135 (130.1125)\t\n",
      "Epoch: [21][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0009 (0.0045)\t RLoss 120.4119 (134.1488)\t\n",
      "Epoch: [21][75/157]\tBatch Time 0.055 (0.054)\tLoss 0.0055 (0.0053)\t RLoss 160.9154 (132.3027)\t\n",
      "Epoch: [21][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0006 (0.0066)\t RLoss 91.3757 (135.3801)\t\n",
      "Epoch: [21][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0013 (0.0061)\t RLoss 130.8984 (135.5371)\t\n",
      "Epoch: [21][150/157]\tBatch Time 0.055 (0.054)\tLoss 0.0042 (0.0059)\t RLoss 141.0026 (136.1271)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0019 (0.0019)\t Regr Loss 119.1373 (119.1373)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0004 (0.0035)\t Regr Loss 131.8687 (130.5573)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0035)\t Regr Loss 86.1233 (126.7191)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0084 (0.0037)\t Regr Loss 139.3870 (128.4580)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0007 (0.0039)\t Regr Loss 134.9472 (129.8821)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0017 (0.0035)\t Regr Loss 113.0582 (130.7153)\t\n",
      "[150/10]\tBatch Time 0.047 (0.048)\tLoss 0.0067 (0.0033)\t Regr Loss 120.4877 (130.9312)\t\n",
      "\n",
      " * LOSS - 0.003\n",
      "\n",
      " * REGR LOSS - 131.126\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [22][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0106 (0.0106)\t RLoss 119.5006 (119.5006)\t\n",
      "Epoch: [22][25/157]\tBatch Time 0.054 (0.055)\tLoss 0.0070 (0.0054)\t RLoss 160.2464 (141.6654)\t\n",
      "Epoch: [22][50/157]\tBatch Time 0.055 (0.055)\tLoss 0.0010 (0.0053)\t RLoss 129.8994 (139.7270)\t\n",
      "Epoch: [22][75/157]\tBatch Time 0.055 (0.055)\tLoss 0.0021 (0.0054)\t RLoss 184.0700 (138.3966)\t\n",
      "Epoch: [22][100/157]\tBatch Time 0.054 (0.055)\tLoss 0.0013 (0.0067)\t RLoss 133.9709 (136.1435)\t\n",
      "Epoch: [22][125/157]\tBatch Time 0.054 (0.055)\tLoss 0.0110 (0.0071)\t RLoss 85.2386 (134.1786)\t\n",
      "Epoch: [22][150/157]\tBatch Time 0.055 (0.055)\tLoss 0.0010 (0.0070)\t RLoss 142.5942 (133.7240)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0085 (0.0085)\t Regr Loss 136.6426 (136.6426)\t\n",
      "[25/10]\tBatch Time 0.047 (0.048)\tLoss 0.0048 (0.0068)\t Regr Loss 92.3350 (114.6607)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0004 (0.0053)\t Regr Loss 144.0349 (112.8469)\t\n",
      "[75/10]\tBatch Time 0.047 (0.047)\tLoss 0.0038 (0.0061)\t Regr Loss 77.6780 (115.4970)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0041 (0.0053)\t Regr Loss 143.6959 (116.1428)\t\n",
      "[125/10]\tBatch Time 0.048 (0.047)\tLoss 0.0006 (0.0052)\t Regr Loss 74.3401 (117.0929)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0008 (0.0055)\t Regr Loss 100.6066 (117.0621)\t\n",
      "\n",
      " * LOSS - 0.006\n",
      "\n",
      " * REGR LOSS - 117.133\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [23][0/157]\tBatch Time 0.056 (0.056)\tLoss 0.0004 (0.0004)\t RLoss 126.5290 (126.5290)\t\n",
      "Epoch: [23][25/157]\tBatch Time 0.055 (0.054)\tLoss 0.0060 (0.0045)\t RLoss 146.5311 (117.2400)\t\n",
      "Epoch: [23][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0004 (0.0048)\t RLoss 134.5934 (119.0304)\t\n",
      "Epoch: [23][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0081 (0.0043)\t RLoss 88.2239 (119.9364)\t\n",
      "Epoch: [23][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0007 (0.0042)\t RLoss 113.5272 (120.6591)\t\n",
      "Epoch: [23][125/157]\tBatch Time 0.055 (0.054)\tLoss 0.0020 (0.0055)\t RLoss 100.0307 (122.6565)\t\n",
      "Epoch: [23][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0057)\t RLoss 97.2974 (123.3265)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0003 (0.0003)\t Regr Loss 102.2244 (102.2244)\t\n",
      "[25/10]\tBatch Time 0.047 (0.047)\tLoss 0.0009 (0.0088)\t Regr Loss 96.7174 (115.4492)\t\n",
      "[50/10]\tBatch Time 0.047 (0.047)\tLoss 0.0004 (0.0076)\t Regr Loss 98.8932 (115.6042)\t\n",
      "[75/10]\tBatch Time 0.047 (0.047)\tLoss 0.0006 (0.0062)\t Regr Loss 92.2893 (114.9120)\t\n",
      "[100/10]\tBatch Time 0.047 (0.047)\tLoss 0.0089 (0.0059)\t Regr Loss 149.5518 (114.0133)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0003 (0.0061)\t Regr Loss 80.7877 (113.2264)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0006 (0.0056)\t Regr Loss 91.6595 (113.1823)\t\n",
      "\n",
      " * LOSS - 0.006\n",
      "\n",
      " * REGR LOSS - 113.310\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [24][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0008 (0.0008)\t RLoss 129.4623 (129.4623)\t\n",
      "Epoch: [24][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0021 (0.0052)\t RLoss 125.7195 (121.8663)\t\n",
      "Epoch: [24][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0172 (0.0050)\t RLoss 145.7040 (118.6984)\t\n",
      "Epoch: [24][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0017 (0.0045)\t RLoss 97.7933 (117.1010)\t\n",
      "Epoch: [24][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0019 (0.0047)\t RLoss 103.0345 (118.4791)\t\n",
      "Epoch: [24][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0004 (0.0049)\t RLoss 123.1339 (118.3301)\t\n",
      "Epoch: [24][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0151 (0.0050)\t RLoss 139.2676 (117.1928)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0031 (0.0031)\t Regr Loss 93.4806 (93.4806)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0037)\t Regr Loss 68.9856 (111.6799)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0001 (0.0028)\t Regr Loss 159.6351 (113.7088)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0005 (0.0026)\t Regr Loss 143.9744 (113.6337)\t\n",
      "[100/10]\tBatch Time 0.047 (0.048)\tLoss 0.0002 (0.0030)\t Regr Loss 66.6274 (111.5285)\t\n",
      "[125/10]\tBatch Time 0.047 (0.048)\tLoss 0.0022 (0.0032)\t Regr Loss 153.1857 (110.9574)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0086 (0.0031)\t Regr Loss 166.0856 (111.9255)\t\n",
      "\n",
      " * LOSS - 0.003\n",
      "\n",
      " * REGR LOSS - 111.334\n",
      "\n",
      "Epoch: [25][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0006 (0.0006)\t RLoss 78.5309 (78.5309)\t\n",
      "Epoch: [25][25/157]\tBatch Time 0.055 (0.054)\tLoss 0.0008 (0.0023)\t RLoss 108.7354 (99.1650)\t\n",
      "Epoch: [25][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0130 (0.0032)\t RLoss 107.6779 (104.3334)\t\n",
      "Epoch: [25][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0009 (0.0033)\t RLoss 68.1284 (108.4862)\t\n",
      "Epoch: [25][100/157]\tBatch Time 0.055 (0.054)\tLoss 0.0005 (0.0034)\t RLoss 100.7598 (108.4793)\t\n",
      "Epoch: [25][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0008 (0.0047)\t RLoss 96.9583 (110.0312)\t\n",
      "Epoch: [25][150/157]\tBatch Time 0.055 (0.054)\tLoss 0.0054 (0.0050)\t RLoss 119.7324 (112.8611)\t\n",
      "[0/10]\tBatch Time 0.047 (0.047)\tLoss 0.0020 (0.0020)\t Regr Loss 82.3600 (82.3600)\t\n",
      "[25/10]\tBatch Time 0.047 (0.047)\tLoss 0.0008 (0.0039)\t Regr Loss 82.4492 (108.6992)\t\n",
      "[50/10]\tBatch Time 0.047 (0.047)\tLoss 0.0018 (0.0032)\t Regr Loss 135.8241 (110.5007)\t\n",
      "[75/10]\tBatch Time 0.047 (0.047)\tLoss 0.0017 (0.0034)\t Regr Loss 95.4655 (107.6910)\t\n",
      "[100/10]\tBatch Time 0.047 (0.047)\tLoss 0.0004 (0.0032)\t Regr Loss 79.8080 (107.1735)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0006 (0.0029)\t Regr Loss 110.4433 (106.9906)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0002 (0.0027)\t Regr Loss 64.1875 (106.3690)\t\n",
      "\n",
      " * LOSS - 0.003\n",
      "\n",
      " * REGR LOSS - 106.181\n",
      "\n",
      "Epoch: [26][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0028 (0.0028)\t RLoss 99.7021 (99.7021)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0028)\t RLoss 69.2647 (107.2338)\t\n",
      "Epoch: [26][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0054 (0.0028)\t RLoss 155.7254 (106.9329)\t\n",
      "Epoch: [26][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0013 (0.0027)\t RLoss 94.3628 (109.5003)\t\n",
      "Epoch: [26][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0038 (0.0024)\t RLoss 113.9532 (109.5857)\t\n",
      "Epoch: [26][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0009 (0.0023)\t RLoss 80.9003 (107.1446)\t\n",
      "Epoch: [26][150/157]\tBatch Time 0.057 (0.054)\tLoss 0.0038 (0.0022)\t RLoss 133.3026 (107.8605)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0008 (0.0008)\t Regr Loss 95.6947 (95.6947)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0003 (0.0009)\t Regr Loss 64.8665 (101.7945)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0012 (0.0012)\t Regr Loss 72.2746 (99.1281)\t\n",
      "[75/10]\tBatch Time 0.047 (0.048)\tLoss 0.0003 (0.0010)\t Regr Loss 98.5310 (97.8755)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0002 (0.0013)\t Regr Loss 115.5860 (98.5774)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0013)\t Regr Loss 125.9489 (98.1532)\t\n",
      "[150/10]\tBatch Time 0.047 (0.048)\tLoss 0.0004 (0.0013)\t Regr Loss 101.4727 (100.4394)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 99.843\n",
      "\n",
      "Epoch: [27][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0044 (0.0044)\t RLoss 105.8699 (105.8699)\t\n",
      "Epoch: [27][25/157]\tBatch Time 0.055 (0.054)\tLoss 0.0001 (0.0036)\t RLoss 91.8870 (109.3643)\t\n",
      "Epoch: [27][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0140 (0.0047)\t RLoss 148.6779 (107.6461)\t\n",
      "Epoch: [27][75/157]\tBatch Time 0.055 (0.054)\tLoss 0.0001 (0.0037)\t RLoss 74.8568 (107.1658)\t\n",
      "Epoch: [27][100/157]\tBatch Time 0.055 (0.054)\tLoss 0.0002 (0.0031)\t RLoss 83.7105 (107.2972)\t\n",
      "Epoch: [27][125/157]\tBatch Time 0.055 (0.054)\tLoss 0.0006 (0.0030)\t RLoss 107.3096 (105.8212)\t\n",
      "Epoch: [27][150/157]\tBatch Time 0.055 (0.054)\tLoss 0.0007 (0.0029)\t RLoss 118.2882 (104.3373)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0006 (0.0006)\t Regr Loss 79.5014 (79.5014)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0036)\t Regr Loss 64.2279 (101.5377)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0006 (0.0028)\t Regr Loss 82.3116 (97.8999)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0294 (0.0027)\t Regr Loss 74.0670 (96.6073)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0025)\t Regr Loss 68.4186 (96.2861)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0008 (0.0028)\t Regr Loss 101.1083 (95.1035)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0002 (0.0032)\t Regr Loss 84.2218 (95.6100)\t\n",
      "\n",
      " * LOSS - 0.003\n",
      "\n",
      " * REGR LOSS - 95.540\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [28][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0001 (0.0001)\t RLoss 100.8879 (100.8879)\t\n",
      "Epoch: [28][25/157]\tBatch Time 0.055 (0.055)\tLoss 0.0006 (0.0018)\t RLoss 115.3516 (92.8272)\t\n",
      "Epoch: [28][50/157]\tBatch Time 0.054 (0.055)\tLoss 0.0009 (0.0034)\t RLoss 91.8815 (92.9196)\t\n",
      "Epoch: [28][75/157]\tBatch Time 0.054 (0.055)\tLoss 0.0075 (0.0037)\t RLoss 97.2544 (93.1073)\t\n",
      "Epoch: [28][100/157]\tBatch Time 0.054 (0.055)\tLoss 0.0228 (0.0047)\t RLoss 90.8039 (96.7307)\t\n",
      "Epoch: [28][125/157]\tBatch Time 0.054 (0.055)\tLoss 0.0020 (0.0050)\t RLoss 83.7728 (96.3441)\t\n",
      "Epoch: [28][150/157]\tBatch Time 0.054 (0.055)\tLoss 0.0063 (0.0056)\t RLoss 130.4658 (97.3388)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0003 (0.0003)\t Regr Loss 107.7723 (107.7723)\t\n",
      "[25/10]\tBatch Time 0.047 (0.048)\tLoss 0.0189 (0.0039)\t Regr Loss 104.3478 (88.8857)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0011 (0.0040)\t Regr Loss 83.0317 (89.5121)\t\n",
      "[75/10]\tBatch Time 0.047 (0.048)\tLoss 0.0012 (0.0043)\t Regr Loss 96.1784 (90.6092)\t\n",
      "[100/10]\tBatch Time 0.047 (0.048)\tLoss 0.0027 (0.0040)\t Regr Loss 115.7285 (91.6925)\t\n",
      "[125/10]\tBatch Time 0.047 (0.048)\tLoss 0.0066 (0.0040)\t Regr Loss 85.9908 (91.8064)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0134 (0.0037)\t Regr Loss 102.6681 (92.5437)\t\n",
      "\n",
      " * LOSS - 0.004\n",
      "\n",
      " * REGR LOSS - 92.601\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [29][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0006 (0.0006)\t RLoss 78.5224 (78.5224)\t\n",
      "Epoch: [29][25/157]\tBatch Time 0.054 (0.055)\tLoss 0.0019 (0.0045)\t RLoss 87.7563 (92.9733)\t\n",
      "Epoch: [29][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0017 (0.0046)\t RLoss 108.0358 (91.4933)\t\n",
      "Epoch: [29][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0076 (0.0045)\t RLoss 128.5121 (94.1525)\t\n",
      "Epoch: [29][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0048 (0.0047)\t RLoss 111.6023 (94.6012)\t\n",
      "Epoch: [29][125/157]\tBatch Time 0.055 (0.054)\tLoss 0.0027 (0.0051)\t RLoss 123.1425 (94.8273)\t\n",
      "Epoch: [29][150/157]\tBatch Time 0.055 (0.054)\tLoss 0.0016 (0.0049)\t RLoss 102.4022 (95.2734)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0006 (0.0006)\t Regr Loss 60.1762 (60.1762)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0047 (0.0022)\t Regr Loss 98.4969 (81.9419)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0002 (0.0020)\t Regr Loss 86.2301 (84.9059)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0020)\t Regr Loss 89.6409 (84.6239)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0017 (0.0022)\t Regr Loss 69.5877 (87.1285)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0002 (0.0021)\t Regr Loss 70.9466 (87.3583)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0003 (0.0022)\t Regr Loss 123.7457 (86.9820)\t\n",
      "\n",
      " * LOSS - 0.002\n",
      "\n",
      " * REGR LOSS - 86.721\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [30][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0018 (0.0018)\t RLoss 89.8952 (89.8952)\t\n",
      "Epoch: [30][25/157]\tBatch Time 0.054 (0.055)\tLoss 0.0165 (0.0027)\t RLoss 77.2891 (89.5921)\t\n",
      "Epoch: [30][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0025)\t RLoss 61.6061 (89.8797)\t\n",
      "Epoch: [30][75/157]\tBatch Time 0.054 (0.055)\tLoss 0.0006 (0.0036)\t RLoss 91.6532 (89.8337)\t\n",
      "Epoch: [30][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0014 (0.0031)\t RLoss 99.0446 (90.2521)\t\n",
      "Epoch: [30][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0031)\t RLoss 75.0391 (91.9412)\t\n",
      "Epoch: [30][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0002 (0.0032)\t RLoss 114.8207 (92.4681)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0006 (0.0006)\t Regr Loss 95.6414 (95.6414)\t\n",
      "[25/10]\tBatch Time 0.047 (0.047)\tLoss 0.0006 (0.0020)\t Regr Loss 81.4783 (85.9713)\t\n",
      "[50/10]\tBatch Time 0.047 (0.047)\tLoss 0.0001 (0.0016)\t Regr Loss 76.5694 (82.4963)\t\n",
      "[75/10]\tBatch Time 0.048 (0.047)\tLoss 0.0001 (0.0015)\t Regr Loss 88.0551 (82.8513)\t\n",
      "[100/10]\tBatch Time 0.048 (0.047)\tLoss 0.0005 (0.0016)\t Regr Loss 62.6529 (81.6574)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0001 (0.0017)\t Regr Loss 51.6038 (82.0620)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0002 (0.0019)\t Regr Loss 92.7249 (82.8293)\t\n",
      "\n",
      " * LOSS - 0.002\n",
      "\n",
      " * REGR LOSS - 82.206\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n",
      "Epoch: [31][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 95.6612 (95.6612)\t\n",
      "Epoch: [31][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0002 (0.0016)\t RLoss 63.8809 (83.8002)\t\n",
      "Epoch: [31][50/157]\tBatch Time 0.055 (0.054)\tLoss 0.0131 (0.0025)\t RLoss 84.5712 (81.8959)\t\n",
      "Epoch: [31][75/157]\tBatch Time 0.055 (0.054)\tLoss 0.0002 (0.0032)\t RLoss 54.0428 (85.1383)\t\n",
      "Epoch: [31][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0004 (0.0043)\t RLoss 82.8660 (85.6105)\t\n",
      "Epoch: [31][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0002 (0.0044)\t RLoss 82.3120 (85.0301)\t\n",
      "Epoch: [31][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0006 (0.0048)\t RLoss 76.5047 (86.2510)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0028 (0.0028)\t Regr Loss 91.6296 (91.6296)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0008 (0.0052)\t Regr Loss 98.4721 (89.0920)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0043 (0.0062)\t Regr Loss 122.0039 (87.0953)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0004 (0.0065)\t Regr Loss 70.3074 (87.5421)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0005 (0.0065)\t Regr Loss 66.9971 (87.3437)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0004 (0.0065)\t Regr Loss 67.1155 (88.2601)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0051 (0.0062)\t Regr Loss 101.5313 (87.7819)\t\n",
      "\n",
      " * LOSS - 0.007\n",
      "\n",
      " * REGR LOSS - 87.848\n",
      "\n",
      "\n",
      "Epochs since last improvement: 5\n",
      "\n",
      "Epoch: [32][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0414 (0.0414)\t RLoss 83.6801 (83.6801)\t\n",
      "Epoch: [32][25/157]\tBatch Time 0.055 (0.055)\tLoss 0.0439 (0.0068)\t RLoss 94.5300 (88.4389)\t\n",
      "Epoch: [32][50/157]\tBatch Time 0.055 (0.055)\tLoss 0.0015 (0.0049)\t RLoss 65.2049 (83.1878)\t\n",
      "Epoch: [32][75/157]\tBatch Time 0.055 (0.055)\tLoss 0.0023 (0.0038)\t RLoss 71.0937 (82.7422)\t\n",
      "Epoch: [32][100/157]\tBatch Time 0.055 (0.055)\tLoss 0.0025 (0.0038)\t RLoss 126.6047 (85.6266)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][125/157]\tBatch Time 0.055 (0.055)\tLoss 0.0003 (0.0034)\t RLoss 105.4082 (84.9422)\t\n",
      "Epoch: [32][150/157]\tBatch Time 0.055 (0.055)\tLoss 0.0007 (0.0031)\t RLoss 135.2543 (84.8977)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0001)\t Regr Loss 72.4718 (72.4718)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0009 (0.0036)\t Regr Loss 78.8329 (86.7622)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0008 (0.0037)\t Regr Loss 82.2251 (82.2437)\t\n",
      "[75/10]\tBatch Time 0.047 (0.048)\tLoss 0.0007 (0.0030)\t Regr Loss 89.7417 (79.4167)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0004 (0.0029)\t Regr Loss 78.8584 (80.3130)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0015 (0.0032)\t Regr Loss 98.5974 (80.2568)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0027 (0.0035)\t Regr Loss 74.0674 (79.5649)\t\n",
      "\n",
      " * LOSS - 0.003\n",
      "\n",
      " * REGR LOSS - 79.543\n",
      "\n",
      "\n",
      "Epochs since last improvement: 6\n",
      "\n",
      "Epoch: [33][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0002 (0.0002)\t RLoss 54.0746 (54.0746)\t\n",
      "Epoch: [33][25/157]\tBatch Time 0.054 (0.055)\tLoss 0.0004 (0.0028)\t RLoss 66.3125 (80.1591)\t\n",
      "Epoch: [33][50/157]\tBatch Time 0.054 (0.055)\tLoss 0.0025 (0.0020)\t RLoss 107.7309 (80.4736)\t\n",
      "Epoch: [33][75/157]\tBatch Time 0.055 (0.055)\tLoss 0.0099 (0.0024)\t RLoss 79.4922 (80.7052)\t\n",
      "Epoch: [33][100/157]\tBatch Time 0.055 (0.055)\tLoss 0.0001 (0.0025)\t RLoss 60.9954 (81.1295)\t\n",
      "Epoch: [33][125/157]\tBatch Time 0.054 (0.055)\tLoss 0.0012 (0.0031)\t RLoss 101.4575 (80.6532)\t\n",
      "Epoch: [33][150/157]\tBatch Time 0.054 (0.055)\tLoss 0.0002 (0.0035)\t RLoss 86.6242 (80.1359)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 50.1656 (50.1656)\t\n",
      "[25/10]\tBatch Time 0.047 (0.047)\tLoss 0.0015 (0.0011)\t Regr Loss 73.9058 (68.2580)\t\n",
      "[50/10]\tBatch Time 0.048 (0.047)\tLoss 0.0009 (0.0013)\t Regr Loss 86.6407 (69.9414)\t\n",
      "[75/10]\tBatch Time 0.047 (0.047)\tLoss 0.0006 (0.0021)\t Regr Loss 57.6032 (72.6206)\t\n",
      "[100/10]\tBatch Time 0.048 (0.047)\tLoss 0.0002 (0.0022)\t Regr Loss 98.8790 (72.2207)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0004 (0.0023)\t Regr Loss 79.6685 (71.5466)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0028 (0.0021)\t Regr Loss 62.8029 (70.7430)\t\n",
      "\n",
      " * LOSS - 0.002\n",
      "\n",
      " * REGR LOSS - 70.503\n",
      "\n",
      "\n",
      "Epochs since last improvement: 7\n",
      "\n",
      "Epoch: [34][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0002 (0.0002)\t RLoss 92.1843 (92.1843)\t\n",
      "Epoch: [34][25/157]\tBatch Time 0.055 (0.054)\tLoss 0.0003 (0.0024)\t RLoss 70.6726 (73.6093)\t\n",
      "Epoch: [34][50/157]\tBatch Time 0.055 (0.055)\tLoss 0.0004 (0.0025)\t RLoss 88.6138 (73.4194)\t\n",
      "Epoch: [34][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0003 (0.0023)\t RLoss 67.2763 (74.0344)\t\n",
      "Epoch: [34][100/157]\tBatch Time 0.055 (0.055)\tLoss 0.0029 (0.0025)\t RLoss 86.2253 (76.6268)\t\n",
      "Epoch: [34][125/157]\tBatch Time 0.055 (0.055)\tLoss 0.0003 (0.0026)\t RLoss 75.2029 (76.7429)\t\n",
      "Epoch: [34][150/157]\tBatch Time 0.054 (0.055)\tLoss 0.0003 (0.0029)\t RLoss 64.3715 (76.4528)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0002 (0.0002)\t Regr Loss 76.5564 (76.5564)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0002 (0.0006)\t Regr Loss 62.4368 (71.7700)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0002 (0.0012)\t Regr Loss 75.3920 (70.2928)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0014)\t Regr Loss 57.9533 (70.5010)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0031 (0.0014)\t Regr Loss 80.9214 (70.1247)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0004 (0.0013)\t Regr Loss 73.6630 (69.5558)\t\n",
      "[150/10]\tBatch Time 0.047 (0.048)\tLoss 0.0002 (0.0013)\t Regr Loss 73.1454 (69.4967)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 69.769\n",
      "\n",
      "\n",
      "Epochs since last improvement: 8\n",
      "\n",
      "Epoch: [35][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0002 (0.0002)\t RLoss 50.2427 (50.2427)\t\n",
      "Epoch: [35][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0003 (0.0014)\t RLoss 68.9710 (64.7347)\t\n",
      "Epoch: [35][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0024)\t RLoss 72.3399 (70.4771)\t\n",
      "Epoch: [35][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0028 (0.0020)\t RLoss 74.5862 (72.0752)\t\n",
      "Epoch: [35][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0017)\t RLoss 76.1901 (73.2121)\t\n",
      "Epoch: [35][125/157]\tBatch Time 0.055 (0.054)\tLoss 0.0002 (0.0015)\t RLoss 95.7057 (74.1230)\t\n",
      "Epoch: [35][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0015)\t RLoss 52.0009 (74.5581)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0002 (0.0002)\t Regr Loss 89.6496 (89.6496)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0013 (0.0017)\t Regr Loss 75.5410 (73.0379)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0045 (0.0011)\t Regr Loss 71.9408 (73.5044)\t\n",
      "[75/10]\tBatch Time 0.047 (0.048)\tLoss 0.0017 (0.0015)\t Regr Loss 111.4314 (73.2174)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0012)\t Regr Loss 65.5863 (72.5978)\t\n",
      "[125/10]\tBatch Time 0.047 (0.048)\tLoss 0.0003 (0.0013)\t Regr Loss 62.9670 (71.7750)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0080 (0.0014)\t Regr Loss 49.8549 (72.5631)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 72.154\n",
      "\n",
      "\n",
      "Epochs since last improvement: 9\n",
      "\n",
      "Epoch: [36][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0001 (0.0001)\t RLoss 67.7332 (67.7332)\t\n",
      "Epoch: [36][25/157]\tBatch Time 0.054 (0.055)\tLoss 0.0013 (0.0018)\t RLoss 59.3363 (78.2479)\t\n",
      "Epoch: [36][50/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0012)\t RLoss 63.3707 (75.9807)\t\n",
      "Epoch: [36][75/157]\tBatch Time 0.054 (0.055)\tLoss 0.0003 (0.0012)\t RLoss 69.1738 (74.1854)\t\n",
      "Epoch: [36][100/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0013)\t RLoss 78.3125 (74.2125)\t\n",
      "Epoch: [36][125/157]\tBatch Time 0.054 (0.055)\tLoss 0.0004 (0.0017)\t RLoss 73.0769 (73.1228)\t\n",
      "Epoch: [36][150/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0018)\t RLoss 66.9632 (72.9535)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0009 (0.0009)\t Regr Loss 66.2753 (66.2753)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0003 (0.0016)\t Regr Loss 90.2273 (67.6979)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0010)\t Regr Loss 77.1964 (71.4138)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0003 (0.0008)\t Regr Loss 87.9411 (69.5413)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0093 (0.0009)\t Regr Loss 62.2834 (68.8823)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0010)\t Regr Loss 119.3367 (69.7431)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0010)\t Regr Loss 73.3971 (70.2727)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 70.121\n",
      "\n",
      "Epoch: [37][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 70.6542 (70.6542)\t\n",
      "Epoch: [37][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0024 (0.0035)\t RLoss 77.3713 (64.8993)\t\n",
      "Epoch: [37][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0034)\t RLoss 60.4079 (62.8650)\t\n",
      "Epoch: [37][75/157]\tBatch Time 0.055 (0.054)\tLoss 0.0001 (0.0037)\t RLoss 83.2107 (64.1929)\t\n",
      "Epoch: [37][100/157]\tBatch Time 0.055 (0.054)\tLoss 0.0063 (0.0043)\t RLoss 63.4772 (65.8917)\t\n",
      "Epoch: [37][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0180 (0.0045)\t RLoss 74.3439 (67.7690)\t\n",
      "Epoch: [37][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0012 (0.0045)\t RLoss 71.8261 (68.9084)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0005 (0.0005)\t Regr Loss 77.4882 (77.4882)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0034)\t Regr Loss 56.9491 (61.0005)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0034)\t Regr Loss 85.9791 (62.5003)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0157 (0.0033)\t Regr Loss 59.7459 (63.0440)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0005 (0.0029)\t Regr Loss 107.2295 (63.6650)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0068 (0.0030)\t Regr Loss 57.9382 (63.6768)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0004 (0.0030)\t Regr Loss 56.3298 (64.8668)\t\n",
      "\n",
      " * LOSS - 0.003\n",
      "\n",
      " * REGR LOSS - 64.714\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [38][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0012 (0.0012)\t RLoss 55.2431 (55.2431)\t\n",
      "Epoch: [38][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0014)\t RLoss 44.1585 (65.7305)\t\n",
      "Epoch: [38][50/157]\tBatch Time 0.055 (0.054)\tLoss 0.0005 (0.0015)\t RLoss 63.8341 (63.4653)\t\n",
      "Epoch: [38][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0023)\t RLoss 64.6872 (64.2879)\t\n",
      "Epoch: [38][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0255 (0.0025)\t RLoss 49.2103 (64.3884)\t\n",
      "Epoch: [38][125/157]\tBatch Time 0.055 (0.054)\tLoss 0.0055 (0.0024)\t RLoss 73.8363 (66.2012)\t\n",
      "Epoch: [38][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0003 (0.0031)\t RLoss 57.2441 (67.2033)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0003 (0.0003)\t Regr Loss 57.2230 (57.2230)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0016 (0.0014)\t Regr Loss 42.1077 (65.1100)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0015)\t Regr Loss 76.6791 (66.0243)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0007 (0.0022)\t Regr Loss 49.4115 (65.8192)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0024)\t Regr Loss 74.4265 (65.2826)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0141 (0.0026)\t Regr Loss 78.5008 (65.4049)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0319 (0.0029)\t Regr Loss 51.7684 (66.3403)\t\n",
      "\n",
      " * LOSS - 0.003\n",
      "\n",
      " * REGR LOSS - 66.541\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [39][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0012 (0.0012)\t RLoss 67.3814 (67.3814)\t\n",
      "Epoch: [39][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0003 (0.0016)\t RLoss 40.0464 (60.1410)\t\n",
      "Epoch: [39][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0013)\t RLoss 53.1556 (60.4986)\t\n",
      "Epoch: [39][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0014)\t RLoss 62.7904 (60.5502)\t\n",
      "Epoch: [39][100/157]\tBatch Time 0.055 (0.054)\tLoss 0.0004 (0.0013)\t RLoss 54.8058 (62.1752)\t\n",
      "Epoch: [39][125/157]\tBatch Time 0.055 (0.054)\tLoss 0.0009 (0.0018)\t RLoss 94.1690 (62.5669)\t\n",
      "Epoch: [39][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0002 (0.0020)\t RLoss 61.3729 (63.1121)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0038 (0.0038)\t Regr Loss 68.9629 (68.9629)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0008)\t Regr Loss 61.4717 (54.8828)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0003 (0.0009)\t Regr Loss 72.5658 (58.5408)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0010)\t Regr Loss 72.3525 (60.6757)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0032 (0.0009)\t Regr Loss 50.7857 (59.6318)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0002 (0.0008)\t Regr Loss 74.8265 (60.4620)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0003 (0.0008)\t Regr Loss 57.8959 (59.8636)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 59.648\n",
      "\n",
      "Epoch: [40][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 43.9582 (43.9582)\t\n",
      "Epoch: [40][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0007 (0.0017)\t RLoss 82.2209 (61.9903)\t\n",
      "Epoch: [40][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0002 (0.0019)\t RLoss 58.9996 (60.4935)\t\n",
      "Epoch: [40][75/157]\tBatch Time 0.055 (0.054)\tLoss 0.0001 (0.0015)\t RLoss 50.0280 (60.7820)\t\n",
      "Epoch: [40][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0013)\t RLoss 66.4333 (61.5512)\t\n",
      "Epoch: [40][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0013)\t RLoss 42.5458 (62.1516)\t\n",
      "Epoch: [40][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0010 (0.0013)\t RLoss 69.7703 (61.9965)\t\n",
      "[0/10]\tBatch Time 0.047 (0.047)\tLoss 0.0004 (0.0004)\t Regr Loss 60.1981 (60.1981)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0018)\t Regr Loss 49.9203 (55.2720)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0033)\t Regr Loss 68.3217 (56.1439)\t\n",
      "[75/10]\tBatch Time 0.047 (0.047)\tLoss 0.0003 (0.0024)\t Regr Loss 61.6584 (57.3488)\t\n",
      "[100/10]\tBatch Time 0.048 (0.047)\tLoss 0.0007 (0.0022)\t Regr Loss 70.3937 (56.8154)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0001 (0.0023)\t Regr Loss 70.3821 (57.4448)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0014 (0.0024)\t Regr Loss 51.7061 (57.2090)\t\n",
      "\n",
      " * LOSS - 0.002\n",
      "\n",
      " * REGR LOSS - 57.082\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [41][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 56.2638 (56.2638)\t\n",
      "Epoch: [41][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0007 (0.0020)\t RLoss 55.2774 (58.7715)\t\n",
      "Epoch: [41][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0035)\t RLoss 35.7794 (59.3082)\t\n",
      "Epoch: [41][75/157]\tBatch Time 0.055 (0.054)\tLoss 0.0005 (0.0032)\t RLoss 49.2270 (59.7854)\t\n",
      "Epoch: [41][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0003 (0.0027)\t RLoss 36.2797 (60.7601)\t\n",
      "Epoch: [41][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0024)\t RLoss 67.5378 (60.7069)\t\n",
      "Epoch: [41][150/157]\tBatch Time 0.057 (0.054)\tLoss 0.0001 (0.0026)\t RLoss 59.3957 (60.7745)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0001)\t Regr Loss 47.0598 (47.0598)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0124 (0.0035)\t Regr Loss 59.8548 (62.9364)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0079 (0.0036)\t Regr Loss 47.7339 (58.9711)\t\n",
      "[75/10]\tBatch Time 0.047 (0.048)\tLoss 0.0001 (0.0036)\t Regr Loss 39.7840 (57.9596)\t\n",
      "[100/10]\tBatch Time 0.047 (0.047)\tLoss 0.0052 (0.0031)\t Regr Loss 69.8656 (57.7523)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0046 (0.0029)\t Regr Loss 69.4681 (57.3747)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0026)\t Regr Loss 64.9995 (56.8433)\t\n",
      "\n",
      " * LOSS - 0.003\n",
      "\n",
      " * REGR LOSS - 56.764\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [42][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0002 (0.0002)\t RLoss 44.8910 (44.8910)\t\n",
      "Epoch: [42][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0176 (0.0019)\t RLoss 64.7078 (60.2522)\t\n",
      "Epoch: [42][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0039 (0.0015)\t RLoss 51.6756 (57.1526)\t\n",
      "Epoch: [42][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0022)\t RLoss 57.4307 (57.4819)\t\n",
      "Epoch: [42][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0023)\t RLoss 42.8717 (58.5462)\t\n",
      "Epoch: [42][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0005 (0.0030)\t RLoss 62.3823 (58.9857)\t\n",
      "Epoch: [42][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0006 (0.0034)\t RLoss 44.4220 (58.1299)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0007 (0.0007)\t Regr Loss 51.9979 (51.9979)\t\n",
      "[25/10]\tBatch Time 0.047 (0.047)\tLoss 0.0091 (0.0070)\t Regr Loss 48.0002 (47.8007)\t\n",
      "[50/10]\tBatch Time 0.047 (0.047)\tLoss 0.0005 (0.0052)\t Regr Loss 63.1637 (48.2314)\t\n",
      "[75/10]\tBatch Time 0.047 (0.047)\tLoss 0.0001 (0.0062)\t Regr Loss 52.1515 (50.9292)\t\n",
      "[100/10]\tBatch Time 0.047 (0.047)\tLoss 0.0013 (0.0054)\t Regr Loss 56.4183 (50.5537)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0006 (0.0055)\t Regr Loss 56.8575 (50.1429)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0009 (0.0051)\t Regr Loss 89.6473 (50.4863)\t\n",
      "\n",
      " * LOSS - 0.005\n",
      "\n",
      " * REGR LOSS - 50.565\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [43][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0005 (0.0005)\t RLoss 50.0956 (50.0956)\t\n",
      "Epoch: [43][25/157]\tBatch Time 0.055 (0.055)\tLoss 0.0007 (0.0060)\t RLoss 40.5519 (50.3604)\t\n",
      "Epoch: [43][50/157]\tBatch Time 0.054 (0.055)\tLoss 0.0004 (0.0040)\t RLoss 56.5510 (52.6261)\t\n",
      "Epoch: [43][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0004 (0.0031)\t RLoss 33.1734 (53.0263)\t\n",
      "Epoch: [43][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0007 (0.0027)\t RLoss 59.5711 (54.1227)\t\n",
      "Epoch: [43][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0023)\t RLoss 42.3372 (54.5347)\t\n",
      "Epoch: [43][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0004 (0.0024)\t RLoss 52.0426 (56.3293)\t\n",
      "[0/10]\tBatch Time 0.047 (0.047)\tLoss 0.0001 (0.0001)\t Regr Loss 40.3020 (40.3020)\t\n",
      "[25/10]\tBatch Time 0.048 (0.047)\tLoss 0.0019 (0.0043)\t Regr Loss 49.3852 (51.1965)\t\n",
      "[50/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0053)\t Regr Loss 51.3971 (53.4000)\t\n",
      "[75/10]\tBatch Time 0.047 (0.047)\tLoss 0.0001 (0.0040)\t Regr Loss 36.4428 (53.3400)\t\n",
      "[100/10]\tBatch Time 0.047 (0.047)\tLoss 0.0016 (0.0033)\t Regr Loss 50.1860 (53.0705)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0055 (0.0031)\t Regr Loss 81.7330 (53.1496)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0002 (0.0034)\t Regr Loss 46.3641 (52.8140)\t\n",
      "\n",
      " * LOSS - 0.003\n",
      "\n",
      " * REGR LOSS - 52.961\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n",
      "Epoch: [44][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0004 (0.0004)\t RLoss 50.4260 (50.4260)\t\n",
      "Epoch: [44][25/157]\tBatch Time 0.055 (0.055)\tLoss 0.0002 (0.0011)\t RLoss 39.4845 (51.1181)\t\n",
      "Epoch: [44][50/157]\tBatch Time 0.054 (0.055)\tLoss 0.0016 (0.0026)\t RLoss 48.3178 (52.2975)\t\n",
      "Epoch: [44][75/157]\tBatch Time 0.055 (0.055)\tLoss 0.0002 (0.0028)\t RLoss 80.5871 (52.8922)\t\n",
      "Epoch: [44][100/157]\tBatch Time 0.055 (0.055)\tLoss 0.0004 (0.0025)\t RLoss 55.0882 (52.5189)\t\n",
      "Epoch: [44][125/157]\tBatch Time 0.054 (0.055)\tLoss 0.0002 (0.0022)\t RLoss 79.6485 (53.2915)\t\n",
      "Epoch: [44][150/157]\tBatch Time 0.055 (0.055)\tLoss 0.0005 (0.0020)\t RLoss 45.2682 (53.7570)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0002 (0.0002)\t Regr Loss 45.9315 (45.9315)\t\n",
      "[25/10]\tBatch Time 0.047 (0.048)\tLoss 0.0001 (0.0004)\t Regr Loss 44.9406 (51.5091)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0004)\t Regr Loss 52.1301 (49.4423)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0003)\t Regr Loss 46.7782 (50.6703)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0006)\t Regr Loss 56.7904 (50.6308)\t\n",
      "[125/10]\tBatch Time 0.047 (0.048)\tLoss 0.0001 (0.0005)\t Regr Loss 45.5645 (50.7081)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0002 (0.0004)\t Regr Loss 49.5896 (50.6281)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 50.426\n",
      "\n",
      "Epoch: [45][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 45.6530 (45.6530)\t\n",
      "Epoch: [45][25/157]\tBatch Time 0.055 (0.055)\tLoss 0.0002 (0.0001)\t RLoss 40.2994 (52.3705)\t\n",
      "Epoch: [45][50/157]\tBatch Time 0.054 (0.055)\tLoss 0.0008 (0.0003)\t RLoss 63.3276 (53.0516)\t\n",
      "Epoch: [45][75/157]\tBatch Time 0.054 (0.055)\tLoss 0.0030 (0.0004)\t RLoss 54.6992 (52.1335)\t\n",
      "Epoch: [45][100/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0007)\t RLoss 36.2953 (51.3630)\t\n",
      "Epoch: [45][125/157]\tBatch Time 0.057 (0.055)\tLoss 0.0192 (0.0010)\t RLoss 88.8654 (51.2279)\t\n",
      "Epoch: [45][150/157]\tBatch Time 0.055 (0.055)\tLoss 0.0001 (0.0016)\t RLoss 53.7232 (51.2513)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 38.5997 (38.5997)\t\n",
      "[25/10]\tBatch Time 0.047 (0.048)\tLoss 0.0004 (0.0017)\t Regr Loss 33.3842 (42.7386)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0002 (0.0017)\t Regr Loss 46.8525 (43.9710)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0013)\t Regr Loss 32.2453 (44.1976)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0015)\t Regr Loss 43.5993 (44.5786)\t\n",
      "[125/10]\tBatch Time 0.047 (0.048)\tLoss 0.0001 (0.0015)\t Regr Loss 41.7784 (44.8015)\t\n",
      "[150/10]\tBatch Time 0.047 (0.048)\tLoss 0.0001 (0.0015)\t Regr Loss 41.7831 (45.1362)\t\n",
      "\n",
      " * LOSS - 0.002\n",
      "\n",
      " * REGR LOSS - 45.282\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [46][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0003 (0.0003)\t RLoss 40.3599 (40.3599)\t\n",
      "Epoch: [46][25/157]\tBatch Time 0.055 (0.054)\tLoss 0.0109 (0.0018)\t RLoss 52.9247 (48.1650)\t\n",
      "Epoch: [46][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0013 (0.0015)\t RLoss 67.9479 (48.6912)\t\n",
      "Epoch: [46][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0013)\t RLoss 46.6799 (48.6870)\t\n",
      "Epoch: [46][100/157]\tBatch Time 0.055 (0.054)\tLoss 0.0001 (0.0010)\t RLoss 47.9581 (47.9058)\t\n",
      "Epoch: [46][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0053 (0.0014)\t RLoss 35.6307 (48.1088)\t\n",
      "Epoch: [46][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0012)\t RLoss 46.3844 (48.3546)\t\n",
      "[0/10]\tBatch Time 0.047 (0.047)\tLoss 0.0001 (0.0001)\t Regr Loss 39.6138 (39.6138)\t\n",
      "[25/10]\tBatch Time 0.047 (0.048)\tLoss 0.0008 (0.0003)\t Regr Loss 32.3074 (40.8074)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0004 (0.0004)\t Regr Loss 62.2965 (41.8520)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0004)\t Regr Loss 34.2285 (42.7526)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0003)\t Regr Loss 45.9756 (42.8452)\t\n",
      "[125/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0004)\t Regr Loss 46.1111 (42.9451)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0007)\t Regr Loss 36.7016 (42.7002)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 42.596\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [47][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0001 (0.0001)\t RLoss 38.6053 (38.6053)\t\n",
      "Epoch: [47][25/157]\tBatch Time 0.055 (0.054)\tLoss 0.0001 (0.0003)\t RLoss 42.9807 (47.8584)\t\n",
      "Epoch: [47][50/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0003)\t RLoss 35.5541 (45.7705)\t\n",
      "Epoch: [47][75/157]\tBatch Time 0.055 (0.054)\tLoss 0.0001 (0.0003)\t RLoss 55.3246 (44.6523)\t\n",
      "Epoch: [47][100/157]\tBatch Time 0.055 (0.055)\tLoss 0.0005 (0.0004)\t RLoss 47.7420 (45.0708)\t\n",
      "Epoch: [47][125/157]\tBatch Time 0.055 (0.055)\tLoss 0.0001 (0.0004)\t RLoss 41.2484 (45.9597)\t\n",
      "Epoch: [47][150/157]\tBatch Time 0.055 (0.055)\tLoss 0.0001 (0.0004)\t RLoss 74.2412 (45.9455)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 32.9509 (32.9509)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0002)\t Regr Loss 35.6107 (45.0392)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0002)\t Regr Loss 34.3395 (43.4814)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0002)\t Regr Loss 47.9449 (43.3433)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0010 (0.0002)\t Regr Loss 34.5987 (43.5541)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0002)\t Regr Loss 39.8944 (43.1767)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0002)\t Regr Loss 29.9818 (43.2245)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 43.200\n",
      "\n",
      "Epoch: [48][0/157]\tBatch Time 0.056 (0.056)\tLoss 0.0001 (0.0001)\t RLoss 40.2358 (40.2358)\t\n",
      "Epoch: [48][25/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0011)\t RLoss 26.4626 (40.3387)\t\n",
      "Epoch: [48][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0008)\t RLoss 31.0340 (40.6358)\t\n",
      "Epoch: [48][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0004 (0.0011)\t RLoss 28.8638 (41.5256)\t\n",
      "Epoch: [48][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0014)\t RLoss 30.0062 (41.8798)\t\n",
      "Epoch: [48][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0013)\t RLoss 43.1925 (42.6808)\t\n",
      "Epoch: [48][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0012)\t RLoss 63.7749 (44.7541)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 48.1474 (48.1474)\t\n",
      "[25/10]\tBatch Time 0.047 (0.048)\tLoss 0.0001 (0.0002)\t Regr Loss 39.1016 (44.3800)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0002 (0.0003)\t Regr Loss 45.4940 (43.9908)\t\n",
      "[75/10]\tBatch Time 0.047 (0.048)\tLoss 0.0001 (0.0004)\t Regr Loss 43.2796 (44.7488)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0007)\t Regr Loss 42.7728 (44.3793)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0002 (0.0009)\t Regr Loss 50.3117 (44.4144)\t\n",
      "[150/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0009)\t Regr Loss 40.1668 (44.0722)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 43.818\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [49][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0006 (0.0006)\t RLoss 59.6205 (59.6205)\t\n",
      "Epoch: [49][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0007)\t RLoss 30.8706 (46.0543)\t\n",
      "Epoch: [49][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0004)\t RLoss 33.8287 (44.6035)\t\n",
      "Epoch: [49][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0003)\t RLoss 52.4634 (44.8992)\t\n",
      "Epoch: [49][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0005)\t RLoss 32.0482 (44.5168)\t\n",
      "Epoch: [49][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0005)\t RLoss 40.0274 (44.4697)\t\n",
      "Epoch: [49][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0006)\t RLoss 33.4605 (44.3030)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 37.7422 (37.7422)\t\n",
      "[25/10]\tBatch Time 0.047 (0.047)\tLoss 0.0002 (0.0003)\t Regr Loss 35.9801 (35.3389)\t\n",
      "[50/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0002)\t Regr Loss 42.0536 (36.6195)\t\n",
      "[75/10]\tBatch Time 0.051 (0.048)\tLoss 0.0000 (0.0002)\t Regr Loss 31.8837 (36.2030)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0002)\t Regr Loss 24.0452 (37.0777)\t\n",
      "[125/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0002)\t Regr Loss 44.9619 (37.2091)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0002)\t Regr Loss 31.2771 (37.8411)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 37.911\n",
      "\n",
      "Epoch: [50][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0001 (0.0001)\t RLoss 43.3007 (43.3007)\t\n",
      "Epoch: [50][25/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0007)\t RLoss 42.5555 (43.1464)\t\n",
      "Epoch: [50][50/157]\tBatch Time 0.054 (0.055)\tLoss 0.0083 (0.0055)\t RLoss 45.4763 (40.4034)\t\n",
      "Epoch: [50][75/157]\tBatch Time 0.055 (0.055)\tLoss 0.0004 (0.0052)\t RLoss 47.6135 (41.0063)\t\n",
      "Epoch: [50][100/157]\tBatch Time 0.054 (0.055)\tLoss 0.0001 (0.0043)\t RLoss 40.6141 (41.7644)\t\n",
      "Epoch: [50][125/157]\tBatch Time 0.054 (0.055)\tLoss 0.0003 (0.0042)\t RLoss 49.5835 (42.7419)\t\n",
      "Epoch: [50][150/157]\tBatch Time 0.054 (0.055)\tLoss 0.0001 (0.0043)\t RLoss 36.8891 (42.3620)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0002 (0.0002)\t Regr Loss 34.7505 (34.7505)\t\n",
      "[25/10]\tBatch Time 0.047 (0.048)\tLoss 0.0036 (0.0041)\t Regr Loss 46.0395 (37.4987)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0061 (0.0042)\t Regr Loss 40.9278 (38.2274)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0046)\t Regr Loss 28.1978 (37.8807)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0003 (0.0052)\t Regr Loss 40.5425 (37.4059)\t\n",
      "[125/10]\tBatch Time 0.049 (0.048)\tLoss 0.0004 (0.0056)\t Regr Loss 43.8669 (37.5283)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0010 (0.0053)\t Regr Loss 33.7035 (37.6789)\t\n",
      "\n",
      " * LOSS - 0.005\n",
      "\n",
      " * REGR LOSS - 37.511\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [51][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0092 (0.0092)\t RLoss 46.5103 (46.5103)\t\n",
      "Epoch: [51][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0003 (0.0044)\t RLoss 40.0833 (40.6449)\t\n",
      "Epoch: [51][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0003 (0.0035)\t RLoss 26.8447 (40.4209)\t\n",
      "Epoch: [51][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0030)\t RLoss 32.9299 (39.8057)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [51][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0005 (0.0024)\t RLoss 44.1197 (39.5336)\t\n",
      "Epoch: [51][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0003 (0.0021)\t RLoss 40.9958 (39.3535)\t\n",
      "Epoch: [51][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0006 (0.0018)\t RLoss 41.7724 (39.9947)\t\n",
      "[0/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 39.0277 (39.0277)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0009)\t Regr Loss 40.4347 (37.9861)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0005)\t Regr Loss 32.1136 (37.5268)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0002 (0.0004)\t Regr Loss 35.0935 (36.8973)\t\n",
      "[100/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0003)\t Regr Loss 30.2068 (36.3805)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0004)\t Regr Loss 34.6264 (36.4765)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0005)\t Regr Loss 29.1748 (36.5531)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 36.451\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [52][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0002 (0.0002)\t RLoss 48.8633 (48.8633)\t\n",
      "Epoch: [52][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0002 (0.0026)\t RLoss 42.9655 (39.2787)\t\n",
      "Epoch: [52][50/157]\tBatch Time 0.055 (0.054)\tLoss 0.0001 (0.0023)\t RLoss 35.6732 (38.8128)\t\n",
      "Epoch: [52][75/157]\tBatch Time 0.055 (0.054)\tLoss 0.0015 (0.0019)\t RLoss 35.1336 (37.9948)\t\n",
      "Epoch: [52][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0449 (0.0034)\t RLoss 41.9184 (37.9495)\t\n",
      "Epoch: [52][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0030)\t RLoss 44.7701 (37.7613)\t\n",
      "Epoch: [52][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0002 (0.0027)\t RLoss 50.7290 (38.3443)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0001)\t Regr Loss 38.3775 (38.3775)\t\n",
      "[25/10]\tBatch Time 0.047 (0.048)\tLoss 0.0003 (0.0012)\t Regr Loss 26.5711 (34.3202)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0002 (0.0007)\t Regr Loss 29.0319 (36.1658)\t\n",
      "[75/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0007)\t Regr Loss 38.3501 (36.3159)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0347 (0.0011)\t Regr Loss 90.6316 (37.0382)\t\n",
      "[125/10]\tBatch Time 0.047 (0.048)\tLoss 0.0007 (0.0012)\t Regr Loss 48.1719 (36.9916)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0002 (0.0011)\t Regr Loss 33.3904 (36.9426)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 36.739\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [53][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 29.5000 (29.5000)\t\n",
      "Epoch: [53][25/157]\tBatch Time 0.055 (0.055)\tLoss 0.0001 (0.0007)\t RLoss 40.2450 (34.7783)\t\n",
      "Epoch: [53][50/157]\tBatch Time 0.055 (0.055)\tLoss 0.0003 (0.0010)\t RLoss 43.1618 (36.4503)\t\n",
      "Epoch: [53][75/157]\tBatch Time 0.055 (0.055)\tLoss 0.0001 (0.0008)\t RLoss 28.2280 (37.1241)\t\n",
      "Epoch: [53][100/157]\tBatch Time 0.055 (0.055)\tLoss 0.0001 (0.0017)\t RLoss 33.0020 (36.9186)\t\n",
      "Epoch: [53][125/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0015)\t RLoss 35.9561 (37.6451)\t\n",
      "Epoch: [53][150/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0015)\t RLoss 30.9678 (37.3675)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0001)\t Regr Loss 38.7611 (38.7611)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0025 (0.0018)\t Regr Loss 40.0831 (31.2090)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0011)\t Regr Loss 28.9988 (32.5514)\t\n",
      "[75/10]\tBatch Time 0.047 (0.048)\tLoss 0.0141 (0.0010)\t Regr Loss 42.6473 (32.4324)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0009)\t Regr Loss 33.0397 (32.7844)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0010)\t Regr Loss 28.7527 (32.7003)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0006 (0.0012)\t Regr Loss 38.1172 (32.7062)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 32.739\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n",
      "Epoch: [54][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0009 (0.0009)\t RLoss 34.9761 (34.9761)\t\n",
      "Epoch: [54][25/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0002)\t RLoss 26.8425 (31.4730)\t\n",
      "Epoch: [54][50/157]\tBatch Time 0.055 (0.055)\tLoss 0.0004 (0.0002)\t RLoss 36.2255 (32.9358)\t\n",
      "Epoch: [54][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0007)\t RLoss 58.5847 (33.0729)\t\n",
      "Epoch: [54][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0007)\t RLoss 24.5402 (33.9442)\t\n",
      "Epoch: [54][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0008)\t RLoss 32.1885 (34.3875)\t\n",
      "Epoch: [54][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0009)\t RLoss 34.1006 (34.7203)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 34.9151 (34.9151)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0016 (0.0016)\t Regr Loss 26.3862 (31.6485)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0010)\t Regr Loss 22.5577 (32.3065)\t\n",
      "[75/10]\tBatch Time 0.047 (0.048)\tLoss 0.0005 (0.0008)\t Regr Loss 31.6852 (32.4599)\t\n",
      "[100/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0006)\t Regr Loss 20.1620 (32.1019)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0006)\t Regr Loss 31.9986 (32.0726)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0009 (0.0008)\t Regr Loss 29.1571 (31.7668)\t\n",
      "\n",
      " * LOSS - 0.001\n",
      "\n",
      " * REGR LOSS - 31.890\n",
      "\n",
      "\n",
      "Epochs since last improvement: 5\n",
      "\n",
      "Epoch: [55][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0031 (0.0031)\t RLoss 38.5482 (38.5482)\t\n",
      "Epoch: [55][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0005)\t RLoss 35.0579 (33.4177)\t\n",
      "Epoch: [55][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0002 (0.0005)\t RLoss 34.4553 (33.5245)\t\n",
      "Epoch: [55][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0006 (0.0021)\t RLoss 25.3272 (33.0086)\t\n",
      "Epoch: [55][100/157]\tBatch Time 0.055 (0.054)\tLoss 0.0001 (0.0020)\t RLoss 29.2248 (33.4848)\t\n",
      "Epoch: [55][125/157]\tBatch Time 0.055 (0.054)\tLoss 0.0002 (0.0019)\t RLoss 44.2524 (33.7985)\t\n",
      "Epoch: [55][150/157]\tBatch Time 0.055 (0.054)\tLoss 0.0001 (0.0022)\t RLoss 28.9021 (33.7468)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0004 (0.0004)\t Regr Loss 24.4067 (24.4067)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0010 (0.0008)\t Regr Loss 30.4906 (30.3991)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0054 (0.0008)\t Regr Loss 41.1764 (30.4821)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0005 (0.0015)\t Regr Loss 30.0220 (31.1288)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0004 (0.0014)\t Regr Loss 24.3599 (30.4858)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0005 (0.0013)\t Regr Loss 29.3536 (30.6945)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0015)\t Regr Loss 37.8099 (31.0187)\t\n",
      "\n",
      " * LOSS - 0.002\n",
      "\n",
      " * REGR LOSS - 30.969\n",
      "\n",
      "\n",
      "Epochs since last improvement: 6\n",
      "\n",
      "Epoch: [56][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0035 (0.0035)\t RLoss 28.5171 (28.5171)\t\n",
      "Epoch: [56][25/157]\tBatch Time 0.055 (0.054)\tLoss 0.0002 (0.0016)\t RLoss 34.5554 (34.9991)\t\n",
      "Epoch: [56][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0014)\t RLoss 39.3265 (33.5334)\t\n",
      "Epoch: [56][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0011)\t RLoss 31.7353 (32.3440)\t\n",
      "Epoch: [56][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0004 (0.0011)\t RLoss 39.4218 (32.4239)\t\n",
      "Epoch: [56][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0002 (0.0009)\t RLoss 36.5490 (32.3298)\t\n",
      "Epoch: [56][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0009)\t RLoss 26.4958 (32.7523)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 22.3206 (22.3206)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0001)\t Regr Loss 22.7982 (29.1953)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0001 (0.0001)\t Regr Loss 16.8117 (28.2863)\t\n",
      "[75/10]\tBatch Time 0.047 (0.048)\tLoss 0.0001 (0.0001)\t Regr Loss 36.2524 (28.9509)\t\n",
      "[100/10]\tBatch Time 0.047 (0.048)\tLoss 0.0001 (0.0001)\t Regr Loss 28.2473 (29.5761)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0011 (0.0002)\t Regr Loss 37.5983 (29.9758)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0002 (0.0001)\t Regr Loss 24.7062 (29.8834)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 29.797\n",
      "\n",
      "Epoch: [57][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 26.2884 (26.2884)\t\n",
      "Epoch: [57][25/157]\tBatch Time 0.055 (0.054)\tLoss 0.0001 (0.0005)\t RLoss 41.7517 (32.3502)\t\n",
      "Epoch: [57][50/157]\tBatch Time 0.055 (0.055)\tLoss 0.0001 (0.0003)\t RLoss 23.5337 (30.5698)\t\n",
      "Epoch: [57][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0002)\t RLoss 34.5642 (31.4807)\t\n",
      "Epoch: [57][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0002)\t RLoss 26.8559 (32.2557)\t\n",
      "Epoch: [57][125/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0002)\t RLoss 24.0566 (32.2011)\t\n",
      "Epoch: [57][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0001 (0.0001)\t RLoss 41.0900 (32.3275)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 28.7833 (28.7833)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 23.6663 (26.4293)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 24.0475 (27.4121)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 30.3771 (28.7227)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 21.2418 (28.3890)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 24.1482 (28.7795)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 28.8340 (28.6796)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 28.631\n",
      "\n",
      "Epoch: [58][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0001 (0.0001)\t RLoss 30.4818 (30.4818)\t\n",
      "Epoch: [58][25/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 20.8314 (28.8303)\t\n",
      "Epoch: [58][50/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 37.8951 (29.6344)\t\n",
      "Epoch: [58][75/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 24.7742 (29.7790)\t\n",
      "Epoch: [58][100/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 42.3843 (29.6828)\t\n",
      "Epoch: [58][125/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 32.1596 (29.8880)\t\n",
      "Epoch: [58][150/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 32.1752 (30.0314)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 25.9157 (25.9157)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 35.2016 (26.3763)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 27.5823 (25.7356)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 27.6096 (26.1850)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 27.4068 (26.3639)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 19.0652 (26.3110)\t\n",
      "[150/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 34.5609 (26.8684)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 26.891\n",
      "\n",
      "Epoch: [59][0/157]\tBatch Time 0.056 (0.056)\tLoss 0.0000 (0.0000)\t RLoss 35.5298 (35.5298)\t\n",
      "Epoch: [59][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 29.3022 (26.3349)\t\n",
      "Epoch: [59][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 21.9539 (25.9239)\t\n",
      "Epoch: [59][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 44.4604 (27.0992)\t\n",
      "Epoch: [59][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 23.7020 (27.7120)\t\n",
      "Epoch: [59][125/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 25.2413 (28.4508)\t\n",
      "Epoch: [59][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 28.2587 (28.8530)\t\n",
      "[0/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 22.0115 (22.0115)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 33.9539 (28.7859)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 28.8729 (29.2388)\t\n",
      "[75/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 27.3732 (28.8143)\t\n",
      "[100/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 35.2135 (28.4437)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 23.1256 (28.0771)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 27.1301 (28.1425)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 27.994\n",
      "\n",
      "Epoch: [60][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 41.4934 (41.4934)\t\n",
      "Epoch: [60][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 34.9889 (29.8760)\t\n",
      "Epoch: [60][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 29.5675 (28.3940)\t\n",
      "Epoch: [60][75/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 26.9471 (27.9667)\t\n",
      "Epoch: [60][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 37.2135 (28.1099)\t\n",
      "Epoch: [60][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 29.9916 (28.0899)\t\n",
      "Epoch: [60][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 24.3032 (28.2188)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 31.6229 (31.6229)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 23.9232 (24.4187)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 36.2895 (24.3841)\t\n",
      "[75/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 26.5443 (24.0677)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 30.0474 (24.1841)\t\n",
      "[125/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 35.4367 (24.2201)\t\n",
      "[150/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 30.1232 (24.0105)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 23.999\n",
      "\n",
      "Epoch: [61][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 23.6375 (23.6375)\t\n",
      "Epoch: [61][25/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 23.7527 (26.0419)\t\n",
      "Epoch: [61][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 26.9438 (25.8795)\t\n",
      "Epoch: [61][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 28.0223 (26.1721)\t\n",
      "Epoch: [61][100/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 24.9341 (25.9562)\t\n",
      "Epoch: [61][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 26.5278 (26.1899)\t\n",
      "Epoch: [61][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 21.3685 (26.6812)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 32.8693 (32.8693)\t\n",
      "[25/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 21.1620 (24.2332)\t\n",
      "[50/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 22.0461 (24.8232)\t\n",
      "[75/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 18.2760 (24.5507)\t\n",
      "[100/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 35.7091 (24.3995)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 18.1420 (24.5276)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 27.1151 (24.4026)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 24.321\n",
      "\n",
      "Epoch: [62][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 26.1851 (26.1851)\t\n",
      "Epoch: [62][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 25.5836 (24.3608)\t\n",
      "Epoch: [62][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 32.1094 (25.0832)\t\n",
      "Epoch: [62][75/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 26.3893 (25.2118)\t\n",
      "Epoch: [62][100/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 32.9386 (25.5850)\t\n",
      "Epoch: [62][125/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 31.6683 (25.8758)\t\n",
      "Epoch: [62][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 30.9130 (26.0558)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 18.6699 (18.6699)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 19.9410 (21.2853)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 19.6634 (22.3596)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 19.9479 (22.3156)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 31.8987 (22.3795)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 22.1648 (22.3966)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 24.4519 (22.4638)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 22.610\n",
      "\n",
      "Epoch: [63][0/157]\tBatch Time 0.056 (0.056)\tLoss 0.0000 (0.0000)\t RLoss 18.9730 (18.9730)\t\n",
      "Epoch: [63][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 21.7007 (23.1519)\t\n",
      "Epoch: [63][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 27.7163 (23.5482)\t\n",
      "Epoch: [63][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 18.4702 (23.5963)\t\n",
      "Epoch: [63][100/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 22.8406 (24.3987)\t\n",
      "Epoch: [63][125/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 19.2869 (24.6368)\t\n",
      "Epoch: [63][150/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 37.6490 (25.1855)\t\n",
      "[0/10]\tBatch Time 0.049 (0.049)\tLoss 0.0000 (0.0000)\t Regr Loss 21.5874 (21.5874)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 23.1793 (22.7040)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 19.2810 (22.1392)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 16.0329 (22.1778)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 18.3868 (22.4931)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 22.7201 (22.4080)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 16.6732 (22.5900)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 22.658\n",
      "\n",
      "Epoch: [64][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 20.5977 (20.5977)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [64][25/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 20.1279 (22.2326)\t\n",
      "Epoch: [64][50/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 15.7125 (22.2377)\t\n",
      "Epoch: [64][75/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 20.7500 (23.1446)\t\n",
      "Epoch: [64][100/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 17.1116 (23.1452)\t\n",
      "Epoch: [64][125/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 21.4384 (23.4132)\t\n",
      "Epoch: [64][150/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 17.7087 (23.4817)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 17.9994 (17.9994)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 24.9200 (20.7512)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 25.2246 (21.3361)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 16.8060 (21.3819)\t\n",
      "[100/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 18.9308 (21.4308)\t\n",
      "[125/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 17.1746 (21.6374)\t\n",
      "[150/10]\tBatch Time 0.049 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 26.5831 (21.4364)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 21.518\n",
      "\n",
      "Epoch: [65][0/157]\tBatch Time 0.056 (0.056)\tLoss 0.0000 (0.0000)\t RLoss 18.8250 (18.8250)\t\n",
      "Epoch: [65][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 29.1431 (23.5233)\t\n",
      "Epoch: [65][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 28.4122 (23.1238)\t\n",
      "Epoch: [65][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 19.5171 (22.4424)\t\n",
      "Epoch: [65][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 25.4768 (22.3008)\t\n",
      "Epoch: [65][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 19.3993 (22.2806)\t\n",
      "Epoch: [65][150/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 23.3495 (22.8183)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 16.6950 (16.6950)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 18.0126 (22.2221)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 36.0004 (23.0160)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 18.8448 (23.0087)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 19.8523 (22.5707)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 20.7514 (22.4750)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 16.9866 (21.9973)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 21.913\n",
      "\n",
      "Epoch: [66][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 19.7722 (19.7722)\t\n",
      "Epoch: [66][25/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 19.3940 (19.7683)\t\n",
      "Epoch: [66][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 26.4515 (21.0521)\t\n",
      "Epoch: [66][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 24.0710 (21.2245)\t\n",
      "Epoch: [66][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 25.9125 (21.6578)\t\n",
      "Epoch: [66][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 23.8371 (21.6068)\t\n",
      "Epoch: [66][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 21.9178 (21.7772)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 18.0761 (18.0761)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 21.8044 (22.4565)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 23.1871 (21.9176)\t\n",
      "[75/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 15.3096 (22.1231)\t\n",
      "[100/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 21.3273 (22.1884)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 16.8414 (21.8952)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 21.5934 (21.8230)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 21.768\n",
      "\n",
      "Epoch: [67][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 21.5273 (21.5273)\t\n",
      "Epoch: [67][25/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 29.3267 (25.4698)\t\n",
      "Epoch: [67][50/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 17.7721 (23.6794)\t\n",
      "Epoch: [67][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 23.5850 (23.6705)\t\n",
      "Epoch: [67][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 20.7549 (24.2297)\t\n",
      "Epoch: [67][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 34.4832 (24.0204)\t\n",
      "Epoch: [67][150/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 21.9524 (24.0154)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 22.2666 (22.2666)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 21.9092 (20.9719)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 26.3716 (21.3661)\t\n",
      "[75/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 19.7166 (21.1583)\t\n",
      "[100/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 16.6409 (20.9798)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 20.7057 (21.1386)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 16.0627 (21.2240)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 21.122\n",
      "\n",
      "Epoch: [68][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 23.1355 (23.1355)\t\n",
      "Epoch: [68][25/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 36.0442 (20.7135)\t\n",
      "Epoch: [68][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 23.8313 (20.8597)\t\n",
      "Epoch: [68][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 20.9601 (20.8265)\t\n",
      "Epoch: [68][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 23.5064 (20.9150)\t\n",
      "Epoch: [68][125/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 25.9298 (21.4500)\t\n",
      "Epoch: [68][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 23.0362 (21.9201)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 15.8643 (15.8643)\t\n",
      "[25/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 18.8241 (21.6883)\t\n",
      "[50/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 22.8420 (21.0284)\t\n",
      "[75/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 19.9316 (21.5517)\t\n",
      "[100/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 63.3092 (22.1547)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 26.7475 (22.2100)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 22.4812 (22.0032)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 21.887\n",
      "\n",
      "Epoch: [69][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 24.3198 (24.3198)\t\n",
      "Epoch: [69][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 20.8154 (18.9955)\t\n",
      "Epoch: [69][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 17.8981 (19.5369)\t\n",
      "Epoch: [69][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 26.9951 (20.5344)\t\n",
      "Epoch: [69][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 20.3912 (21.7543)\t\n",
      "Epoch: [69][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 24.0188 (22.3496)\t\n",
      "Epoch: [69][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 21.9231 (22.9414)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 20.4243 (20.4243)\t\n",
      "[25/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 23.4982 (21.0619)\t\n",
      "[50/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 64.4644 (22.0796)\t\n",
      "[75/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 15.7741 (21.6561)\t\n",
      "[100/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 18.3111 (21.8353)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 20.8568 (21.7448)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 19.8896 (21.7496)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 21.712\n",
      "\n",
      "Epoch: [70][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 25.1029 (25.1029)\t\n",
      "Epoch: [70][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 22.4329 (21.2914)\t\n",
      "Epoch: [70][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 14.9048 (20.2537)\t\n",
      "Epoch: [70][75/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 24.3036 (20.7881)\t\n",
      "Epoch: [70][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 15.5739 (20.7313)\t\n",
      "Epoch: [70][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 23.2863 (21.0187)\t\n",
      "Epoch: [70][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 27.0555 (20.9608)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 18.1271 (18.1271)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 15.9854 (19.3837)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 16.8886 (18.7107)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 19.7659 (18.6505)\t\n",
      "[100/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 16.5725 (18.4956)\t\n",
      "[125/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 15.0673 (18.5636)\t\n",
      "[150/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 14.3798 (18.6920)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 18.645\n",
      "\n",
      "Epoch: [71][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 13.0535 (13.0535)\t\n",
      "Epoch: [71][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 21.8311 (20.2184)\t\n",
      "Epoch: [71][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 23.7224 (19.8021)\t\n",
      "Epoch: [71][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 16.8780 (19.3485)\t\n",
      "Epoch: [71][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 19.0900 (19.2591)\t\n",
      "Epoch: [71][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 19.0108 (19.3166)\t\n",
      "Epoch: [71][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 22.6748 (19.6286)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 14.3212 (14.3212)\t\n",
      "[25/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 13.3876 (19.0158)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 12.7342 (18.5014)\t\n",
      "[75/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 17.4332 (18.1579)\t\n",
      "[100/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 23.3710 (18.2775)\t\n",
      "[125/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 14.8976 (17.9402)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 18.4937 (17.8574)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 17.892\n",
      "\n",
      "Epoch: [72][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 16.3684 (16.3684)\t\n",
      "Epoch: [72][25/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 14.1336 (18.5650)\t\n",
      "Epoch: [72][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 15.7325 (18.1969)\t\n",
      "Epoch: [72][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 13.9971 (18.1995)\t\n",
      "Epoch: [72][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 18.7854 (18.0002)\t\n",
      "Epoch: [72][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 13.4686 (18.1346)\t\n",
      "Epoch: [72][150/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 25.0592 (18.6348)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 18.4695 (18.4695)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 15.5202 (16.7945)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 12.2754 (17.0932)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 21.1691 (17.0151)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 17.9132 (17.0224)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 12.8211 (17.0617)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 15.2269 (17.0708)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 17.069\n",
      "\n",
      "Epoch: [73][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 20.7900 (20.7900)\t\n",
      "Epoch: [73][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 20.9527 (20.2991)\t\n",
      "Epoch: [73][50/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 20.0862 (18.9642)\t\n",
      "Epoch: [73][75/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 16.9372 (18.6402)\t\n",
      "Epoch: [73][100/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 14.6750 (18.1178)\t\n",
      "Epoch: [73][125/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 28.2828 (18.2241)\t\n",
      "Epoch: [73][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 18.7118 (18.0246)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 18.7981 (18.7981)\t\n",
      "[25/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 15.4985 (15.6687)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 13.7562 (15.1025)\t\n",
      "[75/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 18.3276 (14.9524)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 11.9779 (14.9516)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 14.5943 (14.7879)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 13.7689 (14.8606)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 14.790\n",
      "\n",
      "Epoch: [74][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 19.5980 (19.5980)\t\n",
      "Epoch: [74][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 14.5979 (16.1783)\t\n",
      "Epoch: [74][50/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 14.1508 (16.4883)\t\n",
      "Epoch: [74][75/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 17.2600 (16.3332)\t\n",
      "Epoch: [74][100/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 19.5206 (16.6163)\t\n",
      "Epoch: [74][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 14.6066 (16.7942)\t\n",
      "Epoch: [74][150/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 32.6925 (16.8370)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 16.0174 (16.0174)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 17.3804 (17.2922)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 17.0712 (17.5445)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 18.3503 (17.3271)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 18.6996 (17.3890)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 18.9947 (17.2691)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 12.3338 (17.2477)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 17.202\n",
      "\n",
      "Epoch: [75][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 14.9949 (14.9949)\t\n",
      "Epoch: [75][25/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 14.1334 (16.8104)\t\n",
      "Epoch: [75][50/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 18.3108 (16.0764)\t\n",
      "Epoch: [75][75/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 18.6920 (16.2177)\t\n",
      "Epoch: [75][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 13.1025 (15.9884)\t\n",
      "Epoch: [75][125/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 16.3609 (15.9277)\t\n",
      "Epoch: [75][150/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 15.5299 (16.1144)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 18.0078 (18.0078)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 22.0672 (17.7494)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 16.8012 (17.2793)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 22.9872 (17.4450)\t\n",
      "[100/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 16.8984 (17.6463)\t\n",
      "[125/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 12.7499 (17.3068)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 15.4616 (17.2037)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 17.256\n",
      "\n",
      "Epoch: [76][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 12.4390 (12.4390)\t\n",
      "Epoch: [76][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 16.4540 (14.6651)\t\n",
      "Epoch: [76][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 19.6309 (15.2770)\t\n",
      "Epoch: [76][75/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 20.0471 (15.4118)\t\n",
      "Epoch: [76][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 15.2604 (15.5813)\t\n",
      "Epoch: [76][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 17.6096 (15.6432)\t\n",
      "Epoch: [76][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 15.2280 (15.6113)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 12.0012 (12.0012)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 16.0623 (14.0121)\t\n",
      "[50/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 14.7308 (14.3075)\t\n",
      "[75/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 11.6452 (14.2780)\t\n",
      "[100/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 16.5828 (14.1560)\t\n",
      "[125/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 14.1520 (14.0087)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 11.3101 (13.9053)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 13.926\n",
      "\n",
      "Epoch: [77][0/157]\tBatch Time 0.056 (0.056)\tLoss 0.0000 (0.0000)\t RLoss 10.5595 (10.5595)\t\n",
      "Epoch: [77][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 14.5077 (14.5111)\t\n",
      "Epoch: [77][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 12.3148 (14.4697)\t\n",
      "Epoch: [77][75/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 15.5146 (14.8492)\t\n",
      "Epoch: [77][100/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 15.1678 (15.3316)\t\n",
      "Epoch: [77][125/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 28.7936 (15.6806)\t\n",
      "Epoch: [77][150/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 14.0298 (15.6636)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 14.3657 (14.3657)\t\n",
      "[25/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 15.3859 (14.3999)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 15.6502 (15.2633)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 11.0051 (15.0521)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 15.2892 (14.8285)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 13.7856 (14.8004)\t\n",
      "[150/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 16.7210 (14.7704)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 14.685\n",
      "\n",
      "Epoch: [78][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 15.2903 (15.2903)\t\n",
      "Epoch: [78][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 15.3492 (14.4275)\t\n",
      "Epoch: [78][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 14.9897 (14.7219)\t\n",
      "Epoch: [78][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 13.4550 (15.2526)\t\n",
      "Epoch: [78][100/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 17.2866 (15.5253)\t\n",
      "Epoch: [78][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 13.6936 (15.5461)\t\n",
      "Epoch: [78][150/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 10.9459 (15.3166)\t\n",
      "[0/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 10.3949 (10.3949)\t\n",
      "[25/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 10.9423 (12.3605)\t\n",
      "[50/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 12.1020 (12.7233)\t\n",
      "[75/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 10.1145 (12.9283)\t\n",
      "[100/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 11.8674 (12.9007)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 13.0374 (12.8300)\t\n",
      "[150/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 10.4605 (12.7887)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 12.762\n",
      "\n",
      "Epoch: [79][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 12.1327 (12.1327)\t\n",
      "Epoch: [79][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 11.2469 (13.5625)\t\n",
      "Epoch: [79][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 15.7921 (13.8679)\t\n",
      "Epoch: [79][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 25.7620 (13.8246)\t\n",
      "Epoch: [79][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 16.5213 (13.7991)\t\n",
      "Epoch: [79][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 13.2107 (13.7506)\t\n",
      "Epoch: [79][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 16.7144 (14.0305)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 14.8554 (14.8554)\t\n",
      "[25/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 13.5838 (14.6108)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 12.2486 (14.3394)\t\n",
      "[75/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 13.3209 (14.1855)\t\n",
      "[100/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 13.3283 (14.0961)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 14.0093 (14.0600)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 13.6339 (14.0561)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 14.038\n",
      "\n",
      "Epoch: [80][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 15.9871 (15.9871)\t\n",
      "Epoch: [80][25/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 15.9667 (14.2545)\t\n",
      "Epoch: [80][50/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 15.6601 (14.6877)\t\n",
      "Epoch: [80][75/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 18.1477 (14.7062)\t\n",
      "Epoch: [80][100/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 12.4332 (14.5978)\t\n",
      "Epoch: [80][125/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 10.5538 (14.6263)\t\n",
      "Epoch: [80][150/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 14.6401 (14.8169)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 18.0889 (18.0889)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 12.4690 (15.6554)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 19.8437 (15.7519)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 14.2643 (15.5414)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 15.6738 (15.6645)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 18.1717 (15.6658)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 17.2348 (15.6159)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 15.619\n",
      "\n",
      "Epoch: [81][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 17.6401 (17.6401)\t\n",
      "Epoch: [81][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 17.2614 (15.7830)\t\n",
      "Epoch: [81][50/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 12.1701 (15.1761)\t\n",
      "Epoch: [81][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 15.6389 (15.8190)\t\n",
      "Epoch: [81][100/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 15.1128 (16.8028)\t\n",
      "Epoch: [81][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 18.5728 (17.0070)\t\n",
      "Epoch: [81][150/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 14.9275 (16.5881)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 16.4377 (16.4377)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 12.1089 (12.9493)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 12.8991 (13.1195)\t\n",
      "[75/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 15.9629 (13.4915)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 12.9228 (13.5586)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 14.1907 (13.4666)\t\n",
      "[150/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 10.1294 (13.4756)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 13.479\n",
      "\n",
      "Epoch: [82][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 14.1695 (14.1695)\t\n",
      "Epoch: [82][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 14.6860 (15.6426)\t\n",
      "Epoch: [82][50/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 16.0120 (15.3839)\t\n",
      "Epoch: [82][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 15.1800 (15.1362)\t\n",
      "Epoch: [82][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 13.9925 (14.7918)\t\n",
      "Epoch: [82][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 15.1012 (14.6286)\t\n",
      "Epoch: [82][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 12.9508 (14.7061)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 12.6629 (12.6629)\t\n",
      "[25/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 14.8128 (14.2498)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 13.3091 (14.2155)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 13.4570 (14.1415)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 17.3677 (14.0920)\t\n",
      "[125/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 11.0294 (14.2994)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 15.3371 (14.3737)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 14.351\n",
      "\n",
      "Epoch: [83][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 16.1117 (16.1117)\t\n",
      "Epoch: [83][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 15.0405 (14.5296)\t\n",
      "Epoch: [83][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 10.0212 (13.7330)\t\n",
      "Epoch: [83][75/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 15.1866 (13.3529)\t\n",
      "Epoch: [83][100/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 14.8576 (13.2528)\t\n",
      "Epoch: [83][125/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 17.5009 (13.4292)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [83][150/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 12.6805 (13.5131)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 12.1964 (12.1964)\t\n",
      "[25/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 15.0351 (14.1351)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 13.4153 (13.4811)\t\n",
      "[75/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 12.3743 (13.2064)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 19.2584 (13.3076)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 12.3926 (13.3020)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 11.1870 (13.2989)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 13.279\n",
      "\n",
      "Epoch: [84][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 12.7524 (12.7524)\t\n",
      "Epoch: [84][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 16.9007 (13.7586)\t\n",
      "Epoch: [84][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 10.5922 (13.7596)\t\n",
      "Epoch: [84][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 32.1309 (13.8373)\t\n",
      "Epoch: [84][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 10.0023 (13.7076)\t\n",
      "Epoch: [84][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 13.7063 (13.8687)\t\n",
      "Epoch: [84][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 9.1410 (13.7533)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 7.6279 (7.6279)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 9.1546 (12.2636)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 13.3030 (12.6249)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 12.4145 (12.3054)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 10.3841 (12.3587)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 14.4408 (12.1406)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 11.4398 (12.0885)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 12.096\n",
      "\n",
      "Epoch: [85][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 10.7155 (10.7155)\t\n",
      "Epoch: [85][25/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 10.4908 (12.4304)\t\n",
      "Epoch: [85][50/157]\tBatch Time 0.055 (0.056)\tLoss 0.0000 (0.0000)\t RLoss 14.0200 (12.2295)\t\n",
      "Epoch: [85][75/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 11.9113 (12.4123)\t\n",
      "Epoch: [85][100/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 13.4671 (12.7711)\t\n",
      "Epoch: [85][125/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 17.3901 (12.8482)\t\n",
      "Epoch: [85][150/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 14.2472 (12.9581)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 10.0883 (10.0883)\t\n",
      "[25/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 12.1987 (11.5855)\t\n",
      "[50/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 9.0600 (11.6486)\t\n",
      "[75/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 12.3416 (11.6206)\t\n",
      "[100/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 10.7581 (12.0078)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 15.0098 (12.1883)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 13.5447 (12.1270)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 12.203\n",
      "\n",
      "Epoch: [86][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 11.3806 (11.3806)\t\n",
      "Epoch: [86][25/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 10.7835 (11.5046)\t\n",
      "Epoch: [86][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 10.1463 (11.8025)\t\n",
      "Epoch: [86][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 14.5811 (12.2367)\t\n",
      "Epoch: [86][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 18.6917 (12.5377)\t\n",
      "Epoch: [86][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 17.3684 (12.9011)\t\n",
      "Epoch: [86][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 12.4411 (13.1237)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 13.1435 (13.1435)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 10.5209 (12.3066)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 27.9668 (12.5526)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 11.1146 (12.4624)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 13.2914 (12.8089)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 13.2873 (12.7914)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 11.6622 (12.6731)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 12.632\n",
      "\n",
      "Epoch: [87][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 10.0265 (10.0265)\t\n",
      "Epoch: [87][25/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 11.1374 (11.4888)\t\n",
      "Epoch: [87][50/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 12.3123 (11.5654)\t\n",
      "Epoch: [87][75/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 11.1836 (11.7639)\t\n",
      "Epoch: [87][100/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 10.1750 (11.7503)\t\n",
      "Epoch: [87][125/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 16.7853 (11.9937)\t\n",
      "Epoch: [87][150/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 13.1089 (12.1388)\t\n",
      "[0/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 11.4645 (11.4645)\t\n",
      "[25/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 9.8759 (12.4155)\t\n",
      "[50/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 11.5204 (12.5833)\t\n",
      "[75/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 17.2069 (12.4936)\t\n",
      "[100/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 14.9950 (12.4314)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 13.5368 (12.3118)\t\n",
      "[150/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 11.5783 (12.2813)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 12.200\n",
      "\n",
      "Epoch: [88][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 10.5827 (10.5827)\t\n",
      "Epoch: [88][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 11.4437 (15.8931)\t\n",
      "Epoch: [88][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 10.4043 (14.5994)\t\n",
      "Epoch: [88][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 14.1731 (13.9229)\t\n",
      "Epoch: [88][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 13.5785 (13.5099)\t\n",
      "Epoch: [88][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 11.1039 (13.2448)\t\n",
      "Epoch: [88][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 16.1641 (13.0350)\t\n",
      "[0/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 9.4545 (9.4545)\t\n",
      "[25/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 13.2889 (11.4238)\t\n",
      "[50/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 10.0241 (11.5123)\t\n",
      "[75/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 9.6546 (11.5035)\t\n",
      "[100/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 14.1404 (11.4778)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 9.6650 (11.4499)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 11.5911 (11.6071)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 11.620\n",
      "\n",
      "Epoch: [89][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 15.5222 (15.5222)\t\n",
      "Epoch: [89][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 13.0568 (11.8329)\t\n",
      "Epoch: [89][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 19.5464 (15.4846)\t\n",
      "Epoch: [89][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 12.6441 (15.0814)\t\n",
      "Epoch: [89][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 11.9125 (14.4099)\t\n",
      "Epoch: [89][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 22.9035 (14.1418)\t\n",
      "Epoch: [89][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 12.5101 (13.9178)\t\n",
      "[0/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 14.2317 (14.2317)\t\n",
      "[25/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 12.9603 (11.3131)\t\n",
      "[50/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 9.8037 (10.9389)\t\n",
      "[75/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 7.6527 (11.0418)\t\n",
      "[100/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 10.9863 (11.0772)\t\n",
      "[125/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 10.8594 (10.9067)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 9.7682 (10.8551)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 10.886\n",
      "\n",
      "Epoch: [90][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 11.1986 (11.1986)\t\n",
      "Epoch: [90][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 9.1913 (11.1171)\t\n",
      "Epoch: [90][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 11.5161 (11.8924)\t\n",
      "Epoch: [90][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 8.8569 (11.4983)\t\n",
      "Epoch: [90][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 10.5920 (11.3302)\t\n",
      "Epoch: [90][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 10.0148 (11.2540)\t\n",
      "Epoch: [90][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 10.8199 (11.3401)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 12.5170 (12.5170)\t\n",
      "[25/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 9.2909 (9.8026)\t\n",
      "[50/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 11.5917 (9.8832)\t\n",
      "[75/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 11.8167 (9.8732)\t\n",
      "[100/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 8.9356 (10.0226)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 12.4594 (10.1827)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 10.3324 (10.1981)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 10.343\n",
      "\n",
      "Epoch: [91][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 10.2434 (10.2434)\t\n",
      "Epoch: [91][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 8.5580 (10.7573)\t\n",
      "Epoch: [91][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 19.4614 (10.5755)\t\n",
      "Epoch: [91][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 11.2319 (11.1437)\t\n",
      "Epoch: [91][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 12.5451 (11.2939)\t\n",
      "Epoch: [91][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 10.9613 (11.1878)\t\n",
      "Epoch: [91][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 10.2667 (11.1336)\t\n",
      "[0/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 8.6492 (8.6492)\t\n",
      "[25/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 9.4041 (9.5183)\t\n",
      "[50/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 7.9017 (9.6933)\t\n",
      "[75/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 8.0434 (9.5262)\t\n",
      "[100/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 10.2362 (9.5386)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 10.4188 (9.6015)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 8.5793 (9.5567)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 9.568\n",
      "\n",
      "Epoch: [92][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 9.9513 (9.9513)\t\n",
      "Epoch: [92][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 12.0203 (10.1108)\t\n",
      "Epoch: [92][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 11.3265 (10.8434)\t\n",
      "Epoch: [92][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 9.7436 (10.5104)\t\n",
      "Epoch: [92][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 7.7577 (10.4341)\t\n",
      "Epoch: [92][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 9.4408 (10.4050)\t\n",
      "Epoch: [92][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 11.6241 (10.4869)\t\n",
      "[0/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 9.6602 (9.6602)\t\n",
      "[25/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 13.2456 (10.1573)\t\n",
      "[50/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 9.9673 (10.5254)\t\n",
      "[75/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 12.5791 (10.2654)\t\n",
      "[100/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 12.0015 (10.2853)\t\n",
      "[125/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 9.7696 (10.2887)\t\n",
      "[150/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 8.8143 (10.3762)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 10.380\n",
      "\n",
      "Epoch: [93][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 12.4026 (12.4026)\t\n",
      "Epoch: [93][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 8.8652 (10.3642)\t\n",
      "Epoch: [93][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 10.6349 (10.2941)\t\n",
      "Epoch: [93][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 9.8645 (10.1495)\t\n",
      "Epoch: [93][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 6.4153 (10.0100)\t\n",
      "Epoch: [93][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 9.2072 (10.1085)\t\n",
      "Epoch: [93][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 8.7347 (10.1686)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 10.0823 (10.0823)\t\n",
      "[25/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 9.1390 (10.1064)\t\n",
      "[50/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 8.5688 (10.3050)\t\n",
      "[75/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 13.1287 (10.4560)\t\n",
      "[100/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 11.2027 (10.6983)\t\n",
      "[125/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 12.8781 (10.8070)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 10.2833 (10.7700)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 10.732\n",
      "\n",
      "Epoch: [94][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 9.9404 (9.9404)\t\n",
      "Epoch: [94][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 12.3583 (10.1636)\t\n",
      "Epoch: [94][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 8.5450 (9.7430)\t\n",
      "Epoch: [94][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 7.4273 (9.6412)\t\n",
      "Epoch: [94][100/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 9.6204 (9.4635)\t\n",
      "Epoch: [94][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 11.3378 (9.6423)\t\n",
      "Epoch: [94][150/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 7.8897 (9.8978)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 9.2105 (9.2105)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 10.9547 (10.4109)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 8.4745 (10.4216)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 14.7268 (10.6278)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 9.6113 (10.4169)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 12.2313 (10.4850)\t\n",
      "[150/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 10.3415 (10.4830)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 10.472\n",
      "\n",
      "Epoch: [95][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 8.2221 (8.2221)\t\n",
      "Epoch: [95][25/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 10.2229 (10.2879)\t\n",
      "Epoch: [95][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 11.3621 (10.9131)\t\n",
      "Epoch: [95][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 9.2096 (10.8232)\t\n",
      "Epoch: [95][100/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 8.2458 (10.7219)\t\n",
      "Epoch: [95][125/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 10.3741 (10.6485)\t\n",
      "Epoch: [95][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 11.0901 (10.5860)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 9.2762 (9.2762)\t\n",
      "[25/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 8.6598 (8.8092)\t\n",
      "[50/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 9.7810 (8.8898)\t\n",
      "[75/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 7.9318 (8.9613)\t\n",
      "[100/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 9.2572 (8.8553)\t\n",
      "[125/10]\tBatch Time 0.051 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 10.0095 (8.8096)\t\n",
      "[150/10]\tBatch Time 0.048 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 9.6726 (8.7872)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 8.855\n",
      "\n",
      "Epoch: [96][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 13.5116 (13.5116)\t\n",
      "Epoch: [96][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 8.8439 (8.8977)\t\n",
      "Epoch: [96][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 11.9675 (8.9010)\t\n",
      "Epoch: [96][75/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 8.0165 (9.2322)\t\n",
      "Epoch: [96][100/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 11.0794 (9.3777)\t\n",
      "Epoch: [96][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 9.5681 (9.5165)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [96][150/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 9.5289 (9.5840)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 9.8803 (9.8803)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 8.2602 (8.9428)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 8.2744 (8.9450)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 7.3681 (8.7880)\t\n",
      "[100/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 11.3622 (8.8248)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 8.7642 (8.7388)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 9.6087 (8.6679)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 8.724\n",
      "\n",
      "Epoch: [97][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 7.8763 (7.8763)\t\n",
      "Epoch: [97][25/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 7.3175 (9.4000)\t\n",
      "Epoch: [97][50/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 7.8860 (9.5020)\t\n",
      "Epoch: [97][75/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 11.7988 (9.4742)\t\n",
      "Epoch: [97][100/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 9.3686 (9.5373)\t\n",
      "Epoch: [97][125/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 12.2100 (9.9033)\t\n",
      "Epoch: [97][150/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 16.0789 (10.7217)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 14.1588 (14.1588)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 13.9749 (17.6771)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 15.2579 (16.9865)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 15.1852 (16.8276)\t\n",
      "[100/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 17.5049 (16.7619)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 18.1189 (16.6826)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 16.4415 (16.6585)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 16.551\n",
      "\n",
      "Epoch: [98][0/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 13.8393 (13.8393)\t\n",
      "Epoch: [98][25/157]\tBatch Time 0.055 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 23.1146 (16.9869)\t\n",
      "Epoch: [98][50/157]\tBatch Time 0.054 (0.055)\tLoss 0.0000 (0.0000)\t RLoss 11.9713 (16.4948)\t\n",
      "Epoch: [98][75/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 16.3840 (16.7783)\t\n",
      "Epoch: [98][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 19.9726 (16.6414)\t\n",
      "Epoch: [98][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 16.7369 (16.3363)\t\n",
      "Epoch: [98][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 19.8176 (16.1252)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 12.8075 (12.8075)\t\n",
      "[25/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 10.6953 (13.4037)\t\n",
      "[50/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 11.3744 (13.2074)\t\n",
      "[75/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 12.8536 (13.3456)\t\n",
      "[100/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 11.6970 (13.2496)\t\n",
      "[125/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 12.6344 (13.1126)\t\n",
      "[150/10]\tBatch Time 0.047 (0.047)\tLoss 0.0000 (0.0000)\t Regr Loss 12.2746 (12.9685)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 12.962\n",
      "\n",
      "Epoch: [99][0/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 10.7231 (10.7231)\t\n",
      "Epoch: [99][25/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 16.1600 (14.4539)\t\n",
      "Epoch: [99][50/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 13.7705 (13.2474)\t\n",
      "Epoch: [99][75/157]\tBatch Time 0.055 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 8.9406 (12.2418)\t\n",
      "Epoch: [99][100/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 9.5437 (11.9373)\t\n",
      "Epoch: [99][125/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 12.0439 (11.9491)\t\n",
      "Epoch: [99][150/157]\tBatch Time 0.054 (0.054)\tLoss 0.0000 (0.0000)\t RLoss 13.7853 (11.9486)\t\n",
      "[0/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 9.2266 (9.2266)\t\n",
      "[25/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 10.7541 (10.1178)\t\n",
      "[50/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 12.4020 (10.1140)\t\n",
      "[75/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 9.4764 (9.9928)\t\n",
      "[100/10]\tBatch Time 0.047 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 11.2702 (10.1329)\t\n",
      "[125/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 9.1603 (10.1807)\t\n",
      "[150/10]\tBatch Time 0.048 (0.048)\tLoss 0.0000 (0.0000)\t Regr Loss 8.9787 (10.1821)\t\n",
      "\n",
      " * LOSS - 0.000\n",
      "\n",
      " * REGR LOSS - 10.156\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/ogg;base64,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\" type=\"audio/ogg\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main()\n",
    "\n",
    "# alert when training is done\n",
    "sound_file = '/home/rui/Downloads/newyear.ogg'\n",
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Ready!\n",
      "\n",
      "Loaded checkpoint from epoch 100. Best loss so far is 0.000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('All Ready!')\n",
    "\n",
    "\n",
    "filename = \"checkpoint_v2sliding.pth.tar\"\n",
    "checkpoint = torch.load(filename)\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "best_loss = checkpoint['best_loss']\n",
    "print('\\nLoaded checkpoint from epoch %d. Best loss so far is %.3f.\\n' % (start_epoch, best_loss))\n",
    "classifModel = checkpoint['model1']\n",
    "regrModel = checkpoint['model2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "FOR N =  1\n",
      "tensor([0., 0., 0., 0., 1., 1., 0., 1., 1.])\n",
      "[125.0, 142.0, 179.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(test_loader)\n",
    "images, labels, coords = dataiter.next()\n",
    "\n",
    "n = 1\n",
    "print(\"\\n\\nFOR N = \", n)\n",
    "imgSample = images[n]\n",
    "max_idx = torch.argmax(coords[n][:,0])\n",
    "x, y, theta = coords[n][max_idx]\n",
    "print(labels[n])\n",
    "print([float(zed) for zed in (x,y,theta)])\n",
    "#print(x,y,theta)\n",
    "#crops, __ = makeCrops(imgSample, 50, (100, 100), (x,y, theta))\n",
    "\n",
    "plt.imshow(imgSample)\n",
    "#fig, axess = plt.subplots(3,3, figsize=(10,10))\n",
    "#axess = np.array(axess).flatten()#order='F') #'F' means to flatten in column-major (Fortran- style) order.\n",
    "#for i in range(9):\n",
    "#    axess[i].imshow(crops[i])\n",
    "#    axess[i].set_ylabel('# '+str(i))\n",
    "plt.suptitle('Window')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss across batch size of  32 is:  tensor(0.0013, device='cuda:0') tensor(79.7352, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# -- Check the results -------------------------------------------------------\n",
    "\n",
    "# -- Utility ---------------------------------------------\n",
    "def makeCrops(image, stepSize, windowSize, true_center):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    image = image.type(torch.FloatTensor)\n",
    "    crops = []\n",
    "    truths = []\n",
    "    c_x, c_y, orient = true_center\n",
    "    # TODO: look into otdering, why it's y,x !\n",
    "    margin = 5 \n",
    "    # --> is x, but is the column\n",
    "    # to slide horizontally, y must come first\n",
    "    for y in range(0, image.shape[0] - windowSize[0] + 1, stepSize):\n",
    "        for x in range(0, image.shape[1] - windowSize[1] + 1, stepSize):\n",
    "            end_x, end_y = x + windowSize[1], y + windowSize[0]\n",
    "            hasRect = (x + margin < c_x < end_x - margin) and (\n",
    "                y + margin < c_y < end_y - margin\n",
    "            )\n",
    "            truths.append(hasRect)\n",
    "            crops.append(image[y:end_y, x:end_x])\n",
    "    crops = torch.stack(crops)\n",
    "    print(\"shape of crops\", crops.shape)\n",
    "    return crops, truths\n",
    "\n",
    "\n",
    "# -- Get some validation results ---------------------------------------------\n",
    "classifModel.to(device).eval()\n",
    "regrModel.to(device).eval()\n",
    "\n",
    "c_criterion = nn.BCELoss()\n",
    "r_criterion = nn.MSELoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    dataiter = iter(test_loader)\n",
    "    images, labels, coords = dataiter.next()\n",
    "    \n",
    "    # Move to default device\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    coords = coords.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    predicted_class, all_crops = classifModel(images)\n",
    "    # Loss\n",
    "    loss = c_criterion(predicted_class, labels)\n",
    "\n",
    "    \n",
    "    # Forward pass\n",
    "    predicted_coords = regrModel(all_crops)\n",
    "\n",
    "    labels_est = torch.FloatTensor(\n",
    "        predicted_class.detach().cpu().numpy())\n",
    "    mask2 = torch.round(labels_est).type_as(coords)\n",
    "    mask2.unsqueeze_(2)\n",
    "    mask2 = mask2.repeat(1, 1, 3)\n",
    "    masked_est = mask2 * predicted_coords\n",
    "    masked_truth = coords\n",
    "\n",
    "    # Loss\n",
    "    #loss2 = r_criterion(masked_est, masked_truth)\n",
    "    loss2 = r_criterion(predicted_coords, coords)\n",
    "\n",
    "print(\"loss across batch size of \", labels.size()[0], 'is: ', loss, loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of crops torch.Size([9, 100, 100])\n",
      "!-- FOR N =  2\n",
      "y (crops) \n",
      "\t [0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
      "!-- yhat (crops) \n",
      "\t [0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
      "\n",
      "\n",
      "FOR N =  2\n",
      "!-- center y \n",
      "\t [73.0, 136.0, 92.0]\n",
      "!-- center y est \n",
      "\t  [70.31342315673828, 124.98744201660156, 78.8931884765625]\n",
      "tensor([[ -1.0646,   0.5493,  -0.4236],\n",
      "        [ -1.0791,  -0.9135,  -6.0298],\n",
      "        [ -1.4885,   1.0304,   5.1625],\n",
      "        [ 70.3134, 124.9874,  78.8932],\n",
      "        [ 68.5238, 130.0926,  80.7289],\n",
      "        [  1.6906,   2.3211,   2.7990],\n",
      "        [ 67.1859, 125.6823,  84.0848],\n",
      "        [ 70.1525, 129.6729,  90.5853],\n",
      "        [ -0.7017,   0.4389,  -0.7159]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -- Plot windows and print labels -----------------------------------\n",
    "n = 2\n",
    "imgSample = images[n]\n",
    "crops, __ = makeCrops(imgSample, 50, (100, 100), (x,y, theta))\n",
    "# -- Show original image, and the sliding windo crops -------\n",
    "plt.imshow(imgSample)\n",
    "# ---------------------------------------------------\n",
    "foo_label = labels[n]\n",
    "\n",
    "foo_coord = coords[n] \n",
    "predicted_locs = predicted_coords.view(-1,\n",
    "                                     coords.size(1), coords.size(2))\n",
    "foo_coord_est = predicted_locs[n] # 3 per window\n",
    "foo_label_est = predicted_class[n]\n",
    "\n",
    "max_idx = torch.argmax(foo_coord[:,0])\n",
    "x, y, theta = foo_coord[max_idx]\n",
    "est_max_idx = torch.argmax(foo_coord_est[:,0])\n",
    "x_est, y_est, theta_est = foo_coord_est[est_max_idx]\n",
    "\n",
    "# -- Print x,y for one result -------\n",
    "# -- Print window t/f for one result -------\n",
    "print(\"!-- FOR N = \", n)\n",
    "print(\"y (crops) \\n\\t\", [int(l) for l in foo_label])\n",
    "print(\"!-- yhat (crops) \\n\\t\", [int(np.round(p,0)) for p in foo_label_est] )\n",
    "# -------------------------------------------------\n",
    "sns.set(rc={\"figure.figsize\": (8, 6)})\n",
    "\n",
    "print(\"\\n\\nFOR N = \", n)\n",
    "print(\"!-- center y \\n\\t\", [float(zed) for zed in (x, y, theta)])\n",
    "print(\"!-- center y est \\n\\t \", [float(zed) for zed in (x_est, y_est, theta_est)])\n",
    "print(foo_coord_est)\n",
    "\n",
    "fig, axess = plt.subplots(3,3, figsize=(10,10))\n",
    "axess = np.array(axess).flatten()#order='F') #'F' means to flatten in column-major (Fortran- style) order.\n",
    "for i in range(9):\n",
    "    axess[i].imshow(crops[i])\n",
    "    axess[i].set_ylabel('# '+str(i))\n",
    "plt.suptitle('Window')\n",
    "plt.show()\n",
    "\n",
    "#for (i, crop) in enumerate(crops):\n",
    "    # print(\"1-index number of window: \", i+1, 'x', x, 'y', y, 'has rectangle?', hasRect)\n",
    "    #plt.figure()\n",
    "    #plt.suptitle(\"numero: %d\" % (i))\n",
    "    #plt.imshow(crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!-- \n",
      "True locations, defined for each crop\n",
      " tensor([[  0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.],\n",
      "        [ 73., 136.,  92.],\n",
      "        [ 73., 136.,  92.],\n",
      "        [  0.,   0.,   0.],\n",
      "        [ 73., 136.,  92.],\n",
      "        [ 73., 136.,  92.],\n",
      "        [  0.,   0.,   0.]], device='cuda:0')\n",
      "!-- \n",
      "Full predicted locations (3 per crop)\n",
      " tensor([[ -1.0646,   0.5493,  -0.4236],\n",
      "        [ -1.0791,  -0.9135,  -6.0298],\n",
      "        [ -1.4885,   1.0304,   5.1625],\n",
      "        [ 70.3134, 124.9874,  78.8932],\n",
      "        [ 68.5238, 130.0926,  80.7289],\n",
      "        [  1.6906,   2.3211,   2.7990],\n",
      "        [ 67.1859, 125.6823,  84.0848],\n",
      "        [ 70.1525, 129.6729,  90.5853],\n",
      "        [ -0.7017,   0.4389,  -0.7159]], device='cuda:0')\n",
      "tensor([[0, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8],\n",
      "        [1, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 1]])\n",
      "\n",
      "SAMPLE  0\n",
      "!-- y (crops) \n",
      "\t [0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
      "!-- yhat (crops) \n",
      "\t [0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
      "\n",
      "SAMPLE  1\n",
      "!-- y (crops) \n",
      "\t [0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "!-- yhat (crops) \n",
      "\t [0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "\n",
      "SAMPLE  2\n",
      "!-- y (crops) \n",
      "\t [0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
      "!-- yhat (crops) \n",
      "\t [0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
      "\n",
      "SAMPLE  3\n",
      "!-- y (crops) \n",
      "\t [0, 0, 0, 0, 1, 1, 0, 0, 0]\n",
      "!-- yhat (crops) \n",
      "\t [0, 0, 0, 0, 1, 1, 0, 0, 0]\n",
      "\n",
      "SAMPLE  4\n",
      "!-- y (crops) \n",
      "\t [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "!-- yhat (crops) \n",
      "\t [0, 0, 0, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#print(\"!-- yhat \\n\\t\", [int(round(o, 0)) for o in outputs[n].cpu().numpy()])    \n",
    "print(\"!-- \")\n",
    "print('True locations, defined for each crop\\n', foo_coord)\n",
    "print(\"!-- \")\n",
    "print('Full predicted locations (3 per crop)\\n', foo_coord_est)\n",
    "#print(\"\\n ------ x,y center + orient: \", coords[n], \"\\n\"))\n",
    "print(np.argwhere(foo_coord_est > 0) )\n",
    "\n",
    "# -- Print outputs for multiple results -------\n",
    "for ix in range(5):\n",
    "    print('\\nSAMPLE ', ix)\n",
    "    print(\"!-- y (crops) \\n\\t\", [int(l) for l in labels[ix]])\n",
    "    print(\"!-- yhat (crops) \\n\\t\", [int(np.round(p, 0)) for p in predicted_class[ix]] )\n",
    "\n",
    "# -- Main ---------------------------------------------\n",
    "#if __name__ == '__main__':\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
