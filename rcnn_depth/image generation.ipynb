{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import io\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "IMG_X, IMG_Y = 200,200 \n",
    "# length and width of blocks (fixed for now)\n",
    "block_l, block_w = 20, 30\n",
    "\n",
    "# img_list = []\n",
    "\n",
    "\n",
    "# Calc rectangle vertices. makeRectangle() credit Sparkler, stackoverflow, feb 17\n",
    "def makeRectangle(l, w, theta, offset=(0, 0)):\n",
    "    c, s = math.cos(theta), math.sin(theta)\n",
    "    rectCoords = [(l/2.0, w/2.0), (l/2.0, -w/2.0), (-l/2.0, -w/2.0), (-l/2.0, w/2.0)]\n",
    "    return [(c*x-s*y+offset[0], s*x+c*y+offset[1]) for (x, y) in rectCoords]\n",
    "\n",
    "\n",
    "# ---- Make depth images ---\n",
    "def make_dataset(dirname, num_images):\n",
    "    true_coords = []\n",
    "    newpath = './' + dirname  \n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "        print(newpath)\n",
    "    for i in range(num_images):\n",
    "        #orient = 0 # degrees\n",
    "        img = Image.new('RGB', (IMG_X, IMG_Y), 'black')\n",
    "\n",
    "        # block_l and _w offset so blocks don't run off edge of image \n",
    "        rand_x = int(np.random.rand() * (IMG_X-2*block_l)) + block_l\n",
    "        rand_y = int(np.random.rand() * (IMG_Y-2*block_w)) + block_w\n",
    "        orient = int(np.random.rand() * 180)  # .random() is range [0.0, 1.0).\n",
    "        orient = math.radians(orient) # math.cos takes radians!\n",
    "\n",
    "        true_coords.append(np.array((rand_x, rand_y, orient)))\n",
    "\n",
    "        rect_vertices = makeRectangle(block_l, block_w, orient, offset=(rand_x,\n",
    "                                                                        rand_y))\n",
    "\n",
    "        idraw = ImageDraw.Draw(img)\n",
    "        idraw.polygon(rect_vertices, fill='white')\n",
    "\n",
    "        # use a truetype font\n",
    "        #font = imagefont.truetype(\"dejavusans.ttf\", 15)\n",
    "        #font = imagefont.truetype(\"arial.ttf\",14)\n",
    "        #idraw.text((10, 25), '('+ str(rand_x) + ', ' + str(rand_y) +')')\n",
    "        img.save(newpath + '/rect'+str(i)+'.png')\n",
    "    return true_coords\n",
    "\n",
    "train_truth = make_dataset('data', 5000)\n",
    "print(len(train_truth))\n",
    "test_truth = make_dataset('./data/test', 300)\n",
    "    \n",
    "class RectDepthImgsDataset(Dataset):\n",
    "    \"\"\"Artificially generated depth images dataset\"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, coords, transform=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.true_coords = coords\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        #print('true coord len', len(self.true_coords))\n",
    "        return len(self.true_coords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # image = self.images[idx]\n",
    "        image = io.imread(self.img_dir + '/rect'+str(idx)+'.png')\n",
    "        image = torch.FloatTensor(image).permute(2, 0, 1) #PIL and torch expect difft orders\n",
    "        coords = torch.FloatTensor(self.true_coords[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # sample = {'image': image, 'grasp': str(coords[0]) + str(coords[1])}\n",
    "        sample = {'image': image, 'grasp': coords}\n",
    "        sample = image, coords\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available? device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameters\n",
    "\n",
    "num_epochs = 50 \n",
    "num_classes = 3 # predicting x,y,orientation\n",
    "learning_rate = 0.001\n",
    "batch_size = 15 \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA available? device: \", device)\n",
    "\n",
    "# Dataset is depth images of rectangular blocks\n",
    "train_dataset = RectDepthImgsDataset(img_dir='./data', coords=train_truth)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_dataset = RectDepthImgsDataset(img_dir='./data/test', coords=test_truth)\n",
    "\n",
    "# Data loader\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):  # CIFAR is 32x32x3, MNIST is 28x28x1)\n",
    "    def __init__(self, IMG_X, IMG_Y):\n",
    "        super(Net, self).__init__()\n",
    "        self._imgx = IMG_X\n",
    "        self._imgy = IMG_Y\n",
    "        _pool = 2\n",
    "        _stride = 5\n",
    "        _outputlayers = 16\n",
    "        \n",
    "        def _calc(val):\n",
    "            layer_size = (val- (_stride-1)) / _pool\n",
    "            return layer_size \n",
    "        \n",
    "        #print(self._imgx)\n",
    "        self._const = _calc(_calc(self._imgx))\n",
    "        self._const *= _calc(_calc(self._imgy))\n",
    "        self._const *= _outputlayers \n",
    "        #print(self._const)\n",
    "        self._const = int(self._const)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 6, _stride).to(device)\n",
    "        self.pool = nn.MaxPool2d(_pool, _pool).to(device)\n",
    "        self.conv2 = nn.Conv2d(6, _outputlayers, _stride).to(device)\n",
    "        self.fc1 = nn.Linear(self._const, 120).to(device)\n",
    "        self.fc2 = nn.Linear(120, 84).to(device)\n",
    "        self.fc3 = nn.Linear(84, num_classes).to(device)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        x = x.to(device)\n",
    "        x = x.view(-1, 3, IMG_X, IMG_Y)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self._const)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net(IMG_X, IMG_Y)\n",
    "model = model.to(device)\n",
    "\n",
    "# ONLY FOR DEBUGGING (check if code runs at all)\n",
    "#images = iter(train_loader)\n",
    "##outputs = model(images.next()[0])\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model now...\n",
      "Epoch [10/50], Step [1/334], Loss: 2.0297\n",
      "Epoch [10/50], Step [2/334], Loss: 0.7818\n",
      "Epoch [10/50], Step [3/334], Loss: 1.0585\n",
      "Epoch [10/50], Step [4/334], Loss: 2.6760\n",
      "Epoch [10/50], Step [5/334], Loss: 1.3507\n",
      "Epoch [10/50], Step [6/334], Loss: 1.2408\n",
      "Epoch [10/50], Step [7/334], Loss: 1.0066\n",
      "Epoch [10/50], Step [8/334], Loss: 1.3999\n",
      "Epoch [10/50], Step [9/334], Loss: 1.0400\n",
      "Epoch [10/50], Step [10/334], Loss: 0.9177\n",
      "Epoch [10/50], Step [11/334], Loss: 2.2122\n",
      "Epoch [10/50], Step [12/334], Loss: 1.1162\n",
      "Epoch [10/50], Step [13/334], Loss: 1.7629\n",
      "Epoch [10/50], Step [14/334], Loss: 1.4672\n",
      "Epoch [10/50], Step [15/334], Loss: 1.6047\n",
      "Epoch [10/50], Step [16/334], Loss: 0.9843\n",
      "Epoch [10/50], Step [17/334], Loss: 1.0381\n",
      "Epoch [10/50], Step [18/334], Loss: 1.3783\n",
      "Epoch [10/50], Step [19/334], Loss: 0.4510\n",
      "Epoch [10/50], Step [20/334], Loss: 0.7945\n",
      "Epoch [10/50], Step [21/334], Loss: 1.1612\n",
      "Epoch [10/50], Step [22/334], Loss: 0.5211\n",
      "Epoch [10/50], Step [23/334], Loss: 0.7919\n",
      "Epoch [10/50], Step [24/334], Loss: 1.8253\n",
      "Epoch [10/50], Step [25/334], Loss: 0.7902\n",
      "Epoch [10/50], Step [26/334], Loss: 0.8090\n",
      "Epoch [10/50], Step [27/334], Loss: 1.2893\n",
      "Epoch [10/50], Step [28/334], Loss: 1.1929\n",
      "Epoch [10/50], Step [29/334], Loss: 1.8183\n",
      "Epoch [10/50], Step [30/334], Loss: 2.9502\n",
      "Epoch [10/50], Step [31/334], Loss: 1.3672\n",
      "Epoch [10/50], Step [32/334], Loss: 2.3959\n",
      "Epoch [10/50], Step [33/334], Loss: 1.0775\n",
      "Epoch [10/50], Step [34/334], Loss: 2.1718\n",
      "Epoch [10/50], Step [35/334], Loss: 0.8239\n",
      "Epoch [10/50], Step [36/334], Loss: 1.4074\n",
      "Epoch [10/50], Step [37/334], Loss: 1.3332\n",
      "Epoch [10/50], Step [38/334], Loss: 0.5341\n",
      "Epoch [10/50], Step [39/334], Loss: 1.5179\n",
      "Epoch [10/50], Step [40/334], Loss: 0.7570\n",
      "Epoch [10/50], Step [41/334], Loss: 0.7677\n",
      "Epoch [10/50], Step [42/334], Loss: 0.7395\n",
      "Epoch [10/50], Step [43/334], Loss: 1.1259\n",
      "Epoch [10/50], Step [44/334], Loss: 0.8317\n",
      "Epoch [10/50], Step [45/334], Loss: 1.1359\n",
      "Epoch [10/50], Step [46/334], Loss: 2.3271\n",
      "Epoch [10/50], Step [47/334], Loss: 0.8775\n",
      "Epoch [10/50], Step [48/334], Loss: 8.5336\n",
      "Epoch [10/50], Step [49/334], Loss: 1.7680\n",
      "Epoch [10/50], Step [50/334], Loss: 3.0305\n",
      "Epoch [10/50], Step [51/334], Loss: 3.7172\n",
      "Epoch [10/50], Step [52/334], Loss: 1.2897\n",
      "Epoch [10/50], Step [53/334], Loss: 2.1921\n",
      "Epoch [10/50], Step [54/334], Loss: 3.7833\n",
      "Epoch [10/50], Step [55/334], Loss: 1.1074\n",
      "Epoch [10/50], Step [56/334], Loss: 5.3405\n",
      "Epoch [10/50], Step [57/334], Loss: 2.4557\n",
      "Epoch [10/50], Step [58/334], Loss: 1.4911\n",
      "Epoch [10/50], Step [59/334], Loss: 5.3147\n",
      "Epoch [10/50], Step [60/334], Loss: 2.0635\n",
      "Epoch [10/50], Step [61/334], Loss: 1.6946\n",
      "Epoch [10/50], Step [62/334], Loss: 4.3770\n",
      "Epoch [10/50], Step [63/334], Loss: 1.2697\n",
      "Epoch [10/50], Step [64/334], Loss: 3.0117\n",
      "Epoch [10/50], Step [65/334], Loss: 2.3590\n",
      "Epoch [10/50], Step [66/334], Loss: 1.2069\n",
      "Epoch [10/50], Step [67/334], Loss: 2.6983\n",
      "Epoch [10/50], Step [68/334], Loss: 1.4634\n",
      "Epoch [10/50], Step [69/334], Loss: 1.9074\n",
      "Epoch [10/50], Step [70/334], Loss: 2.6621\n",
      "Epoch [10/50], Step [71/334], Loss: 0.7600\n",
      "Epoch [10/50], Step [72/334], Loss: 1.2692\n",
      "Epoch [10/50], Step [73/334], Loss: 2.0461\n",
      "Epoch [10/50], Step [74/334], Loss: 1.3394\n",
      "Epoch [10/50], Step [75/334], Loss: 2.0467\n",
      "Epoch [10/50], Step [76/334], Loss: 0.9506\n",
      "Epoch [10/50], Step [77/334], Loss: 1.9201\n",
      "Epoch [10/50], Step [78/334], Loss: 0.9487\n",
      "Epoch [10/50], Step [79/334], Loss: 1.9603\n",
      "Epoch [10/50], Step [80/334], Loss: 1.1101\n",
      "Epoch [10/50], Step [81/334], Loss: 2.0534\n",
      "Epoch [10/50], Step [82/334], Loss: 0.8133\n",
      "Epoch [10/50], Step [83/334], Loss: 0.9143\n",
      "Epoch [10/50], Step [84/334], Loss: 1.5832\n",
      "Epoch [10/50], Step [85/334], Loss: 1.7285\n",
      "Epoch [10/50], Step [86/334], Loss: 0.8397\n",
      "Epoch [10/50], Step [87/334], Loss: 3.4436\n",
      "Epoch [10/50], Step [88/334], Loss: 0.5582\n",
      "Epoch [10/50], Step [89/334], Loss: 1.3477\n",
      "Epoch [10/50], Step [90/334], Loss: 0.9550\n",
      "Epoch [10/50], Step [91/334], Loss: 0.8752\n",
      "Epoch [10/50], Step [92/334], Loss: 1.2454\n",
      "Epoch [10/50], Step [93/334], Loss: 1.7252\n",
      "Epoch [10/50], Step [94/334], Loss: 1.1880\n",
      "Epoch [10/50], Step [95/334], Loss: 0.8892\n",
      "Epoch [10/50], Step [96/334], Loss: 1.7660\n",
      "Epoch [10/50], Step [97/334], Loss: 1.2411\n",
      "Epoch [10/50], Step [98/334], Loss: 2.2141\n",
      "Epoch [10/50], Step [99/334], Loss: 1.7181\n",
      "Epoch [10/50], Step [100/334], Loss: 1.7790\n",
      "Epoch [10/50], Step [101/334], Loss: 0.7241\n",
      "Epoch [10/50], Step [102/334], Loss: 1.5292\n",
      "Epoch [10/50], Step [103/334], Loss: 2.1751\n",
      "Epoch [10/50], Step [104/334], Loss: 0.8597\n",
      "Epoch [10/50], Step [105/334], Loss: 1.6863\n",
      "Epoch [10/50], Step [106/334], Loss: 3.3348\n",
      "Epoch [10/50], Step [107/334], Loss: 3.9961\n",
      "Epoch [10/50], Step [108/334], Loss: 2.9595\n",
      "Epoch [10/50], Step [109/334], Loss: 0.5625\n",
      "Epoch [10/50], Step [110/334], Loss: 2.4789\n",
      "Epoch [10/50], Step [111/334], Loss: 2.5148\n",
      "Epoch [10/50], Step [112/334], Loss: 1.4172\n",
      "Epoch [10/50], Step [113/334], Loss: 2.2394\n",
      "Epoch [10/50], Step [114/334], Loss: 1.0622\n",
      "Epoch [10/50], Step [115/334], Loss: 1.6663\n",
      "Epoch [10/50], Step [116/334], Loss: 1.3996\n",
      "Epoch [10/50], Step [117/334], Loss: 0.8663\n",
      "Epoch [10/50], Step [118/334], Loss: 1.9320\n",
      "Epoch [10/50], Step [119/334], Loss: 2.3907\n",
      "Epoch [10/50], Step [120/334], Loss: 1.0591\n",
      "Epoch [10/50], Step [121/334], Loss: 0.4807\n",
      "Epoch [10/50], Step [122/334], Loss: 1.3523\n",
      "Epoch [10/50], Step [123/334], Loss: 1.7963\n",
      "Epoch [10/50], Step [124/334], Loss: 1.5696\n",
      "Epoch [10/50], Step [125/334], Loss: 1.2458\n",
      "Epoch [10/50], Step [126/334], Loss: 1.4760\n",
      "Epoch [10/50], Step [127/334], Loss: 0.9205\n",
      "Epoch [10/50], Step [128/334], Loss: 1.8158\n",
      "Epoch [10/50], Step [129/334], Loss: 1.4307\n",
      "Epoch [10/50], Step [130/334], Loss: 1.2594\n",
      "Epoch [10/50], Step [131/334], Loss: 0.5788\n",
      "Epoch [10/50], Step [132/334], Loss: 1.3297\n",
      "Epoch [10/50], Step [133/334], Loss: 2.3802\n",
      "Epoch [10/50], Step [134/334], Loss: 0.7080\n",
      "Epoch [10/50], Step [135/334], Loss: 1.4080\n",
      "Epoch [10/50], Step [136/334], Loss: 1.9175\n",
      "Epoch [10/50], Step [137/334], Loss: 1.2225\n",
      "Epoch [10/50], Step [138/334], Loss: 1.4271\n",
      "Epoch [10/50], Step [139/334], Loss: 1.9544\n",
      "Epoch [10/50], Step [140/334], Loss: 1.2583\n",
      "Epoch [10/50], Step [141/334], Loss: 2.6776\n",
      "Epoch [10/50], Step [142/334], Loss: 1.7544\n",
      "Epoch [10/50], Step [143/334], Loss: 2.3073\n",
      "Epoch [10/50], Step [144/334], Loss: 1.5258\n",
      "Epoch [10/50], Step [145/334], Loss: 1.2325\n",
      "Epoch [10/50], Step [146/334], Loss: 2.7707\n",
      "Epoch [10/50], Step [147/334], Loss: 1.4115\n",
      "Epoch [10/50], Step [148/334], Loss: 0.8916\n",
      "Epoch [10/50], Step [149/334], Loss: 2.3167\n",
      "Epoch [10/50], Step [150/334], Loss: 2.5832\n",
      "Epoch [10/50], Step [151/334], Loss: 1.8548\n",
      "Epoch [10/50], Step [152/334], Loss: 1.1340\n",
      "Epoch [10/50], Step [153/334], Loss: 1.7218\n",
      "Epoch [10/50], Step [154/334], Loss: 0.8303\n",
      "Epoch [10/50], Step [155/334], Loss: 1.3952\n",
      "Epoch [10/50], Step [156/334], Loss: 2.2122\n",
      "Epoch [10/50], Step [157/334], Loss: 1.6305\n",
      "Epoch [10/50], Step [158/334], Loss: 1.7880\n",
      "Epoch [10/50], Step [159/334], Loss: 1.8494\n",
      "Epoch [10/50], Step [160/334], Loss: 1.8531\n",
      "Epoch [10/50], Step [161/334], Loss: 2.3303\n",
      "Epoch [10/50], Step [162/334], Loss: 1.6030\n",
      "Epoch [10/50], Step [163/334], Loss: 1.2556\n",
      "Epoch [10/50], Step [164/334], Loss: 0.9461\n",
      "Epoch [10/50], Step [165/334], Loss: 0.8071\n",
      "Epoch [10/50], Step [166/334], Loss: 1.1376\n",
      "Epoch [10/50], Step [167/334], Loss: 2.5790\n",
      "Epoch [10/50], Step [168/334], Loss: 1.0335\n",
      "Epoch [10/50], Step [169/334], Loss: 2.9098\n",
      "Epoch [10/50], Step [170/334], Loss: 2.5774\n",
      "Epoch [10/50], Step [171/334], Loss: 1.3758\n",
      "Epoch [10/50], Step [172/334], Loss: 1.1708\n",
      "Epoch [10/50], Step [173/334], Loss: 1.1838\n",
      "Epoch [10/50], Step [174/334], Loss: 1.2452\n",
      "Epoch [10/50], Step [175/334], Loss: 1.1372\n",
      "Epoch [10/50], Step [176/334], Loss: 1.7791\n",
      "Epoch [10/50], Step [177/334], Loss: 1.1581\n",
      "Epoch [10/50], Step [178/334], Loss: 1.0096\n",
      "Epoch [10/50], Step [179/334], Loss: 1.2956\n",
      "Epoch [10/50], Step [180/334], Loss: 1.7764\n",
      "Epoch [10/50], Step [181/334], Loss: 2.9975\n",
      "Epoch [10/50], Step [182/334], Loss: 1.2207\n",
      "Epoch [10/50], Step [183/334], Loss: 1.0786\n",
      "Epoch [10/50], Step [184/334], Loss: 1.4159\n",
      "Epoch [10/50], Step [185/334], Loss: 2.0216\n",
      "Epoch [10/50], Step [186/334], Loss: 1.5297\n",
      "Epoch [10/50], Step [187/334], Loss: 2.8186\n",
      "Epoch [10/50], Step [188/334], Loss: 1.1803\n",
      "Epoch [10/50], Step [189/334], Loss: 4.2050\n",
      "Epoch [10/50], Step [190/334], Loss: 2.1944\n",
      "Epoch [10/50], Step [191/334], Loss: 3.7173\n",
      "Epoch [10/50], Step [192/334], Loss: 2.4246\n",
      "Epoch [10/50], Step [193/334], Loss: 0.5447\n",
      "Epoch [10/50], Step [194/334], Loss: 4.1342\n",
      "Epoch [10/50], Step [195/334], Loss: 1.6323\n",
      "Epoch [10/50], Step [196/334], Loss: 2.2796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Step [197/334], Loss: 5.0373\n",
      "Epoch [10/50], Step [198/334], Loss: 2.6823\n",
      "Epoch [10/50], Step [199/334], Loss: 4.6581\n",
      "Epoch [10/50], Step [200/334], Loss: 2.2041\n",
      "Epoch [10/50], Step [201/334], Loss: 1.6319\n",
      "Epoch [10/50], Step [202/334], Loss: 2.6441\n",
      "Epoch [10/50], Step [203/334], Loss: 2.2685\n",
      "Epoch [10/50], Step [204/334], Loss: 2.5646\n",
      "Epoch [10/50], Step [205/334], Loss: 2.2906\n",
      "Epoch [10/50], Step [206/334], Loss: 2.9783\n",
      "Epoch [10/50], Step [207/334], Loss: 3.3135\n",
      "Epoch [10/50], Step [208/334], Loss: 2.0718\n",
      "Epoch [10/50], Step [209/334], Loss: 1.6880\n",
      "Epoch [10/50], Step [210/334], Loss: 3.5483\n",
      "Epoch [10/50], Step [211/334], Loss: 2.9789\n",
      "Epoch [10/50], Step [212/334], Loss: 2.9672\n",
      "Epoch [10/50], Step [213/334], Loss: 3.4187\n",
      "Epoch [10/50], Step [214/334], Loss: 1.3832\n",
      "Epoch [10/50], Step [215/334], Loss: 3.6287\n",
      "Epoch [10/50], Step [216/334], Loss: 5.2310\n",
      "Epoch [10/50], Step [217/334], Loss: 2.3576\n",
      "Epoch [10/50], Step [218/334], Loss: 2.4773\n",
      "Epoch [10/50], Step [219/334], Loss: 3.4743\n",
      "Epoch [10/50], Step [220/334], Loss: 4.5099\n",
      "Epoch [10/50], Step [221/334], Loss: 1.6553\n",
      "Epoch [10/50], Step [222/334], Loss: 3.8339\n",
      "Epoch [10/50], Step [223/334], Loss: 2.7511\n",
      "Epoch [10/50], Step [224/334], Loss: 1.7032\n",
      "Epoch [10/50], Step [225/334], Loss: 2.0626\n",
      "Epoch [10/50], Step [226/334], Loss: 1.6446\n",
      "Epoch [10/50], Step [227/334], Loss: 2.7786\n",
      "Epoch [10/50], Step [228/334], Loss: 1.7664\n",
      "Epoch [10/50], Step [229/334], Loss: 2.0068\n",
      "Epoch [10/50], Step [230/334], Loss: 1.7075\n",
      "Epoch [10/50], Step [231/334], Loss: 1.6620\n",
      "Epoch [10/50], Step [232/334], Loss: 1.4239\n",
      "Epoch [10/50], Step [233/334], Loss: 2.7493\n",
      "Epoch [10/50], Step [234/334], Loss: 1.9568\n",
      "Epoch [10/50], Step [235/334], Loss: 1.9195\n",
      "Epoch [10/50], Step [236/334], Loss: 1.8272\n",
      "Epoch [10/50], Step [237/334], Loss: 3.2014\n",
      "Epoch [10/50], Step [238/334], Loss: 0.9510\n",
      "Epoch [10/50], Step [239/334], Loss: 1.2779\n",
      "Epoch [10/50], Step [240/334], Loss: 4.5849\n",
      "Epoch [10/50], Step [241/334], Loss: 3.9540\n",
      "Epoch [10/50], Step [242/334], Loss: 4.2713\n",
      "Epoch [10/50], Step [243/334], Loss: 1.5680\n",
      "Epoch [10/50], Step [244/334], Loss: 5.1653\n",
      "Epoch [10/50], Step [245/334], Loss: 2.4674\n",
      "Epoch [10/50], Step [246/334], Loss: 2.6555\n",
      "Epoch [10/50], Step [247/334], Loss: 3.1288\n",
      "Epoch [10/50], Step [248/334], Loss: 2.6876\n",
      "Epoch [10/50], Step [249/334], Loss: 3.0063\n",
      "Epoch [10/50], Step [250/334], Loss: 2.4598\n",
      "Epoch [10/50], Step [251/334], Loss: 1.5773\n",
      "Epoch [10/50], Step [252/334], Loss: 1.5633\n",
      "Epoch [10/50], Step [253/334], Loss: 5.5435\n",
      "Epoch [10/50], Step [254/334], Loss: 2.2081\n",
      "Epoch [10/50], Step [255/334], Loss: 4.6831\n",
      "Epoch [10/50], Step [256/334], Loss: 3.0660\n",
      "Epoch [10/50], Step [257/334], Loss: 3.8819\n",
      "Epoch [10/50], Step [258/334], Loss: 3.9721\n",
      "Epoch [10/50], Step [259/334], Loss: 1.3863\n",
      "Epoch [10/50], Step [260/334], Loss: 3.8966\n",
      "Epoch [10/50], Step [261/334], Loss: 2.2304\n",
      "Epoch [10/50], Step [262/334], Loss: 1.7873\n",
      "Epoch [10/50], Step [263/334], Loss: 2.1183\n",
      "Epoch [10/50], Step [264/334], Loss: 1.5608\n",
      "Epoch [10/50], Step [265/334], Loss: 1.9166\n",
      "Epoch [10/50], Step [266/334], Loss: 2.2750\n",
      "Epoch [10/50], Step [267/334], Loss: 3.3009\n",
      "Epoch [10/50], Step [268/334], Loss: 1.0983\n",
      "Epoch [10/50], Step [269/334], Loss: 3.5034\n",
      "Epoch [10/50], Step [270/334], Loss: 1.3297\n",
      "Epoch [10/50], Step [271/334], Loss: 1.1565\n",
      "Epoch [10/50], Step [272/334], Loss: 1.6063\n",
      "Epoch [10/50], Step [273/334], Loss: 1.2011\n",
      "Epoch [10/50], Step [274/334], Loss: 3.1232\n",
      "Epoch [10/50], Step [275/334], Loss: 0.9813\n",
      "Epoch [10/50], Step [276/334], Loss: 2.7151\n",
      "Epoch [10/50], Step [277/334], Loss: 4.1782\n",
      "Epoch [10/50], Step [278/334], Loss: 2.0439\n",
      "Epoch [10/50], Step [279/334], Loss: 4.1700\n",
      "Epoch [10/50], Step [280/334], Loss: 2.8995\n",
      "Epoch [10/50], Step [281/334], Loss: 1.6024\n",
      "Epoch [10/50], Step [282/334], Loss: 6.9608\n",
      "Epoch [10/50], Step [283/334], Loss: 2.4433\n",
      "Epoch [10/50], Step [284/334], Loss: 5.4694\n",
      "Epoch [10/50], Step [285/334], Loss: 2.8280\n",
      "Epoch [10/50], Step [286/334], Loss: 4.1714\n",
      "Epoch [10/50], Step [287/334], Loss: 1.0294\n",
      "Epoch [10/50], Step [288/334], Loss: 2.9574\n",
      "Epoch [10/50], Step [289/334], Loss: 2.4693\n",
      "Epoch [10/50], Step [290/334], Loss: 2.2412\n",
      "Epoch [10/50], Step [291/334], Loss: 1.2511\n",
      "Epoch [10/50], Step [292/334], Loss: 1.6345\n",
      "Epoch [10/50], Step [293/334], Loss: 2.2480\n",
      "Epoch [10/50], Step [294/334], Loss: 0.8140\n",
      "Epoch [10/50], Step [295/334], Loss: 2.5778\n",
      "Epoch [10/50], Step [296/334], Loss: 1.4607\n",
      "Epoch [10/50], Step [297/334], Loss: 2.1760\n",
      "Epoch [10/50], Step [298/334], Loss: 3.0477\n",
      "Epoch [10/50], Step [299/334], Loss: 2.9279\n",
      "Epoch [10/50], Step [300/334], Loss: 2.5207\n",
      "Epoch [10/50], Step [301/334], Loss: 1.1327\n",
      "Epoch [10/50], Step [302/334], Loss: 2.0747\n",
      "Epoch [10/50], Step [303/334], Loss: 2.7679\n",
      "Epoch [10/50], Step [304/334], Loss: 1.5171\n",
      "Epoch [10/50], Step [305/334], Loss: 2.5566\n",
      "Epoch [10/50], Step [306/334], Loss: 1.4430\n",
      "Epoch [10/50], Step [307/334], Loss: 1.9110\n",
      "Epoch [10/50], Step [308/334], Loss: 2.6253\n",
      "Epoch [10/50], Step [309/334], Loss: 1.3027\n",
      "Epoch [10/50], Step [310/334], Loss: 2.8095\n",
      "Epoch [10/50], Step [311/334], Loss: 1.9251\n",
      "Epoch [10/50], Step [312/334], Loss: 3.1271\n",
      "Epoch [10/50], Step [313/334], Loss: 2.0496\n",
      "Epoch [10/50], Step [314/334], Loss: 1.4390\n",
      "Epoch [10/50], Step [315/334], Loss: 1.6576\n",
      "Epoch [10/50], Step [316/334], Loss: 2.2485\n",
      "Epoch [10/50], Step [317/334], Loss: 1.6682\n",
      "Epoch [10/50], Step [318/334], Loss: 1.7254\n",
      "Epoch [10/50], Step [319/334], Loss: 3.6396\n",
      "Epoch [10/50], Step [320/334], Loss: 2.0130\n",
      "Epoch [10/50], Step [321/334], Loss: 2.9987\n",
      "Epoch [10/50], Step [322/334], Loss: 0.8670\n",
      "Epoch [10/50], Step [323/334], Loss: 2.4288\n",
      "Epoch [10/50], Step [324/334], Loss: 3.7763\n",
      "Epoch [10/50], Step [325/334], Loss: 4.0634\n",
      "Epoch [10/50], Step [326/334], Loss: 6.1399\n",
      "Epoch [10/50], Step [327/334], Loss: 0.9582\n",
      "Epoch [10/50], Step [328/334], Loss: 5.7041\n",
      "Epoch [10/50], Step [329/334], Loss: 2.5457\n",
      "Epoch [10/50], Step [330/334], Loss: 1.3599\n",
      "Epoch [10/50], Step [331/334], Loss: 7.3481\n",
      "Epoch [10/50], Step [332/334], Loss: 2.1708\n",
      "Epoch [10/50], Step [333/334], Loss: 4.1253\n",
      "Epoch [10/50], Step [334/334], Loss: 2.0577\n",
      "Epoch [20/50], Step [1/334], Loss: 1.8072\n",
      "Epoch [20/50], Step [2/334], Loss: 3.7653\n",
      "Epoch [20/50], Step [3/334], Loss: 1.1062\n",
      "Epoch [20/50], Step [4/334], Loss: 5.6261\n",
      "Epoch [20/50], Step [5/334], Loss: 1.7126\n",
      "Epoch [20/50], Step [6/334], Loss: 2.5695\n",
      "Epoch [20/50], Step [7/334], Loss: 1.7632\n",
      "Epoch [20/50], Step [8/334], Loss: 1.7473\n",
      "Epoch [20/50], Step [9/334], Loss: 1.4583\n",
      "Epoch [20/50], Step [10/334], Loss: 3.5928\n",
      "Epoch [20/50], Step [11/334], Loss: 1.2530\n",
      "Epoch [20/50], Step [12/334], Loss: 2.3073\n",
      "Epoch [20/50], Step [13/334], Loss: 3.9226\n",
      "Epoch [20/50], Step [14/334], Loss: 1.6142\n",
      "Epoch [20/50], Step [15/334], Loss: 3.5000\n",
      "Epoch [20/50], Step [16/334], Loss: 2.1820\n",
      "Epoch [20/50], Step [17/334], Loss: 3.0053\n",
      "Epoch [20/50], Step [18/334], Loss: 2.0851\n",
      "Epoch [20/50], Step [19/334], Loss: 1.6813\n",
      "Epoch [20/50], Step [20/334], Loss: 0.9280\n",
      "Epoch [20/50], Step [21/334], Loss: 2.3788\n",
      "Epoch [20/50], Step [22/334], Loss: 1.2759\n",
      "Epoch [20/50], Step [23/334], Loss: 2.6616\n",
      "Epoch [20/50], Step [24/334], Loss: 2.4594\n",
      "Epoch [20/50], Step [25/334], Loss: 2.0499\n",
      "Epoch [20/50], Step [26/334], Loss: 2.6249\n",
      "Epoch [20/50], Step [27/334], Loss: 0.9651\n",
      "Epoch [20/50], Step [28/334], Loss: 1.8849\n",
      "Epoch [20/50], Step [29/334], Loss: 1.5060\n",
      "Epoch [20/50], Step [30/334], Loss: 1.1435\n",
      "Epoch [20/50], Step [31/334], Loss: 2.5577\n",
      "Epoch [20/50], Step [32/334], Loss: 1.5924\n",
      "Epoch [20/50], Step [33/334], Loss: 1.4584\n",
      "Epoch [20/50], Step [34/334], Loss: 1.3205\n",
      "Epoch [20/50], Step [35/334], Loss: 1.1851\n",
      "Epoch [20/50], Step [36/334], Loss: 2.1763\n",
      "Epoch [20/50], Step [37/334], Loss: 3.8272\n",
      "Epoch [20/50], Step [38/334], Loss: 1.6071\n",
      "Epoch [20/50], Step [39/334], Loss: 1.3678\n",
      "Epoch [20/50], Step [40/334], Loss: 1.3545\n",
      "Epoch [20/50], Step [41/334], Loss: 1.1837\n",
      "Epoch [20/50], Step [42/334], Loss: 1.0445\n",
      "Epoch [20/50], Step [43/334], Loss: 1.2781\n",
      "Epoch [20/50], Step [44/334], Loss: 1.0977\n",
      "Epoch [20/50], Step [45/334], Loss: 1.9851\n",
      "Epoch [20/50], Step [46/334], Loss: 0.8420\n",
      "Epoch [20/50], Step [47/334], Loss: 0.6795\n",
      "Epoch [20/50], Step [48/334], Loss: 1.0655\n",
      "Epoch [20/50], Step [49/334], Loss: 1.1579\n",
      "Epoch [20/50], Step [50/334], Loss: 1.4240\n",
      "Epoch [20/50], Step [51/334], Loss: 2.4520\n",
      "Epoch [20/50], Step [52/334], Loss: 1.2059\n",
      "Epoch [20/50], Step [53/334], Loss: 2.6763\n",
      "Epoch [20/50], Step [54/334], Loss: 1.8679\n",
      "Epoch [20/50], Step [55/334], Loss: 1.3694\n",
      "Epoch [20/50], Step [56/334], Loss: 4.8642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/50], Step [57/334], Loss: 1.5699\n",
      "Epoch [20/50], Step [58/334], Loss: 3.2554\n",
      "Epoch [20/50], Step [59/334], Loss: 1.1570\n",
      "Epoch [20/50], Step [60/334], Loss: 0.8791\n",
      "Epoch [20/50], Step [61/334], Loss: 4.4398\n",
      "Epoch [20/50], Step [62/334], Loss: 1.0555\n",
      "Epoch [20/50], Step [63/334], Loss: 4.0892\n",
      "Epoch [20/50], Step [64/334], Loss: 1.0471\n",
      "Epoch [20/50], Step [65/334], Loss: 4.1828\n",
      "Epoch [20/50], Step [66/334], Loss: 2.2109\n",
      "Epoch [20/50], Step [67/334], Loss: 3.4989\n",
      "Epoch [20/50], Step [68/334], Loss: 2.9266\n",
      "Epoch [20/50], Step [69/334], Loss: 1.5717\n",
      "Epoch [20/50], Step [70/334], Loss: 4.9216\n",
      "Epoch [20/50], Step [71/334], Loss: 0.8355\n",
      "Epoch [20/50], Step [72/334], Loss: 0.9503\n",
      "Epoch [20/50], Step [73/334], Loss: 2.2683\n",
      "Epoch [20/50], Step [74/334], Loss: 1.1647\n",
      "Epoch [20/50], Step [75/334], Loss: 2.9187\n",
      "Epoch [20/50], Step [76/334], Loss: 1.7712\n",
      "Epoch [20/50], Step [77/334], Loss: 1.8992\n",
      "Epoch [20/50], Step [78/334], Loss: 1.4809\n",
      "Epoch [20/50], Step [79/334], Loss: 1.1028\n",
      "Epoch [20/50], Step [80/334], Loss: 3.4846\n",
      "Epoch [20/50], Step [81/334], Loss: 1.5477\n",
      "Epoch [20/50], Step [82/334], Loss: 2.6579\n",
      "Epoch [20/50], Step [83/334], Loss: 1.4293\n",
      "Epoch [20/50], Step [84/334], Loss: 1.9792\n",
      "Epoch [20/50], Step [85/334], Loss: 1.4381\n",
      "Epoch [20/50], Step [86/334], Loss: 1.2093\n",
      "Epoch [20/50], Step [87/334], Loss: 2.5595\n",
      "Epoch [20/50], Step [88/334], Loss: 1.1055\n",
      "Epoch [20/50], Step [89/334], Loss: 1.2263\n",
      "Epoch [20/50], Step [90/334], Loss: 1.2134\n",
      "Epoch [20/50], Step [91/334], Loss: 3.4163\n",
      "Epoch [20/50], Step [92/334], Loss: 1.7215\n",
      "Epoch [20/50], Step [93/334], Loss: 2.0363\n",
      "Epoch [20/50], Step [94/334], Loss: 2.6983\n",
      "Epoch [20/50], Step [95/334], Loss: 2.4833\n",
      "Epoch [20/50], Step [96/334], Loss: 3.8882\n",
      "Epoch [20/50], Step [97/334], Loss: 1.8129\n",
      "Epoch [20/50], Step [98/334], Loss: 1.9479\n",
      "Epoch [20/50], Step [99/334], Loss: 0.9276\n",
      "Epoch [20/50], Step [100/334], Loss: 1.4678\n",
      "Epoch [20/50], Step [101/334], Loss: 1.0016\n",
      "Epoch [20/50], Step [102/334], Loss: 0.8336\n",
      "Epoch [20/50], Step [103/334], Loss: 1.4331\n",
      "Epoch [20/50], Step [104/334], Loss: 2.1210\n",
      "Epoch [20/50], Step [105/334], Loss: 1.7808\n",
      "Epoch [20/50], Step [106/334], Loss: 1.4658\n",
      "Epoch [20/50], Step [107/334], Loss: 1.7122\n",
      "Epoch [20/50], Step [108/334], Loss: 1.8738\n",
      "Epoch [20/50], Step [109/334], Loss: 0.8241\n",
      "Epoch [20/50], Step [110/334], Loss: 2.4122\n",
      "Epoch [20/50], Step [111/334], Loss: 1.2639\n",
      "Epoch [20/50], Step [112/334], Loss: 1.1879\n",
      "Epoch [20/50], Step [113/334], Loss: 2.5184\n",
      "Epoch [20/50], Step [114/334], Loss: 1.4746\n",
      "Epoch [20/50], Step [115/334], Loss: 2.5483\n",
      "Epoch [20/50], Step [116/334], Loss: 1.3199\n",
      "Epoch [20/50], Step [117/334], Loss: 2.6128\n",
      "Epoch [20/50], Step [118/334], Loss: 1.3531\n",
      "Epoch [20/50], Step [119/334], Loss: 2.7296\n",
      "Epoch [20/50], Step [120/334], Loss: 1.9374\n",
      "Epoch [20/50], Step [121/334], Loss: 1.4783\n",
      "Epoch [20/50], Step [122/334], Loss: 1.1993\n",
      "Epoch [20/50], Step [123/334], Loss: 1.6552\n",
      "Epoch [20/50], Step [124/334], Loss: 1.7629\n",
      "Epoch [20/50], Step [125/334], Loss: 0.6679\n",
      "Epoch [20/50], Step [126/334], Loss: 1.1619\n",
      "Epoch [20/50], Step [127/334], Loss: 1.0101\n",
      "Epoch [20/50], Step [128/334], Loss: 0.8191\n",
      "Epoch [20/50], Step [129/334], Loss: 1.2070\n",
      "Epoch [20/50], Step [130/334], Loss: 2.0784\n",
      "Epoch [20/50], Step [131/334], Loss: 1.5782\n",
      "Epoch [20/50], Step [132/334], Loss: 1.3231\n",
      "Epoch [20/50], Step [133/334], Loss: 0.8747\n",
      "Epoch [20/50], Step [134/334], Loss: 0.9658\n",
      "Epoch [20/50], Step [135/334], Loss: 0.6695\n",
      "Epoch [20/50], Step [136/334], Loss: 0.8744\n",
      "Epoch [20/50], Step [137/334], Loss: 0.6619\n",
      "Epoch [20/50], Step [138/334], Loss: 1.3456\n",
      "Epoch [20/50], Step [139/334], Loss: 0.5667\n",
      "Epoch [20/50], Step [140/334], Loss: 1.0551\n",
      "Epoch [20/50], Step [141/334], Loss: 0.7246\n",
      "Epoch [20/50], Step [142/334], Loss: 0.6812\n",
      "Epoch [20/50], Step [143/334], Loss: 1.1777\n",
      "Epoch [20/50], Step [144/334], Loss: 0.7592\n",
      "Epoch [20/50], Step [145/334], Loss: 0.4033\n",
      "Epoch [20/50], Step [146/334], Loss: 0.7453\n",
      "Epoch [20/50], Step [147/334], Loss: 1.6597\n",
      "Epoch [20/50], Step [148/334], Loss: 0.9371\n",
      "Epoch [20/50], Step [149/334], Loss: 1.1456\n",
      "Epoch [20/50], Step [150/334], Loss: 1.1959\n",
      "Epoch [20/50], Step [151/334], Loss: 0.6967\n",
      "Epoch [20/50], Step [152/334], Loss: 0.7435\n",
      "Epoch [20/50], Step [153/334], Loss: 1.8036\n",
      "Epoch [20/50], Step [154/334], Loss: 0.7983\n",
      "Epoch [20/50], Step [155/334], Loss: 1.2358\n",
      "Epoch [20/50], Step [156/334], Loss: 1.4328\n",
      "Epoch [20/50], Step [157/334], Loss: 2.1837\n",
      "Epoch [20/50], Step [158/334], Loss: 0.8848\n",
      "Epoch [20/50], Step [159/334], Loss: 2.1234\n",
      "Epoch [20/50], Step [160/334], Loss: 1.1195\n",
      "Epoch [20/50], Step [161/334], Loss: 0.8743\n",
      "Epoch [20/50], Step [162/334], Loss: 1.1202\n",
      "Epoch [20/50], Step [163/334], Loss: 0.7933\n",
      "Epoch [20/50], Step [164/334], Loss: 0.9683\n",
      "Epoch [20/50], Step [165/334], Loss: 1.9113\n",
      "Epoch [20/50], Step [166/334], Loss: 0.6969\n",
      "Epoch [20/50], Step [167/334], Loss: 1.7923\n",
      "Epoch [20/50], Step [168/334], Loss: 0.7238\n",
      "Epoch [20/50], Step [169/334], Loss: 1.5876\n",
      "Epoch [20/50], Step [170/334], Loss: 0.8557\n",
      "Epoch [20/50], Step [171/334], Loss: 1.1433\n",
      "Epoch [20/50], Step [172/334], Loss: 1.6987\n",
      "Epoch [20/50], Step [173/334], Loss: 1.2068\n",
      "Epoch [20/50], Step [174/334], Loss: 1.8202\n",
      "Epoch [20/50], Step [175/334], Loss: 1.4519\n",
      "Epoch [20/50], Step [176/334], Loss: 1.4273\n",
      "Epoch [20/50], Step [177/334], Loss: 1.0938\n",
      "Epoch [20/50], Step [178/334], Loss: 2.0204\n",
      "Epoch [20/50], Step [179/334], Loss: 1.0224\n",
      "Epoch [20/50], Step [180/334], Loss: 1.8538\n",
      "Epoch [20/50], Step [181/334], Loss: 2.5617\n",
      "Epoch [20/50], Step [182/334], Loss: 1.4048\n",
      "Epoch [20/50], Step [183/334], Loss: 1.6099\n",
      "Epoch [20/50], Step [184/334], Loss: 1.7706\n",
      "Epoch [20/50], Step [185/334], Loss: 1.9396\n",
      "Epoch [20/50], Step [186/334], Loss: 1.2596\n",
      "Epoch [20/50], Step [187/334], Loss: 0.6826\n",
      "Epoch [20/50], Step [188/334], Loss: 1.8571\n",
      "Epoch [20/50], Step [189/334], Loss: 1.0813\n",
      "Epoch [20/50], Step [190/334], Loss: 1.7148\n",
      "Epoch [20/50], Step [191/334], Loss: 1.5243\n",
      "Epoch [20/50], Step [192/334], Loss: 0.6739\n",
      "Epoch [20/50], Step [193/334], Loss: 1.6383\n",
      "Epoch [20/50], Step [194/334], Loss: 1.2507\n",
      "Epoch [20/50], Step [195/334], Loss: 1.1271\n",
      "Epoch [20/50], Step [196/334], Loss: 2.1953\n",
      "Epoch [20/50], Step [197/334], Loss: 2.5068\n",
      "Epoch [20/50], Step [198/334], Loss: 0.9629\n",
      "Epoch [20/50], Step [199/334], Loss: 1.0332\n",
      "Epoch [20/50], Step [200/334], Loss: 2.4901\n",
      "Epoch [20/50], Step [201/334], Loss: 2.2548\n",
      "Epoch [20/50], Step [202/334], Loss: 2.2206\n",
      "Epoch [20/50], Step [203/334], Loss: 0.8512\n",
      "Epoch [20/50], Step [204/334], Loss: 2.6351\n",
      "Epoch [20/50], Step [205/334], Loss: 0.6325\n",
      "Epoch [20/50], Step [206/334], Loss: 1.6395\n",
      "Epoch [20/50], Step [207/334], Loss: 1.8796\n",
      "Epoch [20/50], Step [208/334], Loss: 2.0503\n",
      "Epoch [20/50], Step [209/334], Loss: 1.0460\n",
      "Epoch [20/50], Step [210/334], Loss: 5.2565\n",
      "Epoch [20/50], Step [211/334], Loss: 7.1038\n",
      "Epoch [20/50], Step [212/334], Loss: 2.3026\n",
      "Epoch [20/50], Step [213/334], Loss: 2.9577\n",
      "Epoch [20/50], Step [214/334], Loss: 1.5522\n",
      "Epoch [20/50], Step [215/334], Loss: 0.8937\n",
      "Epoch [20/50], Step [216/334], Loss: 1.8737\n",
      "Epoch [20/50], Step [217/334], Loss: 2.0826\n",
      "Epoch [20/50], Step [218/334], Loss: 1.8785\n",
      "Epoch [20/50], Step [219/334], Loss: 0.8533\n",
      "Epoch [20/50], Step [220/334], Loss: 1.3674\n",
      "Epoch [20/50], Step [221/334], Loss: 2.9812\n",
      "Epoch [20/50], Step [222/334], Loss: 0.8460\n",
      "Epoch [20/50], Step [223/334], Loss: 2.8926\n",
      "Epoch [20/50], Step [224/334], Loss: 1.3520\n",
      "Epoch [20/50], Step [225/334], Loss: 1.0658\n",
      "Epoch [20/50], Step [226/334], Loss: 3.0303\n",
      "Epoch [20/50], Step [227/334], Loss: 1.6296\n",
      "Epoch [20/50], Step [228/334], Loss: 0.9753\n",
      "Epoch [20/50], Step [229/334], Loss: 2.6210\n",
      "Epoch [20/50], Step [230/334], Loss: 1.7789\n",
      "Epoch [20/50], Step [231/334], Loss: 2.0459\n",
      "Epoch [20/50], Step [232/334], Loss: 1.9430\n",
      "Epoch [20/50], Step [233/334], Loss: 0.8836\n",
      "Epoch [20/50], Step [234/334], Loss: 0.9206\n",
      "Epoch [20/50], Step [235/334], Loss: 1.3114\n",
      "Epoch [20/50], Step [236/334], Loss: 0.7521\n",
      "Epoch [20/50], Step [237/334], Loss: 0.8878\n",
      "Epoch [20/50], Step [238/334], Loss: 1.0115\n",
      "Epoch [20/50], Step [239/334], Loss: 1.2928\n",
      "Epoch [20/50], Step [240/334], Loss: 1.3599\n",
      "Epoch [20/50], Step [241/334], Loss: 1.2217\n",
      "Epoch [20/50], Step [242/334], Loss: 0.9380\n",
      "Epoch [20/50], Step [243/334], Loss: 0.8321\n",
      "Epoch [20/50], Step [244/334], Loss: 0.8800\n",
      "Epoch [20/50], Step [245/334], Loss: 1.2039\n",
      "Epoch [20/50], Step [246/334], Loss: 0.5921\n",
      "Epoch [20/50], Step [247/334], Loss: 0.8539\n",
      "Epoch [20/50], Step [248/334], Loss: 1.0352\n",
      "Epoch [20/50], Step [249/334], Loss: 1.3897\n",
      "Epoch [20/50], Step [250/334], Loss: 0.7522\n",
      "Epoch [20/50], Step [251/334], Loss: 0.9219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/50], Step [252/334], Loss: 1.5426\n",
      "Epoch [20/50], Step [253/334], Loss: 0.8089\n",
      "Epoch [20/50], Step [254/334], Loss: 1.7047\n",
      "Epoch [20/50], Step [255/334], Loss: 0.7444\n",
      "Epoch [20/50], Step [256/334], Loss: 1.9034\n",
      "Epoch [20/50], Step [257/334], Loss: 1.1979\n",
      "Epoch [20/50], Step [258/334], Loss: 1.1130\n",
      "Epoch [20/50], Step [259/334], Loss: 1.8602\n",
      "Epoch [20/50], Step [260/334], Loss: 1.3783\n",
      "Epoch [20/50], Step [261/334], Loss: 1.3586\n",
      "Epoch [20/50], Step [262/334], Loss: 1.4355\n",
      "Epoch [20/50], Step [263/334], Loss: 0.7668\n",
      "Epoch [20/50], Step [264/334], Loss: 1.3241\n",
      "Epoch [20/50], Step [265/334], Loss: 0.7213\n",
      "Epoch [20/50], Step [266/334], Loss: 1.0572\n",
      "Epoch [20/50], Step [267/334], Loss: 0.8504\n",
      "Epoch [20/50], Step [268/334], Loss: 1.0564\n",
      "Epoch [20/50], Step [269/334], Loss: 0.7334\n",
      "Epoch [20/50], Step [270/334], Loss: 0.5626\n",
      "Epoch [20/50], Step [271/334], Loss: 1.4984\n",
      "Epoch [20/50], Step [272/334], Loss: 0.8470\n",
      "Epoch [20/50], Step [273/334], Loss: 1.4180\n",
      "Epoch [20/50], Step [274/334], Loss: 0.6696\n",
      "Epoch [20/50], Step [275/334], Loss: 1.4475\n",
      "Epoch [20/50], Step [276/334], Loss: 0.7349\n",
      "Epoch [20/50], Step [277/334], Loss: 1.1399\n",
      "Epoch [20/50], Step [278/334], Loss: 0.8542\n",
      "Epoch [20/50], Step [279/334], Loss: 0.9103\n",
      "Epoch [20/50], Step [280/334], Loss: 1.1032\n",
      "Epoch [20/50], Step [281/334], Loss: 1.1085\n",
      "Epoch [20/50], Step [282/334], Loss: 0.4271\n",
      "Epoch [20/50], Step [283/334], Loss: 0.6939\n",
      "Epoch [20/50], Step [284/334], Loss: 1.0925\n",
      "Epoch [20/50], Step [285/334], Loss: 0.6227\n",
      "Epoch [20/50], Step [286/334], Loss: 0.4973\n",
      "Epoch [20/50], Step [287/334], Loss: 1.5395\n",
      "Epoch [20/50], Step [288/334], Loss: 1.5318\n",
      "Epoch [20/50], Step [289/334], Loss: 0.9706\n",
      "Epoch [20/50], Step [290/334], Loss: 1.2793\n",
      "Epoch [20/50], Step [291/334], Loss: 0.5907\n",
      "Epoch [20/50], Step [292/334], Loss: 0.5850\n",
      "Epoch [20/50], Step [293/334], Loss: 1.3539\n",
      "Epoch [20/50], Step [294/334], Loss: 0.7452\n",
      "Epoch [20/50], Step [295/334], Loss: 1.1110\n",
      "Epoch [20/50], Step [296/334], Loss: 1.2507\n",
      "Epoch [20/50], Step [297/334], Loss: 1.0728\n",
      "Epoch [20/50], Step [298/334], Loss: 0.6339\n",
      "Epoch [20/50], Step [299/334], Loss: 1.1742\n",
      "Epoch [20/50], Step [300/334], Loss: 1.6621\n",
      "Epoch [20/50], Step [301/334], Loss: 2.8067\n",
      "Epoch [20/50], Step [302/334], Loss: 1.7867\n",
      "Epoch [20/50], Step [303/334], Loss: 2.5112\n",
      "Epoch [20/50], Step [304/334], Loss: 1.7844\n",
      "Epoch [20/50], Step [305/334], Loss: 0.5568\n",
      "Epoch [20/50], Step [306/334], Loss: 2.6142\n",
      "Epoch [20/50], Step [307/334], Loss: 2.3157\n",
      "Epoch [20/50], Step [308/334], Loss: 0.9827\n",
      "Epoch [20/50], Step [309/334], Loss: 2.0290\n",
      "Epoch [20/50], Step [310/334], Loss: 1.4487\n",
      "Epoch [20/50], Step [311/334], Loss: 3.3383\n",
      "Epoch [20/50], Step [312/334], Loss: 1.1851\n",
      "Epoch [20/50], Step [313/334], Loss: 1.5630\n",
      "Epoch [20/50], Step [314/334], Loss: 5.3141\n",
      "Epoch [20/50], Step [315/334], Loss: 1.1932\n",
      "Epoch [20/50], Step [316/334], Loss: 2.2966\n",
      "Epoch [20/50], Step [317/334], Loss: 2.9452\n",
      "Epoch [20/50], Step [318/334], Loss: 1.9527\n",
      "Epoch [20/50], Step [319/334], Loss: 1.8726\n",
      "Epoch [20/50], Step [320/334], Loss: 3.9417\n",
      "Epoch [20/50], Step [321/334], Loss: 3.7335\n",
      "Epoch [20/50], Step [322/334], Loss: 1.7131\n",
      "Epoch [20/50], Step [323/334], Loss: 2.1532\n",
      "Epoch [20/50], Step [324/334], Loss: 2.0658\n",
      "Epoch [20/50], Step [325/334], Loss: 1.6895\n",
      "Epoch [20/50], Step [326/334], Loss: 1.0901\n",
      "Epoch [20/50], Step [327/334], Loss: 2.6354\n",
      "Epoch [20/50], Step [328/334], Loss: 2.0089\n",
      "Epoch [20/50], Step [329/334], Loss: 3.8024\n",
      "Epoch [20/50], Step [330/334], Loss: 1.6211\n",
      "Epoch [20/50], Step [331/334], Loss: 1.9387\n",
      "Epoch [20/50], Step [332/334], Loss: 2.0016\n",
      "Epoch [20/50], Step [333/334], Loss: 0.9160\n",
      "Epoch [20/50], Step [334/334], Loss: 1.8298\n",
      "Epoch [30/50], Step [1/334], Loss: 1.3042\n",
      "Epoch [30/50], Step [2/334], Loss: 0.7922\n",
      "Epoch [30/50], Step [3/334], Loss: 1.4535\n",
      "Epoch [30/50], Step [4/334], Loss: 0.6994\n",
      "Epoch [30/50], Step [5/334], Loss: 0.8309\n",
      "Epoch [30/50], Step [6/334], Loss: 1.5402\n",
      "Epoch [30/50], Step [7/334], Loss: 0.6361\n",
      "Epoch [30/50], Step [8/334], Loss: 1.4962\n",
      "Epoch [30/50], Step [9/334], Loss: 0.8715\n",
      "Epoch [30/50], Step [10/334], Loss: 0.7314\n",
      "Epoch [30/50], Step [11/334], Loss: 1.3868\n",
      "Epoch [30/50], Step [12/334], Loss: 0.4835\n",
      "Epoch [30/50], Step [13/334], Loss: 1.4996\n",
      "Epoch [30/50], Step [14/334], Loss: 0.7347\n",
      "Epoch [30/50], Step [15/334], Loss: 0.5550\n",
      "Epoch [30/50], Step [16/334], Loss: 1.3983\n",
      "Epoch [30/50], Step [17/334], Loss: 1.2377\n",
      "Epoch [30/50], Step [18/334], Loss: 0.6274\n",
      "Epoch [30/50], Step [19/334], Loss: 1.3715\n",
      "Epoch [30/50], Step [20/334], Loss: 1.0036\n",
      "Epoch [30/50], Step [21/334], Loss: 0.8110\n",
      "Epoch [30/50], Step [22/334], Loss: 0.8144\n",
      "Epoch [30/50], Step [23/334], Loss: 0.8917\n",
      "Epoch [30/50], Step [24/334], Loss: 1.8414\n",
      "Epoch [30/50], Step [25/334], Loss: 1.0883\n",
      "Epoch [30/50], Step [26/334], Loss: 0.9427\n",
      "Epoch [30/50], Step [27/334], Loss: 0.5656\n",
      "Epoch [30/50], Step [28/334], Loss: 0.7209\n",
      "Epoch [30/50], Step [29/334], Loss: 1.2170\n",
      "Epoch [30/50], Step [30/334], Loss: 1.1953\n",
      "Epoch [30/50], Step [31/334], Loss: 0.9182\n",
      "Epoch [30/50], Step [32/334], Loss: 0.8995\n",
      "Epoch [30/50], Step [33/334], Loss: 0.7111\n",
      "Epoch [30/50], Step [34/334], Loss: 1.3359\n",
      "Epoch [30/50], Step [35/334], Loss: 0.4686\n",
      "Epoch [30/50], Step [36/334], Loss: 0.9457\n",
      "Epoch [30/50], Step [37/334], Loss: 0.9402\n",
      "Epoch [30/50], Step [38/334], Loss: 0.9067\n",
      "Epoch [30/50], Step [39/334], Loss: 0.8810\n",
      "Epoch [30/50], Step [40/334], Loss: 1.0215\n",
      "Epoch [30/50], Step [41/334], Loss: 2.3177\n",
      "Epoch [30/50], Step [42/334], Loss: 0.9199\n",
      "Epoch [30/50], Step [43/334], Loss: 0.9854\n",
      "Epoch [30/50], Step [44/334], Loss: 1.1421\n",
      "Epoch [30/50], Step [45/334], Loss: 0.9283\n",
      "Epoch [30/50], Step [46/334], Loss: 0.7810\n",
      "Epoch [30/50], Step [47/334], Loss: 1.3288\n",
      "Epoch [30/50], Step [48/334], Loss: 0.5400\n",
      "Epoch [30/50], Step [49/334], Loss: 0.8219\n",
      "Epoch [30/50], Step [50/334], Loss: 1.2988\n",
      "Epoch [30/50], Step [51/334], Loss: 0.8092\n",
      "Epoch [30/50], Step [52/334], Loss: 0.4654\n",
      "Epoch [30/50], Step [53/334], Loss: 0.8738\n",
      "Epoch [30/50], Step [54/334], Loss: 1.0340\n",
      "Epoch [30/50], Step [55/334], Loss: 0.8600\n",
      "Epoch [30/50], Step [56/334], Loss: 0.6625\n",
      "Epoch [30/50], Step [57/334], Loss: 0.5218\n",
      "Epoch [30/50], Step [58/334], Loss: 0.6106\n",
      "Epoch [30/50], Step [59/334], Loss: 0.6206\n",
      "Epoch [30/50], Step [60/334], Loss: 0.3314\n",
      "Epoch [30/50], Step [61/334], Loss: 0.4480\n",
      "Epoch [30/50], Step [62/334], Loss: 0.5409\n",
      "Epoch [30/50], Step [63/334], Loss: 0.4926\n",
      "Epoch [30/50], Step [64/334], Loss: 0.5490\n",
      "Epoch [30/50], Step [65/334], Loss: 0.6911\n",
      "Epoch [30/50], Step [66/334], Loss: 0.4555\n",
      "Epoch [30/50], Step [67/334], Loss: 1.0096\n",
      "Epoch [30/50], Step [68/334], Loss: 0.3410\n",
      "Epoch [30/50], Step [69/334], Loss: 0.5313\n",
      "Epoch [30/50], Step [70/334], Loss: 0.7752\n",
      "Epoch [30/50], Step [71/334], Loss: 0.4378\n",
      "Epoch [30/50], Step [72/334], Loss: 0.7742\n",
      "Epoch [30/50], Step [73/334], Loss: 0.5756\n",
      "Epoch [30/50], Step [74/334], Loss: 0.3969\n",
      "Epoch [30/50], Step [75/334], Loss: 1.0928\n",
      "Epoch [30/50], Step [76/334], Loss: 0.4385\n",
      "Epoch [30/50], Step [77/334], Loss: 0.6716\n",
      "Epoch [30/50], Step [78/334], Loss: 0.6920\n",
      "Epoch [30/50], Step [79/334], Loss: 0.5887\n",
      "Epoch [30/50], Step [80/334], Loss: 0.7505\n",
      "Epoch [30/50], Step [81/334], Loss: 0.4400\n",
      "Epoch [30/50], Step [82/334], Loss: 0.3193\n",
      "Epoch [30/50], Step [83/334], Loss: 0.6064\n",
      "Epoch [30/50], Step [84/334], Loss: 0.6321\n",
      "Epoch [30/50], Step [85/334], Loss: 0.6843\n",
      "Epoch [30/50], Step [86/334], Loss: 0.4769\n",
      "Epoch [30/50], Step [87/334], Loss: 1.0804\n",
      "Epoch [30/50], Step [88/334], Loss: 0.6183\n",
      "Epoch [30/50], Step [89/334], Loss: 0.9033\n",
      "Epoch [30/50], Step [90/334], Loss: 1.0801\n",
      "Epoch [30/50], Step [91/334], Loss: 0.3905\n",
      "Epoch [30/50], Step [92/334], Loss: 0.7218\n",
      "Epoch [30/50], Step [93/334], Loss: 0.5397\n",
      "Epoch [30/50], Step [94/334], Loss: 0.8639\n",
      "Epoch [30/50], Step [95/334], Loss: 0.6369\n",
      "Epoch [30/50], Step [96/334], Loss: 0.6667\n",
      "Epoch [30/50], Step [97/334], Loss: 0.7304\n",
      "Epoch [30/50], Step [98/334], Loss: 0.7151\n",
      "Epoch [30/50], Step [99/334], Loss: 0.7516\n",
      "Epoch [30/50], Step [100/334], Loss: 0.9764\n",
      "Epoch [30/50], Step [101/334], Loss: 0.4137\n",
      "Epoch [30/50], Step [102/334], Loss: 0.9828\n",
      "Epoch [30/50], Step [103/334], Loss: 1.2019\n",
      "Epoch [30/50], Step [104/334], Loss: 0.6644\n",
      "Epoch [30/50], Step [105/334], Loss: 0.9043\n",
      "Epoch [30/50], Step [106/334], Loss: 0.9678\n",
      "Epoch [30/50], Step [107/334], Loss: 0.6463\n",
      "Epoch [30/50], Step [108/334], Loss: 1.9278\n",
      "Epoch [30/50], Step [109/334], Loss: 0.5297\n",
      "Epoch [30/50], Step [110/334], Loss: 2.1523\n",
      "Epoch [30/50], Step [111/334], Loss: 1.1103\n",
      "Epoch [30/50], Step [112/334], Loss: 0.9731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/50], Step [113/334], Loss: 2.1072\n",
      "Epoch [30/50], Step [114/334], Loss: 0.8358\n",
      "Epoch [30/50], Step [115/334], Loss: 1.9057\n",
      "Epoch [30/50], Step [116/334], Loss: 1.8746\n",
      "Epoch [30/50], Step [117/334], Loss: 0.5468\n",
      "Epoch [30/50], Step [118/334], Loss: 1.7313\n",
      "Epoch [30/50], Step [119/334], Loss: 1.5991\n",
      "Epoch [30/50], Step [120/334], Loss: 1.0676\n",
      "Epoch [30/50], Step [121/334], Loss: 1.5564\n",
      "Epoch [30/50], Step [122/334], Loss: 0.8101\n",
      "Epoch [30/50], Step [123/334], Loss: 2.2458\n",
      "Epoch [30/50], Step [124/334], Loss: 0.5970\n",
      "Epoch [30/50], Step [125/334], Loss: 0.5757\n",
      "Epoch [30/50], Step [126/334], Loss: 1.7513\n",
      "Epoch [30/50], Step [127/334], Loss: 1.3118\n",
      "Epoch [30/50], Step [128/334], Loss: 1.8511\n",
      "Epoch [30/50], Step [129/334], Loss: 0.8793\n",
      "Epoch [30/50], Step [130/334], Loss: 1.2691\n",
      "Epoch [30/50], Step [131/334], Loss: 2.2175\n",
      "Epoch [30/50], Step [132/334], Loss: 1.3860\n",
      "Epoch [30/50], Step [133/334], Loss: 0.7426\n",
      "Epoch [30/50], Step [134/334], Loss: 2.0257\n",
      "Epoch [30/50], Step [135/334], Loss: 2.6858\n",
      "Epoch [30/50], Step [136/334], Loss: 0.9091\n",
      "Epoch [30/50], Step [137/334], Loss: 0.8250\n",
      "Epoch [30/50], Step [138/334], Loss: 2.0542\n",
      "Epoch [30/50], Step [139/334], Loss: 2.6266\n",
      "Epoch [30/50], Step [140/334], Loss: 0.8130\n",
      "Epoch [30/50], Step [141/334], Loss: 2.1368\n",
      "Epoch [30/50], Step [142/334], Loss: 1.2638\n",
      "Epoch [30/50], Step [143/334], Loss: 1.3096\n",
      "Epoch [30/50], Step [144/334], Loss: 0.9904\n",
      "Epoch [30/50], Step [145/334], Loss: 1.6377\n",
      "Epoch [30/50], Step [146/334], Loss: 1.3500\n",
      "Epoch [30/50], Step [147/334], Loss: 1.5143\n",
      "Epoch [30/50], Step [148/334], Loss: 1.9129\n",
      "Epoch [30/50], Step [149/334], Loss: 1.6813\n",
      "Epoch [30/50], Step [150/334], Loss: 1.1921\n",
      "Epoch [30/50], Step [151/334], Loss: 1.3739\n",
      "Epoch [30/50], Step [152/334], Loss: 1.8458\n",
      "Epoch [30/50], Step [153/334], Loss: 1.1835\n",
      "Epoch [30/50], Step [154/334], Loss: 1.4097\n",
      "Epoch [30/50], Step [155/334], Loss: 1.1363\n",
      "Epoch [30/50], Step [156/334], Loss: 0.9373\n",
      "Epoch [30/50], Step [157/334], Loss: 0.6556\n",
      "Epoch [30/50], Step [158/334], Loss: 0.9760\n",
      "Epoch [30/50], Step [159/334], Loss: 1.3618\n",
      "Epoch [30/50], Step [160/334], Loss: 1.6137\n",
      "Epoch [30/50], Step [161/334], Loss: 0.7548\n",
      "Epoch [30/50], Step [162/334], Loss: 1.0764\n",
      "Epoch [30/50], Step [163/334], Loss: 1.0777\n",
      "Epoch [30/50], Step [164/334], Loss: 1.8677\n",
      "Epoch [30/50], Step [165/334], Loss: 1.0774\n",
      "Epoch [30/50], Step [166/334], Loss: 1.6864\n",
      "Epoch [30/50], Step [167/334], Loss: 1.2700\n",
      "Epoch [30/50], Step [168/334], Loss: 0.8403\n",
      "Epoch [30/50], Step [169/334], Loss: 2.0876\n",
      "Epoch [30/50], Step [170/334], Loss: 1.1777\n",
      "Epoch [30/50], Step [171/334], Loss: 0.5830\n",
      "Epoch [30/50], Step [172/334], Loss: 1.8430\n",
      "Epoch [30/50], Step [173/334], Loss: 1.1498\n",
      "Epoch [30/50], Step [174/334], Loss: 1.2809\n",
      "Epoch [30/50], Step [175/334], Loss: 1.1989\n",
      "Epoch [30/50], Step [176/334], Loss: 0.6424\n",
      "Epoch [30/50], Step [177/334], Loss: 0.8829\n",
      "Epoch [30/50], Step [178/334], Loss: 1.2068\n",
      "Epoch [30/50], Step [179/334], Loss: 0.4640\n",
      "Epoch [30/50], Step [180/334], Loss: 1.3752\n",
      "Epoch [30/50], Step [181/334], Loss: 0.8273\n",
      "Epoch [30/50], Step [182/334], Loss: 1.1312\n",
      "Epoch [30/50], Step [183/334], Loss: 1.2945\n",
      "Epoch [30/50], Step [184/334], Loss: 3.7513\n",
      "Epoch [30/50], Step [185/334], Loss: 3.7707\n",
      "Epoch [30/50], Step [186/334], Loss: 3.2740\n",
      "Epoch [30/50], Step [187/334], Loss: 2.7238\n",
      "Epoch [30/50], Step [188/334], Loss: 3.6210\n",
      "Epoch [30/50], Step [189/334], Loss: 1.5003\n",
      "Epoch [30/50], Step [190/334], Loss: 3.2141\n",
      "Epoch [30/50], Step [191/334], Loss: 2.2261\n",
      "Epoch [30/50], Step [192/334], Loss: 0.8810\n",
      "Epoch [30/50], Step [193/334], Loss: 3.0348\n",
      "Epoch [30/50], Step [194/334], Loss: 4.9988\n",
      "Epoch [30/50], Step [195/334], Loss: 5.6389\n",
      "Epoch [30/50], Step [196/334], Loss: 7.1523\n",
      "Epoch [30/50], Step [197/334], Loss: 2.4235\n",
      "Epoch [30/50], Step [198/334], Loss: 9.7547\n",
      "Epoch [30/50], Step [199/334], Loss: 3.2575\n",
      "Epoch [30/50], Step [200/334], Loss: 10.5704\n",
      "Epoch [30/50], Step [201/334], Loss: 10.6712\n",
      "Epoch [30/50], Step [202/334], Loss: 6.9593\n",
      "Epoch [30/50], Step [203/334], Loss: 16.8275\n",
      "Epoch [30/50], Step [204/334], Loss: 2.0597\n",
      "Epoch [30/50], Step [205/334], Loss: 10.4922\n",
      "Epoch [30/50], Step [206/334], Loss: 2.3858\n",
      "Epoch [30/50], Step [207/334], Loss: 4.9085\n",
      "Epoch [30/50], Step [208/334], Loss: 4.3910\n",
      "Epoch [30/50], Step [209/334], Loss: 2.6272\n",
      "Epoch [30/50], Step [210/334], Loss: 2.7252\n",
      "Epoch [30/50], Step [211/334], Loss: 4.2058\n",
      "Epoch [30/50], Step [212/334], Loss: 2.4931\n",
      "Epoch [30/50], Step [213/334], Loss: 5.9044\n",
      "Epoch [30/50], Step [214/334], Loss: 1.0120\n",
      "Epoch [30/50], Step [215/334], Loss: 2.5260\n",
      "Epoch [30/50], Step [216/334], Loss: 3.4214\n",
      "Epoch [30/50], Step [217/334], Loss: 1.1377\n",
      "Epoch [30/50], Step [218/334], Loss: 2.8842\n",
      "Epoch [30/50], Step [219/334], Loss: 1.4753\n",
      "Epoch [30/50], Step [220/334], Loss: 2.0384\n",
      "Epoch [30/50], Step [221/334], Loss: 1.3718\n",
      "Epoch [30/50], Step [222/334], Loss: 3.4410\n",
      "Epoch [30/50], Step [223/334], Loss: 2.7412\n",
      "Epoch [30/50], Step [224/334], Loss: 2.1437\n",
      "Epoch [30/50], Step [225/334], Loss: 4.4802\n",
      "Epoch [30/50], Step [226/334], Loss: 2.3931\n",
      "Epoch [30/50], Step [227/334], Loss: 10.4529\n",
      "Epoch [30/50], Step [228/334], Loss: 2.2672\n",
      "Epoch [30/50], Step [229/334], Loss: 5.4452\n",
      "Epoch [30/50], Step [230/334], Loss: 5.0802\n",
      "Epoch [30/50], Step [231/334], Loss: 3.8035\n",
      "Epoch [30/50], Step [232/334], Loss: 6.8127\n",
      "Epoch [30/50], Step [233/334], Loss: 1.9363\n",
      "Epoch [30/50], Step [234/334], Loss: 2.5352\n",
      "Epoch [30/50], Step [235/334], Loss: 6.5190\n",
      "Epoch [30/50], Step [236/334], Loss: 1.4067\n",
      "Epoch [30/50], Step [237/334], Loss: 6.1606\n",
      "Epoch [30/50], Step [238/334], Loss: 1.9066\n",
      "Epoch [30/50], Step [239/334], Loss: 6.4179\n",
      "Epoch [30/50], Step [240/334], Loss: 2.9258\n",
      "Epoch [30/50], Step [241/334], Loss: 1.4462\n",
      "Epoch [30/50], Step [242/334], Loss: 5.6962\n",
      "Epoch [30/50], Step [243/334], Loss: 2.5745\n",
      "Epoch [30/50], Step [244/334], Loss: 1.7325\n",
      "Epoch [30/50], Step [245/334], Loss: 2.4966\n",
      "Epoch [30/50], Step [246/334], Loss: 6.0153\n",
      "Epoch [30/50], Step [247/334], Loss: 5.4483\n",
      "Epoch [30/50], Step [248/334], Loss: 6.3781\n",
      "Epoch [30/50], Step [249/334], Loss: 0.6191\n",
      "Epoch [30/50], Step [250/334], Loss: 7.7443\n",
      "Epoch [30/50], Step [251/334], Loss: 3.2441\n",
      "Epoch [30/50], Step [252/334], Loss: 1.2368\n",
      "Epoch [30/50], Step [253/334], Loss: 3.1033\n",
      "Epoch [30/50], Step [254/334], Loss: 1.8140\n",
      "Epoch [30/50], Step [255/334], Loss: 2.3477\n",
      "Epoch [30/50], Step [256/334], Loss: 3.1261\n",
      "Epoch [30/50], Step [257/334], Loss: 1.9539\n",
      "Epoch [30/50], Step [258/334], Loss: 2.4983\n",
      "Epoch [30/50], Step [259/334], Loss: 2.1812\n",
      "Epoch [30/50], Step [260/334], Loss: 2.0839\n",
      "Epoch [30/50], Step [261/334], Loss: 1.4432\n",
      "Epoch [30/50], Step [262/334], Loss: 2.2141\n",
      "Epoch [30/50], Step [263/334], Loss: 1.1427\n",
      "Epoch [30/50], Step [264/334], Loss: 1.8661\n",
      "Epoch [30/50], Step [265/334], Loss: 2.7855\n",
      "Epoch [30/50], Step [266/334], Loss: 2.2353\n",
      "Epoch [30/50], Step [267/334], Loss: 1.7939\n",
      "Epoch [30/50], Step [268/334], Loss: 1.4908\n",
      "Epoch [30/50], Step [269/334], Loss: 1.3499\n",
      "Epoch [30/50], Step [270/334], Loss: 3.4433\n",
      "Epoch [30/50], Step [271/334], Loss: 1.9456\n",
      "Epoch [30/50], Step [272/334], Loss: 2.0706\n",
      "Epoch [30/50], Step [273/334], Loss: 0.9687\n",
      "Epoch [30/50], Step [274/334], Loss: 1.2525\n",
      "Epoch [30/50], Step [275/334], Loss: 1.6054\n",
      "Epoch [30/50], Step [276/334], Loss: 1.4825\n",
      "Epoch [30/50], Step [277/334], Loss: 1.0760\n",
      "Epoch [30/50], Step [278/334], Loss: 0.7154\n",
      "Epoch [30/50], Step [279/334], Loss: 2.5893\n",
      "Epoch [30/50], Step [280/334], Loss: 1.3957\n",
      "Epoch [30/50], Step [281/334], Loss: 1.5999\n",
      "Epoch [30/50], Step [282/334], Loss: 0.7292\n",
      "Epoch [30/50], Step [283/334], Loss: 1.2225\n",
      "Epoch [30/50], Step [284/334], Loss: 2.0058\n",
      "Epoch [30/50], Step [285/334], Loss: 1.7548\n",
      "Epoch [30/50], Step [286/334], Loss: 0.4480\n",
      "Epoch [30/50], Step [287/334], Loss: 1.8881\n",
      "Epoch [30/50], Step [288/334], Loss: 1.7877\n",
      "Epoch [30/50], Step [289/334], Loss: 1.3575\n",
      "Epoch [30/50], Step [290/334], Loss: 1.2243\n",
      "Epoch [30/50], Step [291/334], Loss: 1.2372\n",
      "Epoch [30/50], Step [292/334], Loss: 1.1802\n",
      "Epoch [30/50], Step [293/334], Loss: 2.0896\n",
      "Epoch [30/50], Step [294/334], Loss: 1.2299\n",
      "Epoch [30/50], Step [295/334], Loss: 1.5940\n",
      "Epoch [30/50], Step [296/334], Loss: 1.1240\n",
      "Epoch [30/50], Step [297/334], Loss: 1.8955\n",
      "Epoch [30/50], Step [298/334], Loss: 0.8751\n",
      "Epoch [30/50], Step [299/334], Loss: 0.9050\n",
      "Epoch [30/50], Step [300/334], Loss: 1.4966\n",
      "Epoch [30/50], Step [301/334], Loss: 0.9063\n",
      "Epoch [30/50], Step [302/334], Loss: 0.7121\n",
      "Epoch [30/50], Step [303/334], Loss: 1.1448\n",
      "Epoch [30/50], Step [304/334], Loss: 2.6977\n",
      "Epoch [30/50], Step [305/334], Loss: 0.7002\n",
      "Epoch [30/50], Step [306/334], Loss: 2.2471\n",
      "Epoch [30/50], Step [307/334], Loss: 2.0686\n",
      "Epoch [30/50], Step [308/334], Loss: 1.8313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/50], Step [309/334], Loss: 2.5381\n",
      "Epoch [30/50], Step [310/334], Loss: 1.3644\n",
      "Epoch [30/50], Step [311/334], Loss: 1.1255\n",
      "Epoch [30/50], Step [312/334], Loss: 3.1444\n",
      "Epoch [30/50], Step [313/334], Loss: 0.6679\n",
      "Epoch [30/50], Step [314/334], Loss: 3.5750\n",
      "Epoch [30/50], Step [315/334], Loss: 0.5925\n",
      "Epoch [30/50], Step [316/334], Loss: 1.8018\n",
      "Epoch [30/50], Step [317/334], Loss: 3.7796\n",
      "Epoch [30/50], Step [318/334], Loss: 1.2554\n",
      "Epoch [30/50], Step [319/334], Loss: 4.0078\n",
      "Epoch [30/50], Step [320/334], Loss: 1.1599\n",
      "Epoch [30/50], Step [321/334], Loss: 2.0265\n",
      "Epoch [30/50], Step [322/334], Loss: 4.9450\n",
      "Epoch [30/50], Step [323/334], Loss: 1.2705\n",
      "Epoch [30/50], Step [324/334], Loss: 4.8489\n",
      "Epoch [30/50], Step [325/334], Loss: 1.9542\n",
      "Epoch [30/50], Step [326/334], Loss: 4.6741\n",
      "Epoch [30/50], Step [327/334], Loss: 6.1975\n",
      "Epoch [30/50], Step [328/334], Loss: 0.7930\n",
      "Epoch [30/50], Step [329/334], Loss: 6.3896\n",
      "Epoch [30/50], Step [330/334], Loss: 3.0634\n",
      "Epoch [30/50], Step [331/334], Loss: 4.9627\n",
      "Epoch [30/50], Step [332/334], Loss: 5.0734\n",
      "Epoch [30/50], Step [333/334], Loss: 0.6386\n",
      "Epoch [30/50], Step [334/334], Loss: 3.0485\n",
      "Epoch [40/50], Step [1/334], Loss: 1.4669\n",
      "Epoch [40/50], Step [2/334], Loss: 2.4246\n",
      "Epoch [40/50], Step [3/334], Loss: 0.8685\n",
      "Epoch [40/50], Step [4/334], Loss: 1.7365\n",
      "Epoch [40/50], Step [5/334], Loss: 2.8009\n",
      "Epoch [40/50], Step [6/334], Loss: 0.8850\n",
      "Epoch [40/50], Step [7/334], Loss: 1.2863\n",
      "Epoch [40/50], Step [8/334], Loss: 1.6333\n",
      "Epoch [40/50], Step [9/334], Loss: 0.8243\n",
      "Epoch [40/50], Step [10/334], Loss: 3.8803\n",
      "Epoch [40/50], Step [11/334], Loss: 1.1726\n",
      "Epoch [40/50], Step [12/334], Loss: 1.1592\n",
      "Epoch [40/50], Step [13/334], Loss: 1.8783\n",
      "Epoch [40/50], Step [14/334], Loss: 0.4333\n",
      "Epoch [40/50], Step [15/334], Loss: 0.9450\n",
      "Epoch [40/50], Step [16/334], Loss: 0.9571\n",
      "Epoch [40/50], Step [17/334], Loss: 0.7254\n",
      "Epoch [40/50], Step [18/334], Loss: 0.6478\n",
      "Epoch [40/50], Step [19/334], Loss: 0.8187\n",
      "Epoch [40/50], Step [20/334], Loss: 1.6033\n",
      "Epoch [40/50], Step [21/334], Loss: 0.8366\n",
      "Epoch [40/50], Step [22/334], Loss: 0.7862\n",
      "Epoch [40/50], Step [23/334], Loss: 0.5182\n",
      "Epoch [40/50], Step [24/334], Loss: 0.7597\n",
      "Epoch [40/50], Step [25/334], Loss: 0.8335\n",
      "Epoch [40/50], Step [26/334], Loss: 0.7140\n",
      "Epoch [40/50], Step [27/334], Loss: 0.6162\n",
      "Epoch [40/50], Step [28/334], Loss: 0.8443\n",
      "Epoch [40/50], Step [29/334], Loss: 0.9615\n",
      "Epoch [40/50], Step [30/334], Loss: 1.0204\n",
      "Epoch [40/50], Step [31/334], Loss: 0.5911\n",
      "Epoch [40/50], Step [32/334], Loss: 0.9877\n",
      "Epoch [40/50], Step [33/334], Loss: 0.6879\n",
      "Epoch [40/50], Step [34/334], Loss: 0.6780\n",
      "Epoch [40/50], Step [35/334], Loss: 0.5848\n",
      "Epoch [40/50], Step [36/334], Loss: 0.9330\n",
      "Epoch [40/50], Step [37/334], Loss: 0.4421\n",
      "Epoch [40/50], Step [38/334], Loss: 1.7175\n",
      "Epoch [40/50], Step [39/334], Loss: 0.6022\n",
      "Epoch [40/50], Step [40/334], Loss: 1.2099\n",
      "Epoch [40/50], Step [41/334], Loss: 0.7051\n",
      "Epoch [40/50], Step [42/334], Loss: 1.4729\n",
      "Epoch [40/50], Step [43/334], Loss: 0.9412\n",
      "Epoch [40/50], Step [44/334], Loss: 0.6719\n",
      "Epoch [40/50], Step [45/334], Loss: 1.2709\n",
      "Epoch [40/50], Step [46/334], Loss: 0.9793\n",
      "Epoch [40/50], Step [47/334], Loss: 1.8470\n",
      "Epoch [40/50], Step [48/334], Loss: 0.8055\n",
      "Epoch [40/50], Step [49/334], Loss: 0.7528\n",
      "Epoch [40/50], Step [50/334], Loss: 1.8584\n",
      "Epoch [40/50], Step [51/334], Loss: 0.5426\n",
      "Epoch [40/50], Step [52/334], Loss: 1.0565\n",
      "Epoch [40/50], Step [53/334], Loss: 1.1125\n",
      "Epoch [40/50], Step [54/334], Loss: 1.0508\n",
      "Epoch [40/50], Step [55/334], Loss: 1.3554\n",
      "Epoch [40/50], Step [56/334], Loss: 0.4393\n",
      "Epoch [40/50], Step [57/334], Loss: 0.9854\n",
      "Epoch [40/50], Step [58/334], Loss: 1.3529\n",
      "Epoch [40/50], Step [59/334], Loss: 0.4786\n",
      "Epoch [40/50], Step [60/334], Loss: 1.3891\n",
      "Epoch [40/50], Step [61/334], Loss: 1.4809\n",
      "Epoch [40/50], Step [62/334], Loss: 0.7384\n",
      "Epoch [40/50], Step [63/334], Loss: 1.9548\n",
      "Epoch [40/50], Step [64/334], Loss: 0.5762\n",
      "Epoch [40/50], Step [65/334], Loss: 1.1649\n",
      "Epoch [40/50], Step [66/334], Loss: 1.8620\n",
      "Epoch [40/50], Step [67/334], Loss: 1.2511\n",
      "Epoch [40/50], Step [68/334], Loss: 1.3437\n",
      "Epoch [40/50], Step [69/334], Loss: 0.6028\n",
      "Epoch [40/50], Step [70/334], Loss: 0.9220\n",
      "Epoch [40/50], Step [71/334], Loss: 1.7839\n",
      "Epoch [40/50], Step [72/334], Loss: 1.3825\n",
      "Epoch [40/50], Step [73/334], Loss: 1.0813\n",
      "Epoch [40/50], Step [74/334], Loss: 0.8771\n",
      "Epoch [40/50], Step [75/334], Loss: 0.8129\n",
      "Epoch [40/50], Step [76/334], Loss: 0.9096\n",
      "Epoch [40/50], Step [77/334], Loss: 0.7365\n",
      "Epoch [40/50], Step [78/334], Loss: 0.2998\n",
      "Epoch [40/50], Step [79/334], Loss: 0.4293\n",
      "Epoch [40/50], Step [80/334], Loss: 1.0067\n",
      "Epoch [40/50], Step [81/334], Loss: 1.4332\n",
      "Epoch [40/50], Step [82/334], Loss: 0.4176\n",
      "Epoch [40/50], Step [83/334], Loss: 0.6433\n",
      "Epoch [40/50], Step [84/334], Loss: 0.8044\n",
      "Epoch [40/50], Step [85/334], Loss: 0.5893\n",
      "Epoch [40/50], Step [86/334], Loss: 1.0722\n",
      "Epoch [40/50], Step [87/334], Loss: 0.9793\n",
      "Epoch [40/50], Step [88/334], Loss: 0.8185\n",
      "Epoch [40/50], Step [89/334], Loss: 1.2830\n",
      "Epoch [40/50], Step [90/334], Loss: 0.8373\n",
      "Epoch [40/50], Step [91/334], Loss: 1.4346\n",
      "Epoch [40/50], Step [92/334], Loss: 0.7955\n",
      "Epoch [40/50], Step [93/334], Loss: 0.4955\n",
      "Epoch [40/50], Step [94/334], Loss: 0.7722\n",
      "Epoch [40/50], Step [95/334], Loss: 0.7332\n",
      "Epoch [40/50], Step [96/334], Loss: 0.7935\n",
      "Epoch [40/50], Step [97/334], Loss: 1.1246\n",
      "Epoch [40/50], Step [98/334], Loss: 0.8117\n",
      "Epoch [40/50], Step [99/334], Loss: 0.5747\n",
      "Epoch [40/50], Step [100/334], Loss: 1.3084\n",
      "Epoch [40/50], Step [101/334], Loss: 0.6943\n",
      "Epoch [40/50], Step [102/334], Loss: 0.8375\n",
      "Epoch [40/50], Step [103/334], Loss: 0.6345\n",
      "Epoch [40/50], Step [104/334], Loss: 0.8843\n",
      "Epoch [40/50], Step [105/334], Loss: 0.8717\n",
      "Epoch [40/50], Step [106/334], Loss: 0.4301\n",
      "Epoch [40/50], Step [107/334], Loss: 0.8044\n",
      "Epoch [40/50], Step [108/334], Loss: 0.5025\n",
      "Epoch [40/50], Step [109/334], Loss: 0.9830\n",
      "Epoch [40/50], Step [110/334], Loss: 0.3531\n",
      "Epoch [40/50], Step [111/334], Loss: 0.6090\n",
      "Epoch [40/50], Step [112/334], Loss: 0.4945\n",
      "Epoch [40/50], Step [113/334], Loss: 0.2816\n",
      "Epoch [40/50], Step [114/334], Loss: 0.9560\n",
      "Epoch [40/50], Step [115/334], Loss: 0.6951\n",
      "Epoch [40/50], Step [116/334], Loss: 0.4712\n",
      "Epoch [40/50], Step [117/334], Loss: 0.4800\n",
      "Epoch [40/50], Step [118/334], Loss: 0.6892\n",
      "Epoch [40/50], Step [119/334], Loss: 0.6470\n",
      "Epoch [40/50], Step [120/334], Loss: 1.0794\n",
      "Epoch [40/50], Step [121/334], Loss: 0.7916\n",
      "Epoch [40/50], Step [122/334], Loss: 0.7262\n",
      "Epoch [40/50], Step [123/334], Loss: 1.1802\n",
      "Epoch [40/50], Step [124/334], Loss: 0.4482\n",
      "Epoch [40/50], Step [125/334], Loss: 1.0975\n",
      "Epoch [40/50], Step [126/334], Loss: 1.4256\n",
      "Epoch [40/50], Step [127/334], Loss: 0.6179\n",
      "Epoch [40/50], Step [128/334], Loss: 0.8820\n",
      "Epoch [40/50], Step [129/334], Loss: 1.4816\n",
      "Epoch [40/50], Step [130/334], Loss: 1.3124\n",
      "Epoch [40/50], Step [131/334], Loss: 1.1421\n",
      "Epoch [40/50], Step [132/334], Loss: 1.2852\n",
      "Epoch [40/50], Step [133/334], Loss: 0.8749\n",
      "Epoch [40/50], Step [134/334], Loss: 1.1709\n",
      "Epoch [40/50], Step [135/334], Loss: 1.1123\n",
      "Epoch [40/50], Step [136/334], Loss: 0.5347\n",
      "Epoch [40/50], Step [137/334], Loss: 0.7870\n",
      "Epoch [40/50], Step [138/334], Loss: 1.0670\n",
      "Epoch [40/50], Step [139/334], Loss: 0.7113\n",
      "Epoch [40/50], Step [140/334], Loss: 0.7105\n",
      "Epoch [40/50], Step [141/334], Loss: 0.9392\n",
      "Epoch [40/50], Step [142/334], Loss: 0.7724\n",
      "Epoch [40/50], Step [143/334], Loss: 0.9450\n",
      "Epoch [40/50], Step [144/334], Loss: 0.5792\n",
      "Epoch [40/50], Step [145/334], Loss: 0.4663\n",
      "Epoch [40/50], Step [146/334], Loss: 0.2795\n",
      "Epoch [40/50], Step [147/334], Loss: 0.4159\n",
      "Epoch [40/50], Step [148/334], Loss: 0.7796\n",
      "Epoch [40/50], Step [149/334], Loss: 0.4734\n",
      "Epoch [40/50], Step [150/334], Loss: 0.8408\n",
      "Epoch [40/50], Step [151/334], Loss: 0.4248\n",
      "Epoch [40/50], Step [152/334], Loss: 0.4332\n",
      "Epoch [40/50], Step [153/334], Loss: 0.7294\n",
      "Epoch [40/50], Step [154/334], Loss: 0.3939\n",
      "Epoch [40/50], Step [155/334], Loss: 0.7494\n",
      "Epoch [40/50], Step [156/334], Loss: 0.8019\n",
      "Epoch [40/50], Step [157/334], Loss: 0.6413\n",
      "Epoch [40/50], Step [158/334], Loss: 0.4501\n",
      "Epoch [40/50], Step [159/334], Loss: 0.7657\n",
      "Epoch [40/50], Step [160/334], Loss: 0.5671\n",
      "Epoch [40/50], Step [161/334], Loss: 0.3704\n",
      "Epoch [40/50], Step [162/334], Loss: 0.8650\n",
      "Epoch [40/50], Step [163/334], Loss: 0.6388\n",
      "Epoch [40/50], Step [164/334], Loss: 0.5503\n",
      "Epoch [40/50], Step [165/334], Loss: 0.6898\n",
      "Epoch [40/50], Step [166/334], Loss: 0.3763\n",
      "Epoch [40/50], Step [167/334], Loss: 0.4184\n",
      "Epoch [40/50], Step [168/334], Loss: 0.4904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/50], Step [169/334], Loss: 0.5766\n",
      "Epoch [40/50], Step [170/334], Loss: 0.6164\n",
      "Epoch [40/50], Step [171/334], Loss: 0.5879\n",
      "Epoch [40/50], Step [172/334], Loss: 0.8065\n",
      "Epoch [40/50], Step [173/334], Loss: 0.4416\n",
      "Epoch [40/50], Step [174/334], Loss: 0.3719\n",
      "Epoch [40/50], Step [175/334], Loss: 0.7648\n",
      "Epoch [40/50], Step [176/334], Loss: 0.8392\n",
      "Epoch [40/50], Step [177/334], Loss: 0.5286\n",
      "Epoch [40/50], Step [178/334], Loss: 0.5434\n",
      "Epoch [40/50], Step [179/334], Loss: 0.2514\n",
      "Epoch [40/50], Step [180/334], Loss: 0.2397\n",
      "Epoch [40/50], Step [181/334], Loss: 0.2903\n",
      "Epoch [40/50], Step [182/334], Loss: 0.5721\n",
      "Epoch [40/50], Step [183/334], Loss: 0.3624\n",
      "Epoch [40/50], Step [184/334], Loss: 0.3061\n",
      "Epoch [40/50], Step [185/334], Loss: 0.3863\n",
      "Epoch [40/50], Step [186/334], Loss: 0.3769\n",
      "Epoch [40/50], Step [187/334], Loss: 0.4438\n",
      "Epoch [40/50], Step [188/334], Loss: 0.4495\n",
      "Epoch [40/50], Step [189/334], Loss: 0.4895\n",
      "Epoch [40/50], Step [190/334], Loss: 0.4249\n",
      "Epoch [40/50], Step [191/334], Loss: 0.5558\n",
      "Epoch [40/50], Step [192/334], Loss: 0.4182\n",
      "Epoch [40/50], Step [193/334], Loss: 0.4855\n",
      "Epoch [40/50], Step [194/334], Loss: 0.4585\n",
      "Epoch [40/50], Step [195/334], Loss: 0.4161\n",
      "Epoch [40/50], Step [196/334], Loss: 0.4097\n",
      "Epoch [40/50], Step [197/334], Loss: 0.6247\n",
      "Epoch [40/50], Step [198/334], Loss: 0.2119\n",
      "Epoch [40/50], Step [199/334], Loss: 0.9128\n",
      "Epoch [40/50], Step [200/334], Loss: 0.3737\n",
      "Epoch [40/50], Step [201/334], Loss: 0.4151\n",
      "Epoch [40/50], Step [202/334], Loss: 0.6203\n",
      "Epoch [40/50], Step [203/334], Loss: 0.4422\n",
      "Epoch [40/50], Step [204/334], Loss: 0.5912\n",
      "Epoch [40/50], Step [205/334], Loss: 0.8252\n",
      "Epoch [40/50], Step [206/334], Loss: 0.5490\n",
      "Epoch [40/50], Step [207/334], Loss: 0.7175\n",
      "Epoch [40/50], Step [208/334], Loss: 0.9766\n",
      "Epoch [40/50], Step [209/334], Loss: 0.8391\n",
      "Epoch [40/50], Step [210/334], Loss: 0.4014\n",
      "Epoch [40/50], Step [211/334], Loss: 0.3545\n",
      "Epoch [40/50], Step [212/334], Loss: 0.6284\n",
      "Epoch [40/50], Step [213/334], Loss: 0.6677\n",
      "Epoch [40/50], Step [214/334], Loss: 0.3351\n",
      "Epoch [40/50], Step [215/334], Loss: 1.2169\n",
      "Epoch [40/50], Step [216/334], Loss: 0.9122\n",
      "Epoch [40/50], Step [217/334], Loss: 0.6029\n",
      "Epoch [40/50], Step [218/334], Loss: 1.2214\n",
      "Epoch [40/50], Step [219/334], Loss: 0.3813\n",
      "Epoch [40/50], Step [220/334], Loss: 0.6933\n",
      "Epoch [40/50], Step [221/334], Loss: 0.6738\n",
      "Epoch [40/50], Step [222/334], Loss: 0.4729\n",
      "Epoch [40/50], Step [223/334], Loss: 1.6022\n",
      "Epoch [40/50], Step [224/334], Loss: 0.3852\n",
      "Epoch [40/50], Step [225/334], Loss: 1.1252\n",
      "Epoch [40/50], Step [226/334], Loss: 0.3779\n",
      "Epoch [40/50], Step [227/334], Loss: 0.4035\n",
      "Epoch [40/50], Step [228/334], Loss: 0.9118\n",
      "Epoch [40/50], Step [229/334], Loss: 1.0985\n",
      "Epoch [40/50], Step [230/334], Loss: 0.3680\n",
      "Epoch [40/50], Step [231/334], Loss: 0.3941\n",
      "Epoch [40/50], Step [232/334], Loss: 0.4409\n",
      "Epoch [40/50], Step [233/334], Loss: 0.6303\n",
      "Epoch [40/50], Step [234/334], Loss: 0.4157\n",
      "Epoch [40/50], Step [235/334], Loss: 0.3615\n",
      "Epoch [40/50], Step [236/334], Loss: 0.8107\n",
      "Epoch [40/50], Step [237/334], Loss: 0.7368\n",
      "Epoch [40/50], Step [238/334], Loss: 1.6369\n",
      "Epoch [40/50], Step [239/334], Loss: 0.6396\n",
      "Epoch [40/50], Step [240/334], Loss: 1.7923\n",
      "Epoch [40/50], Step [241/334], Loss: 0.3246\n",
      "Epoch [40/50], Step [242/334], Loss: 0.7462\n",
      "Epoch [40/50], Step [243/334], Loss: 0.5731\n",
      "Epoch [40/50], Step [244/334], Loss: 1.4778\n",
      "Epoch [40/50], Step [245/334], Loss: 0.8989\n",
      "Epoch [40/50], Step [246/334], Loss: 0.3108\n",
      "Epoch [40/50], Step [247/334], Loss: 1.5426\n",
      "Epoch [40/50], Step [248/334], Loss: 0.5163\n",
      "Epoch [40/50], Step [249/334], Loss: 0.9314\n",
      "Epoch [40/50], Step [250/334], Loss: 0.6652\n",
      "Epoch [40/50], Step [251/334], Loss: 0.8098\n",
      "Epoch [40/50], Step [252/334], Loss: 0.6355\n",
      "Epoch [40/50], Step [253/334], Loss: 1.1470\n",
      "Epoch [40/50], Step [254/334], Loss: 0.8040\n",
      "Epoch [40/50], Step [255/334], Loss: 0.7424\n",
      "Epoch [40/50], Step [256/334], Loss: 1.3333\n",
      "Epoch [40/50], Step [257/334], Loss: 0.9585\n",
      "Epoch [40/50], Step [258/334], Loss: 1.2759\n",
      "Epoch [40/50], Step [259/334], Loss: 1.1905\n",
      "Epoch [40/50], Step [260/334], Loss: 0.9978\n",
      "Epoch [40/50], Step [261/334], Loss: 1.9411\n",
      "Epoch [40/50], Step [262/334], Loss: 1.1014\n",
      "Epoch [40/50], Step [263/334], Loss: 1.7551\n",
      "Epoch [40/50], Step [264/334], Loss: 0.8049\n",
      "Epoch [40/50], Step [265/334], Loss: 1.5450\n",
      "Epoch [40/50], Step [266/334], Loss: 1.8073\n",
      "Epoch [40/50], Step [267/334], Loss: 1.0972\n",
      "Epoch [40/50], Step [268/334], Loss: 1.4279\n",
      "Epoch [40/50], Step [269/334], Loss: 1.6280\n",
      "Epoch [40/50], Step [270/334], Loss: 0.5455\n",
      "Epoch [40/50], Step [271/334], Loss: 3.4647\n",
      "Epoch [40/50], Step [272/334], Loss: 0.7741\n",
      "Epoch [40/50], Step [273/334], Loss: 2.0531\n",
      "Epoch [40/50], Step [274/334], Loss: 2.0713\n",
      "Epoch [40/50], Step [275/334], Loss: 0.6334\n",
      "Epoch [40/50], Step [276/334], Loss: 2.6168\n",
      "Epoch [40/50], Step [277/334], Loss: 0.3881\n",
      "Epoch [40/50], Step [278/334], Loss: 1.2193\n",
      "Epoch [40/50], Step [279/334], Loss: 1.0743\n",
      "Epoch [40/50], Step [280/334], Loss: 0.6546\n",
      "Epoch [40/50], Step [281/334], Loss: 1.0169\n",
      "Epoch [40/50], Step [282/334], Loss: 0.6216\n",
      "Epoch [40/50], Step [283/334], Loss: 0.9782\n",
      "Epoch [40/50], Step [284/334], Loss: 0.8842\n",
      "Epoch [40/50], Step [285/334], Loss: 1.2000\n",
      "Epoch [40/50], Step [286/334], Loss: 0.8254\n",
      "Epoch [40/50], Step [287/334], Loss: 0.3869\n",
      "Epoch [40/50], Step [288/334], Loss: 1.0941\n",
      "Epoch [40/50], Step [289/334], Loss: 0.7444\n",
      "Epoch [40/50], Step [290/334], Loss: 1.4051\n",
      "Epoch [40/50], Step [291/334], Loss: 1.4452\n",
      "Epoch [40/50], Step [292/334], Loss: 0.9008\n",
      "Epoch [40/50], Step [293/334], Loss: 1.8360\n",
      "Epoch [40/50], Step [294/334], Loss: 1.5752\n",
      "Epoch [40/50], Step [295/334], Loss: 1.0573\n",
      "Epoch [40/50], Step [296/334], Loss: 1.2196\n",
      "Epoch [40/50], Step [297/334], Loss: 0.7190\n",
      "Epoch [40/50], Step [298/334], Loss: 1.7798\n",
      "Epoch [40/50], Step [299/334], Loss: 0.8970\n",
      "Epoch [40/50], Step [300/334], Loss: 0.7192\n",
      "Epoch [40/50], Step [301/334], Loss: 1.3286\n",
      "Epoch [40/50], Step [302/334], Loss: 1.0008\n",
      "Epoch [40/50], Step [303/334], Loss: 1.1507\n",
      "Epoch [40/50], Step [304/334], Loss: 1.0077\n",
      "Epoch [40/50], Step [305/334], Loss: 1.6788\n",
      "Epoch [40/50], Step [306/334], Loss: 0.6809\n",
      "Epoch [40/50], Step [307/334], Loss: 1.0101\n",
      "Epoch [40/50], Step [308/334], Loss: 0.4020\n",
      "Epoch [40/50], Step [309/334], Loss: 0.5786\n",
      "Epoch [40/50], Step [310/334], Loss: 0.5547\n",
      "Epoch [40/50], Step [311/334], Loss: 1.1565\n",
      "Epoch [40/50], Step [312/334], Loss: 1.0295\n",
      "Epoch [40/50], Step [313/334], Loss: 0.8557\n",
      "Epoch [40/50], Step [314/334], Loss: 0.3483\n",
      "Epoch [40/50], Step [315/334], Loss: 2.0013\n",
      "Epoch [40/50], Step [316/334], Loss: 0.5490\n",
      "Epoch [40/50], Step [317/334], Loss: 1.3639\n",
      "Epoch [40/50], Step [318/334], Loss: 1.1050\n",
      "Epoch [40/50], Step [319/334], Loss: 1.1016\n",
      "Epoch [40/50], Step [320/334], Loss: 0.7533\n",
      "Epoch [40/50], Step [321/334], Loss: 0.8439\n",
      "Epoch [40/50], Step [322/334], Loss: 0.6853\n",
      "Epoch [40/50], Step [323/334], Loss: 2.7840\n",
      "Epoch [40/50], Step [324/334], Loss: 2.5203\n",
      "Epoch [40/50], Step [325/334], Loss: 1.7665\n",
      "Epoch [40/50], Step [326/334], Loss: 1.9233\n",
      "Epoch [40/50], Step [327/334], Loss: 1.5347\n",
      "Epoch [40/50], Step [328/334], Loss: 0.7006\n",
      "Epoch [40/50], Step [329/334], Loss: 1.8430\n",
      "Epoch [40/50], Step [330/334], Loss: 1.2380\n",
      "Epoch [40/50], Step [331/334], Loss: 1.7965\n",
      "Epoch [40/50], Step [332/334], Loss: 1.5053\n",
      "Epoch [40/50], Step [333/334], Loss: 0.6886\n",
      "Epoch [40/50], Step [334/334], Loss: 1.2613\n",
      "Epoch [50/50], Step [1/334], Loss: 3.0501\n",
      "Epoch [50/50], Step [2/334], Loss: 4.1963\n",
      "Epoch [50/50], Step [3/334], Loss: 1.2592\n",
      "Epoch [50/50], Step [4/334], Loss: 5.3248\n",
      "Epoch [50/50], Step [5/334], Loss: 0.5022\n",
      "Epoch [50/50], Step [6/334], Loss: 3.2652\n",
      "Epoch [50/50], Step [7/334], Loss: 1.8065\n",
      "Epoch [50/50], Step [8/334], Loss: 1.1047\n",
      "Epoch [50/50], Step [9/334], Loss: 2.8257\n",
      "Epoch [50/50], Step [10/334], Loss: 1.5162\n",
      "Epoch [50/50], Step [11/334], Loss: 0.5835\n",
      "Epoch [50/50], Step [12/334], Loss: 2.2941\n",
      "Epoch [50/50], Step [13/334], Loss: 1.2252\n",
      "Epoch [50/50], Step [14/334], Loss: 1.4379\n",
      "Epoch [50/50], Step [15/334], Loss: 0.7607\n",
      "Epoch [50/50], Step [16/334], Loss: 0.6641\n",
      "Epoch [50/50], Step [17/334], Loss: 0.4494\n",
      "Epoch [50/50], Step [18/334], Loss: 0.4466\n",
      "Epoch [50/50], Step [19/334], Loss: 1.0912\n",
      "Epoch [50/50], Step [20/334], Loss: 1.3502\n",
      "Epoch [50/50], Step [21/334], Loss: 1.0930\n",
      "Epoch [50/50], Step [22/334], Loss: 1.6896\n",
      "Epoch [50/50], Step [23/334], Loss: 2.0928\n",
      "Epoch [50/50], Step [24/334], Loss: 0.9804\n",
      "Epoch [50/50], Step [25/334], Loss: 0.7430\n",
      "Epoch [50/50], Step [26/334], Loss: 1.5027\n",
      "Epoch [50/50], Step [27/334], Loss: 1.2423\n",
      "Epoch [50/50], Step [28/334], Loss: 0.7755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/50], Step [29/334], Loss: 1.9783\n",
      "Epoch [50/50], Step [30/334], Loss: 1.2472\n",
      "Epoch [50/50], Step [31/334], Loss: 0.8324\n",
      "Epoch [50/50], Step [32/334], Loss: 2.8615\n",
      "Epoch [50/50], Step [33/334], Loss: 0.6819\n",
      "Epoch [50/50], Step [34/334], Loss: 2.6284\n",
      "Epoch [50/50], Step [35/334], Loss: 0.5007\n",
      "Epoch [50/50], Step [36/334], Loss: 1.5230\n",
      "Epoch [50/50], Step [37/334], Loss: 1.0366\n",
      "Epoch [50/50], Step [38/334], Loss: 0.5699\n",
      "Epoch [50/50], Step [39/334], Loss: 2.6927\n",
      "Epoch [50/50], Step [40/334], Loss: 0.7610\n",
      "Epoch [50/50], Step [41/334], Loss: 0.3976\n",
      "Epoch [50/50], Step [42/334], Loss: 0.6538\n",
      "Epoch [50/50], Step [43/334], Loss: 1.2724\n",
      "Epoch [50/50], Step [44/334], Loss: 0.6414\n",
      "Epoch [50/50], Step [45/334], Loss: 0.8153\n",
      "Epoch [50/50], Step [46/334], Loss: 0.4152\n",
      "Epoch [50/50], Step [47/334], Loss: 0.4518\n",
      "Epoch [50/50], Step [48/334], Loss: 0.5848\n",
      "Epoch [50/50], Step [49/334], Loss: 0.6956\n",
      "Epoch [50/50], Step [50/334], Loss: 0.7626\n",
      "Epoch [50/50], Step [51/334], Loss: 0.9221\n",
      "Epoch [50/50], Step [52/334], Loss: 0.7807\n",
      "Epoch [50/50], Step [53/334], Loss: 0.7885\n",
      "Epoch [50/50], Step [54/334], Loss: 0.5079\n",
      "Epoch [50/50], Step [55/334], Loss: 0.8566\n",
      "Epoch [50/50], Step [56/334], Loss: 0.5968\n",
      "Epoch [50/50], Step [57/334], Loss: 0.4727\n",
      "Epoch [50/50], Step [58/334], Loss: 0.6383\n",
      "Epoch [50/50], Step [59/334], Loss: 0.5194\n",
      "Epoch [50/50], Step [60/334], Loss: 0.5869\n",
      "Epoch [50/50], Step [61/334], Loss: 0.8514\n",
      "Epoch [50/50], Step [62/334], Loss: 0.6344\n",
      "Epoch [50/50], Step [63/334], Loss: 0.3179\n",
      "Epoch [50/50], Step [64/334], Loss: 0.6496\n",
      "Epoch [50/50], Step [65/334], Loss: 0.5502\n",
      "Epoch [50/50], Step [66/334], Loss: 0.4077\n",
      "Epoch [50/50], Step [67/334], Loss: 0.5187\n",
      "Epoch [50/50], Step [68/334], Loss: 0.7975\n",
      "Epoch [50/50], Step [69/334], Loss: 0.6559\n",
      "Epoch [50/50], Step [70/334], Loss: 0.7598\n",
      "Epoch [50/50], Step [71/334], Loss: 0.9282\n",
      "Epoch [50/50], Step [72/334], Loss: 0.4446\n",
      "Epoch [50/50], Step [73/334], Loss: 1.1417\n",
      "Epoch [50/50], Step [74/334], Loss: 0.5653\n",
      "Epoch [50/50], Step [75/334], Loss: 0.3940\n",
      "Epoch [50/50], Step [76/334], Loss: 0.5056\n",
      "Epoch [50/50], Step [77/334], Loss: 0.3570\n",
      "Epoch [50/50], Step [78/334], Loss: 0.7158\n",
      "Epoch [50/50], Step [79/334], Loss: 0.4643\n",
      "Epoch [50/50], Step [80/334], Loss: 0.3885\n",
      "Epoch [50/50], Step [81/334], Loss: 0.9415\n",
      "Epoch [50/50], Step [82/334], Loss: 1.0046\n",
      "Epoch [50/50], Step [83/334], Loss: 0.5724\n",
      "Epoch [50/50], Step [84/334], Loss: 0.8118\n",
      "Epoch [50/50], Step [85/334], Loss: 0.5234\n",
      "Epoch [50/50], Step [86/334], Loss: 0.5307\n",
      "Epoch [50/50], Step [87/334], Loss: 0.8958\n",
      "Epoch [50/50], Step [88/334], Loss: 0.3673\n",
      "Epoch [50/50], Step [89/334], Loss: 0.8193\n",
      "Epoch [50/50], Step [90/334], Loss: 1.1861\n",
      "Epoch [50/50], Step [91/334], Loss: 0.4218\n",
      "Epoch [50/50], Step [92/334], Loss: 1.1553\n",
      "Epoch [50/50], Step [93/334], Loss: 0.7019\n",
      "Epoch [50/50], Step [94/334], Loss: 0.9491\n",
      "Epoch [50/50], Step [95/334], Loss: 0.8743\n",
      "Epoch [50/50], Step [96/334], Loss: 0.4845\n",
      "Epoch [50/50], Step [97/334], Loss: 0.5875\n",
      "Epoch [50/50], Step [98/334], Loss: 0.5319\n",
      "Epoch [50/50], Step [99/334], Loss: 0.4470\n",
      "Epoch [50/50], Step [100/334], Loss: 0.9531\n",
      "Epoch [50/50], Step [101/334], Loss: 0.3297\n",
      "Epoch [50/50], Step [102/334], Loss: 0.8223\n",
      "Epoch [50/50], Step [103/334], Loss: 0.5720\n",
      "Epoch [50/50], Step [104/334], Loss: 0.4497\n",
      "Epoch [50/50], Step [105/334], Loss: 1.9313\n",
      "Epoch [50/50], Step [106/334], Loss: 1.2330\n",
      "Epoch [50/50], Step [107/334], Loss: 1.4670\n",
      "Epoch [50/50], Step [108/334], Loss: 0.6536\n",
      "Epoch [50/50], Step [109/334], Loss: 1.9784\n",
      "Epoch [50/50], Step [110/334], Loss: 0.3504\n",
      "Epoch [50/50], Step [111/334], Loss: 0.5243\n",
      "Epoch [50/50], Step [112/334], Loss: 1.0654\n",
      "Epoch [50/50], Step [113/334], Loss: 0.4083\n",
      "Epoch [50/50], Step [114/334], Loss: 1.1998\n",
      "Epoch [50/50], Step [115/334], Loss: 0.9383\n",
      "Epoch [50/50], Step [116/334], Loss: 0.6583\n",
      "Epoch [50/50], Step [117/334], Loss: 1.9859\n",
      "Epoch [50/50], Step [118/334], Loss: 0.6187\n",
      "Epoch [50/50], Step [119/334], Loss: 0.5849\n",
      "Epoch [50/50], Step [120/334], Loss: 1.4842\n",
      "Epoch [50/50], Step [121/334], Loss: 0.8020\n",
      "Epoch [50/50], Step [122/334], Loss: 3.6885\n",
      "Epoch [50/50], Step [123/334], Loss: 0.6709\n",
      "Epoch [50/50], Step [124/334], Loss: 2.8666\n",
      "Epoch [50/50], Step [125/334], Loss: 1.2367\n",
      "Epoch [50/50], Step [126/334], Loss: 1.0268\n",
      "Epoch [50/50], Step [127/334], Loss: 1.5705\n",
      "Epoch [50/50], Step [128/334], Loss: 0.6744\n",
      "Epoch [50/50], Step [129/334], Loss: 1.1826\n",
      "Epoch [50/50], Step [130/334], Loss: 1.1502\n",
      "Epoch [50/50], Step [131/334], Loss: 0.7857\n",
      "Epoch [50/50], Step [132/334], Loss: 0.6960\n",
      "Epoch [50/50], Step [133/334], Loss: 0.8545\n",
      "Epoch [50/50], Step [134/334], Loss: 1.6607\n",
      "Epoch [50/50], Step [135/334], Loss: 0.4152\n",
      "Epoch [50/50], Step [136/334], Loss: 0.8259\n",
      "Epoch [50/50], Step [137/334], Loss: 1.3984\n",
      "Epoch [50/50], Step [138/334], Loss: 0.6390\n",
      "Epoch [50/50], Step [139/334], Loss: 0.7903\n",
      "Epoch [50/50], Step [140/334], Loss: 1.9660\n",
      "Epoch [50/50], Step [141/334], Loss: 1.2311\n",
      "Epoch [50/50], Step [142/334], Loss: 1.9817\n",
      "Epoch [50/50], Step [143/334], Loss: 1.2485\n",
      "Epoch [50/50], Step [144/334], Loss: 1.8133\n",
      "Epoch [50/50], Step [145/334], Loss: 0.5986\n",
      "Epoch [50/50], Step [146/334], Loss: 0.6928\n",
      "Epoch [50/50], Step [147/334], Loss: 0.9009\n",
      "Epoch [50/50], Step [148/334], Loss: 0.9933\n",
      "Epoch [50/50], Step [149/334], Loss: 0.7325\n",
      "Epoch [50/50], Step [150/334], Loss: 0.7515\n",
      "Epoch [50/50], Step [151/334], Loss: 0.7597\n",
      "Epoch [50/50], Step [152/334], Loss: 0.4982\n",
      "Epoch [50/50], Step [153/334], Loss: 0.6821\n",
      "Epoch [50/50], Step [154/334], Loss: 0.4761\n",
      "Epoch [50/50], Step [155/334], Loss: 0.5151\n",
      "Epoch [50/50], Step [156/334], Loss: 0.7064\n",
      "Epoch [50/50], Step [157/334], Loss: 0.6089\n",
      "Epoch [50/50], Step [158/334], Loss: 1.2550\n",
      "Epoch [50/50], Step [159/334], Loss: 0.2691\n",
      "Epoch [50/50], Step [160/334], Loss: 0.7782\n",
      "Epoch [50/50], Step [161/334], Loss: 1.7000\n",
      "Epoch [50/50], Step [162/334], Loss: 0.6674\n",
      "Epoch [50/50], Step [163/334], Loss: 1.0356\n",
      "Epoch [50/50], Step [164/334], Loss: 0.6067\n",
      "Epoch [50/50], Step [165/334], Loss: 0.8987\n",
      "Epoch [50/50], Step [166/334], Loss: 0.9185\n",
      "Epoch [50/50], Step [167/334], Loss: 0.8633\n",
      "Epoch [50/50], Step [168/334], Loss: 1.2356\n",
      "Epoch [50/50], Step [169/334], Loss: 0.7937\n",
      "Epoch [50/50], Step [170/334], Loss: 0.5531\n",
      "Epoch [50/50], Step [171/334], Loss: 0.5735\n",
      "Epoch [50/50], Step [172/334], Loss: 0.6557\n",
      "Epoch [50/50], Step [173/334], Loss: 0.7970\n",
      "Epoch [50/50], Step [174/334], Loss: 0.5239\n",
      "Epoch [50/50], Step [175/334], Loss: 0.7676\n",
      "Epoch [50/50], Step [176/334], Loss: 0.5246\n",
      "Epoch [50/50], Step [177/334], Loss: 0.7089\n",
      "Epoch [50/50], Step [178/334], Loss: 0.6008\n",
      "Epoch [50/50], Step [179/334], Loss: 0.7945\n",
      "Epoch [50/50], Step [180/334], Loss: 0.4394\n",
      "Epoch [50/50], Step [181/334], Loss: 0.8323\n",
      "Epoch [50/50], Step [182/334], Loss: 0.7083\n",
      "Epoch [50/50], Step [183/334], Loss: 0.2902\n",
      "Epoch [50/50], Step [184/334], Loss: 0.6858\n",
      "Epoch [50/50], Step [185/334], Loss: 0.5868\n",
      "Epoch [50/50], Step [186/334], Loss: 1.4036\n",
      "Epoch [50/50], Step [187/334], Loss: 0.6510\n",
      "Epoch [50/50], Step [188/334], Loss: 0.5701\n",
      "Epoch [50/50], Step [189/334], Loss: 0.9239\n",
      "Epoch [50/50], Step [190/334], Loss: 1.1847\n",
      "Epoch [50/50], Step [191/334], Loss: 0.7334\n",
      "Epoch [50/50], Step [192/334], Loss: 1.4802\n",
      "Epoch [50/50], Step [193/334], Loss: 0.7656\n",
      "Epoch [50/50], Step [194/334], Loss: 1.6566\n",
      "Epoch [50/50], Step [195/334], Loss: 1.6234\n",
      "Epoch [50/50], Step [196/334], Loss: 0.3773\n",
      "Epoch [50/50], Step [197/334], Loss: 1.6955\n",
      "Epoch [50/50], Step [198/334], Loss: 2.8346\n",
      "Epoch [50/50], Step [199/334], Loss: 1.5229\n",
      "Epoch [50/50], Step [200/334], Loss: 3.4833\n",
      "Epoch [50/50], Step [201/334], Loss: 1.0829\n",
      "Epoch [50/50], Step [202/334], Loss: 1.0905\n",
      "Epoch [50/50], Step [203/334], Loss: 3.2426\n",
      "Epoch [50/50], Step [204/334], Loss: 1.8564\n",
      "Epoch [50/50], Step [205/334], Loss: 3.7361\n",
      "Epoch [50/50], Step [206/334], Loss: 1.9464\n",
      "Epoch [50/50], Step [207/334], Loss: 1.9141\n",
      "Epoch [50/50], Step [208/334], Loss: 2.7500\n",
      "Epoch [50/50], Step [209/334], Loss: 0.9157\n",
      "Epoch [50/50], Step [210/334], Loss: 1.4141\n",
      "Epoch [50/50], Step [211/334], Loss: 2.1314\n",
      "Epoch [50/50], Step [212/334], Loss: 0.6531\n",
      "Epoch [50/50], Step [213/334], Loss: 2.3390\n",
      "Epoch [50/50], Step [214/334], Loss: 0.7689\n",
      "Epoch [50/50], Step [215/334], Loss: 0.6081\n",
      "Epoch [50/50], Step [216/334], Loss: 1.7664\n",
      "Epoch [50/50], Step [217/334], Loss: 0.6993\n",
      "Epoch [50/50], Step [218/334], Loss: 0.6000\n",
      "Epoch [50/50], Step [219/334], Loss: 0.7764\n",
      "Epoch [50/50], Step [220/334], Loss: 1.0048\n",
      "Epoch [50/50], Step [221/334], Loss: 0.8595\n",
      "Epoch [50/50], Step [222/334], Loss: 1.5044\n",
      "Epoch [50/50], Step [223/334], Loss: 1.3705\n",
      "Epoch [50/50], Step [224/334], Loss: 0.9062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/50], Step [225/334], Loss: 2.1417\n",
      "Epoch [50/50], Step [226/334], Loss: 0.5021\n",
      "Epoch [50/50], Step [227/334], Loss: 0.7301\n",
      "Epoch [50/50], Step [228/334], Loss: 0.6278\n",
      "Epoch [50/50], Step [229/334], Loss: 0.5274\n",
      "Epoch [50/50], Step [230/334], Loss: 0.5800\n",
      "Epoch [50/50], Step [231/334], Loss: 0.7184\n",
      "Epoch [50/50], Step [232/334], Loss: 0.8896\n",
      "Epoch [50/50], Step [233/334], Loss: 0.5214\n",
      "Epoch [50/50], Step [234/334], Loss: 0.8144\n",
      "Epoch [50/50], Step [235/334], Loss: 0.5720\n",
      "Epoch [50/50], Step [236/334], Loss: 0.5084\n",
      "Epoch [50/50], Step [237/334], Loss: 0.9533\n",
      "Epoch [50/50], Step [238/334], Loss: 0.3385\n",
      "Epoch [50/50], Step [239/334], Loss: 0.5615\n",
      "Epoch [50/50], Step [240/334], Loss: 0.6930\n",
      "Epoch [50/50], Step [241/334], Loss: 0.9085\n",
      "Epoch [50/50], Step [242/334], Loss: 1.2746\n",
      "Epoch [50/50], Step [243/334], Loss: 0.4366\n",
      "Epoch [50/50], Step [244/334], Loss: 0.3180\n",
      "Epoch [50/50], Step [245/334], Loss: 1.4716\n",
      "Epoch [50/50], Step [246/334], Loss: 0.5702\n",
      "Epoch [50/50], Step [247/334], Loss: 1.3192\n",
      "Epoch [50/50], Step [248/334], Loss: 0.6715\n",
      "Epoch [50/50], Step [249/334], Loss: 0.5481\n",
      "Epoch [50/50], Step [250/334], Loss: 0.3848\n",
      "Epoch [50/50], Step [251/334], Loss: 0.6858\n",
      "Epoch [50/50], Step [252/334], Loss: 0.7503\n",
      "Epoch [50/50], Step [253/334], Loss: 0.8760\n",
      "Epoch [50/50], Step [254/334], Loss: 0.7330\n",
      "Epoch [50/50], Step [255/334], Loss: 0.4868\n",
      "Epoch [50/50], Step [256/334], Loss: 0.5688\n",
      "Epoch [50/50], Step [257/334], Loss: 0.8528\n",
      "Epoch [50/50], Step [258/334], Loss: 0.5000\n",
      "Epoch [50/50], Step [259/334], Loss: 1.1615\n",
      "Epoch [50/50], Step [260/334], Loss: 0.4113\n",
      "Epoch [50/50], Step [261/334], Loss: 0.4655\n",
      "Epoch [50/50], Step [262/334], Loss: 0.7260\n",
      "Epoch [50/50], Step [263/334], Loss: 0.6855\n",
      "Epoch [50/50], Step [264/334], Loss: 0.8276\n",
      "Epoch [50/50], Step [265/334], Loss: 1.5127\n",
      "Epoch [50/50], Step [266/334], Loss: 0.3955\n",
      "Epoch [50/50], Step [267/334], Loss: 0.9163\n",
      "Epoch [50/50], Step [268/334], Loss: 0.4784\n",
      "Epoch [50/50], Step [269/334], Loss: 0.4101\n",
      "Epoch [50/50], Step [270/334], Loss: 1.7756\n",
      "Epoch [50/50], Step [271/334], Loss: 0.4290\n",
      "Epoch [50/50], Step [272/334], Loss: 0.3650\n",
      "Epoch [50/50], Step [273/334], Loss: 0.8119\n",
      "Epoch [50/50], Step [274/334], Loss: 0.8422\n",
      "Epoch [50/50], Step [275/334], Loss: 1.3362\n",
      "Epoch [50/50], Step [276/334], Loss: 0.5128\n",
      "Epoch [50/50], Step [277/334], Loss: 0.7772\n",
      "Epoch [50/50], Step [278/334], Loss: 1.0238\n",
      "Epoch [50/50], Step [279/334], Loss: 0.8533\n",
      "Epoch [50/50], Step [280/334], Loss: 0.6729\n",
      "Epoch [50/50], Step [281/334], Loss: 0.7174\n",
      "Epoch [50/50], Step [282/334], Loss: 0.8290\n",
      "Epoch [50/50], Step [283/334], Loss: 0.7329\n",
      "Epoch [50/50], Step [284/334], Loss: 2.8205\n",
      "Epoch [50/50], Step [285/334], Loss: 1.3519\n",
      "Epoch [50/50], Step [286/334], Loss: 2.7349\n",
      "Epoch [50/50], Step [287/334], Loss: 0.8325\n",
      "Epoch [50/50], Step [288/334], Loss: 2.2914\n",
      "Epoch [50/50], Step [289/334], Loss: 0.9588\n",
      "Epoch [50/50], Step [290/334], Loss: 1.3431\n",
      "Epoch [50/50], Step [291/334], Loss: 1.6491\n",
      "Epoch [50/50], Step [292/334], Loss: 0.8626\n",
      "Epoch [50/50], Step [293/334], Loss: 1.6685\n",
      "Epoch [50/50], Step [294/334], Loss: 1.2931\n",
      "Epoch [50/50], Step [295/334], Loss: 0.5788\n",
      "Epoch [50/50], Step [296/334], Loss: 2.5799\n",
      "Epoch [50/50], Step [297/334], Loss: 0.9529\n",
      "Epoch [50/50], Step [298/334], Loss: 0.9940\n",
      "Epoch [50/50], Step [299/334], Loss: 1.4145\n",
      "Epoch [50/50], Step [300/334], Loss: 1.1033\n",
      "Epoch [50/50], Step [301/334], Loss: 1.3677\n",
      "Epoch [50/50], Step [302/334], Loss: 0.9575\n",
      "Epoch [50/50], Step [303/334], Loss: 0.7299\n",
      "Epoch [50/50], Step [304/334], Loss: 1.7847\n",
      "Epoch [50/50], Step [305/334], Loss: 1.5510\n",
      "Epoch [50/50], Step [306/334], Loss: 0.5897\n",
      "Epoch [50/50], Step [307/334], Loss: 0.9737\n",
      "Epoch [50/50], Step [308/334], Loss: 1.8023\n",
      "Epoch [50/50], Step [309/334], Loss: 0.7926\n",
      "Epoch [50/50], Step [310/334], Loss: 1.3949\n",
      "Epoch [50/50], Step [311/334], Loss: 1.7229\n",
      "Epoch [50/50], Step [312/334], Loss: 0.6272\n",
      "Epoch [50/50], Step [313/334], Loss: 0.4317\n",
      "Epoch [50/50], Step [314/334], Loss: 1.5555\n",
      "Epoch [50/50], Step [315/334], Loss: 0.8612\n",
      "Epoch [50/50], Step [316/334], Loss: 1.6935\n",
      "Epoch [50/50], Step [317/334], Loss: 1.7433\n",
      "Epoch [50/50], Step [318/334], Loss: 0.7118\n",
      "Epoch [50/50], Step [319/334], Loss: 1.2851\n",
      "Epoch [50/50], Step [320/334], Loss: 1.5021\n",
      "Epoch [50/50], Step [321/334], Loss: 1.3296\n",
      "Epoch [50/50], Step [322/334], Loss: 4.1745\n",
      "Epoch [50/50], Step [323/334], Loss: 0.6610\n",
      "Epoch [50/50], Step [324/334], Loss: 2.6516\n",
      "Epoch [50/50], Step [325/334], Loss: 1.9885\n",
      "Epoch [50/50], Step [326/334], Loss: 0.6785\n",
      "Epoch [50/50], Step [327/334], Loss: 2.1690\n",
      "Epoch [50/50], Step [328/334], Loss: 3.0398\n",
      "Epoch [50/50], Step [329/334], Loss: 1.4696\n",
      "Epoch [50/50], Step [330/334], Loss: 2.8419\n",
      "Epoch [50/50], Step [331/334], Loss: 2.5469\n",
      "Epoch [50/50], Step [332/334], Loss: 1.2027\n",
      "Epoch [50/50], Step [333/334], Loss: 3.1122\n",
      "Epoch [50/50], Step [334/334], Loss: 0.7346\n"
     ]
    }
   ],
   "source": [
    "losses_list = []\n",
    "ct = 0\n",
    "print('Training model now...')\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i_batch, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images).to(device)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #if (i_batch+1) % 1 == 0:\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1,\n",
    "                                                                      num_epochs,\n",
    "                                                                      i_batch+1,\n",
    "                                                                      total_step,\n",
    "                                                                      loss.item()))\n",
    "        losses_list.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "334\n",
      "20\n",
      "20\n",
      "i_batch 0\n",
      "i_batch 1\n",
      "i_batch 2\n",
      "i_batch 3\n",
      "i_batch 4\n",
      "i_batch 5\n",
      "i_batch 6\n",
      "i_batch 7\n",
      "i_batch 8\n",
      "i_batch 9\n",
      "i_batch 10\n",
      "i_batch 11\n",
      "i_batch 12\n",
      "i_batch 13\n",
      "i_batch 14\n",
      "i_batch 15\n",
      "i_batch 16\n",
      "i_batch 17\n",
      "i_batch 18\n",
      "i_batch 19\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "model.to(device).eval()  \n",
    "# eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "\n",
    "# evaluation: MSE loss of center predictions\n",
    "# evaluation: MSE loss of degree predictions\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "print(len(test_loader))\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))\n",
    "print(len(test_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_err = 0\n",
    "    n_total = 0\n",
    "    for i_batch, (images, labels) in enumerate(test_loader):\n",
    "    #for i_batch, (images, labels) in enumerate(train_loader):\n",
    "        print('i_batch', i_batch)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images).to(device)\n",
    "        diff = outputs - labels\n",
    "        diff = torch.sum(diff, 0) #column sum\n",
    "        total_err += diff\n",
    "        #loss = criterion(outputs, labels)\n",
    "        #print(loss)\n",
    "        #x, y, orient = outputs.squeeze() # remove extra dim, so not nested list of list\n",
    "        #true_x, true_y, true_orient = labels.squeeze()\n",
    "        #xy_err +=criterion( torch.Tensor([x,y]), torch.Tensor([true_x, true_y]))\n",
    "        #orient_err += criterion(orient, true_orient)\n",
    "        #print(n_total)\n",
    "        #n_total += 1\n",
    "        #_, predicted = torch.max(outputs.data, 1)\n",
    "        #total += labels.size(0)\n",
    "        #correct += (predicted == labels).sum().item()\n",
    "print(n_total * batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAADICAIAAAAiOjnJAAAA1UlEQVR4nO3auQ2AMBAAQYzov2WTEyIt70wBvpO8keVlAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgaY9QjtnoAp11w/Z317gX4JmGREBYJYZEQFglhkRAWCWGREBYJYT3XnLM7vH7WFxYJYZEQFglhkRAWCWGREBYJYZEQFvzPq7+9AwAAAAAAAAAAAAAAAAAAAAAAAAAAADzADrRkAzSNaG7+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=200x200 at 0x7F9B5DFE6D68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-28.7152, 299.4125,  84.1611], device='cuda:0')\n",
      "tensor([-0.0957,  0.9980,  0.2805], device='cuda:0')\n",
      "\n",
      " ----------------------------\n",
      "Across a total of 300 images, the average error was -0.10 and 1.00 pixels for x and y, and 16.07 degrees for orientation\n",
      "\n",
      " ----------------------------\n"
     ]
    }
   ],
   "source": [
    "n_total = len(test_loader.dataset)\n",
    "\n",
    "print(n_total)\n",
    "from IPython.display import display # to display images\n",
    "\n",
    "import torchvision\n",
    "img = torchvision.transforms.ToPILImage()(images[0].cpu())\n",
    "display(img)\n",
    "\n",
    "avg_err = total_err / n_total \n",
    "print(total_err)\n",
    "print(avg_err)\n",
    "xerr, yerr, orienterr = avg_err\n",
    "print('\\n ----------------------------')\n",
    "print('Across a total of %d images, the average error was %0.2f and %0.2f pixels for x and y, \\\n",
    "and %0.2f degrees for orientation' % (n_total, xerr, yerr, math.degrees(orienterr)))\n",
    "print('\\n ----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display # to display images\n",
    "#font = ImageFont.truetype(\"Arial.ttf\",14) # OSError: cannot open resource\n",
    "font = ImageFont.truetype(\"/usr/share/fonts/dejavu/DejaVuSans.ttf\",14)\n",
    "def imshow_coord(img, a_label):\n",
    "    img = torchvision.transforms.ToPILImage()(img)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    #draw.text((10, 25), '(' + np.array2string(np.around(a_label.numpy()), separator=', ') + ')')\n",
    "    x,y,orient = a_label\n",
    "    rect_vertices = makeRectangle(5, 15, orient, offset=(x,y))\n",
    "    draw.polygon(rect_vertices, fill='red')\n",
    "    draw = draw_crosspointer(a_label, draw, 'green', 'white', length=8)\n",
    "    display(img)\n",
    "    \n",
    "def draw_crosspointer(xy, draw, fillcolor_X='green', fillcolor='white', length=2):\n",
    "    a,b, orient = tuple(xy)\n",
    "    draw.line((a-length, b+length, a+length, b-length), fill=fillcolor_X)\n",
    "    draw.line((a-length, b-length, a+length, b+length), fill=fillcolor_X)\n",
    "    draw.point((a,b))\n",
    "    return draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torchvision \n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "# get some random training images\n",
    "with torch.no_grad():\n",
    "    dataiter = iter(train_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    outputs = model(images)\n",
    "    \n",
    "    # show images\n",
    "    #imshow_coord(torchvision.utils.make_grid(images), (outputs))\n",
    "\n",
    "\n",
    "    # print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                                  # for j in range(4)))\n",
    "        \n",
    "    loss = criterion(outputs.to(device), labels.to(device))\n",
    " \n",
    "n =0 \n",
    "#print(len(labels))\n",
    "print('x,y truth', labels[n])\n",
    "print('x,y guess by net', outputs[n])\n",
    "\n",
    "images_copy = copy.deepcopy(images)\n",
    "print('image size: ', images_copy[n].size())\n",
    "print(loss)\n",
    "print('net output: ')\n",
    "imshow_coord(images_copy[n], outputs[n])\n",
    "print('truth label: ')\n",
    "imshow_coord(images_copy[n], labels[n])\n",
    "\n",
    "#imagePIL = torchvision.transforms.ToPILImage()(images[n])\n",
    "#print('PIL image size', imagePIL.size)\n",
    "#imagePIL.save('test.png')\n",
    "#display(imagePIL)\n",
    "\n",
    "#display(Image.open('./data/rect'+str(n)+'.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#im = Image.open(\"Mew.jpg\")\n",
    "n = 12\n",
    "n = np.random.rand()*len(images)\n",
    "n = int(n)\n",
    "\n",
    "image = io.imread('./data/rect'+str(n)+'.png')\n",
    "image_tensor = torch.FloatTensor(image).permute(2, 0, 1) #PIL and torch expect difft orders\n",
    "coords = torch.FloatTensor(true_coords[n])\n",
    "with torch.no_grad():\n",
    "    output = model(image_tensor)\n",
    "\n",
    "print(true_coords[n])\n",
    "print(output)\n",
    "x,y,orient = output.cpu().numpy().flatten()\n",
    "imshow_coord(image, (x,y, orient))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#img = Image.new('RGB', (300,400), 'gray')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"ticks\")\n",
    "n = 0\n",
    "#imshow_coord(images[n], outputs[n], labels[n])\n",
    "\n",
    "print(images.size())\n",
    "a = torchvision.utils.make_grid(images)\n",
    "print(a.max(), a.min(), a.size())\n",
    "#a = a / 2 + 0.5     # unnormalize\n",
    "\n",
    "a = a.cpu().numpy()\n",
    "a = np.transpose(a,(1,2,0))\n",
    "print(a.shape)\n",
    "print(a.ndim)\n",
    "\n",
    "#ran = a.max() - a.min()\n",
    "#a = (a/ran )\n",
    "\n",
    "fig,ax = plt.subplots(1)\n",
    "# Display the image\n",
    "ax.imshow(a)\n",
    "#x,y = labels[0].numpy()\n",
    "#plt.scatter(x,y, color='g', marker='x', linewidth='1')\n",
    "labels = labels.cpu()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [30,5]\n",
    "import matplotlib.patches as patches\n",
    "for i in range(len(labels)):\n",
    "    x,y, orient = labels[i].numpy()\n",
    "\n",
    "# class matplotlib.patches.Rectangle(xy, width, height, angle=0.0, **kwargs)[source]\n",
    "\n",
    "    rect = patches.Rectangle((x + i*IMG_X, y), 1,20,angle=orient, fill=True, color='orange')\n",
    "    ax.add_patch(rect)\n",
    "    ax.scatter(x + i*IMG_X, y, color='r', marker='x', linewidth='1')\n",
    "#plt.imshow(np.transpose(a, (2,0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "f = plt.subplot()\n",
    "f.plot(range(len(losses_list)), losses_list)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [30, 5]\n",
    "xposition = np.array(range(num_epoch))  * (total_step)\n",
    "#print(xposition)\n",
    "for xc in xposition:\n",
    "    #plt.axvline(x=xc, color='k', linestyle='--')\n",
    "    pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = plt.subplot()\n",
    "trunc = 100\n",
    "g.plot(range(trunc), losses_list[-trunc:])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't run the following cell (takes long time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = [10,20,50,70,100,150] \n",
    "num_classes = 3 # predicting x,y,orientation\n",
    "batch_size = 5 \n",
    "learning_rates = [0.001, 0.005, 0.01, 0.1]\n",
    "\n",
    "meta_losses_list = []\n",
    "for i in range(len(num_epochs)):\n",
    "    num_epoch = num_epochs[i]\n",
    "    learning_rate = learning_rates[0]\n",
    "    losses_list = []\n",
    "    print('Training model now... with: %d num_epoch and %d learning rate' % (num_epoch, learning_rate))\n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epoch):\n",
    "        for i_batch, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            # magnify orientation loss\n",
    "            outputs[2] *= 10\n",
    "            labels[2] *= 10\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i_batch+1) % 1 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1,\n",
    "                                                                          num_epochs,\n",
    "                                                                          i_batch+1,\n",
    "                                                                          total_step,\n",
    "                                                                          loss.item()))\n",
    "                losses_list.append(loss.item())\n",
    "    meta_losses_list.append(losses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots()\n",
    "trunc = 100\n",
    "for l in meta_losses_list:\n",
    "    plt.plot(range(len(l[-trunc:])), l[-trunc:])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
