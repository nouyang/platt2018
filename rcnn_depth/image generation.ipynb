{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import io\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IMG_X, IMG_Y = 200,200 \n",
    "# length and width of blocks (fixed for now)\n",
    "block_l, block_w = 20, 30\n",
    "\n",
    "# img_list = []\n",
    "true_coords = []\n",
    "\n",
    "\n",
    "# Calc rectangle vertices. makeRectangle() credit Sparkler, stackoverflow, feb 17\n",
    "def makeRectangle(l, w, theta, offset=(0, 0)):\n",
    "    c, s = math.cos(theta), math.sin(theta)\n",
    "    rectCoords = [(l/2.0, w/2.0), (l/2.0, -w/2.0), (-l/2.0, -w/2.0), (-l/2.0, w/2.0)]\n",
    "    return [(c*x-s*y+offset[0], s*x+c*y+offset[1]) for (x, y) in rectCoords]\n",
    "\n",
    "\n",
    "# ---- Make depth images ---\n",
    "def make_dataset(dirname, num_images):\n",
    "    newpath = './' + dirname  \n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "        print(newpath)\n",
    "    for i in range(num_images):\n",
    "        #orient = 0 # degrees\n",
    "        img = Image.new('RGB', (IMG_X, IMG_Y), 'black')\n",
    "\n",
    "        # block_l and _w offset so blocks don't run off edge of image \n",
    "        rand_x = int(np.random.rand() * (IMG_X-2*block_l)) + block_l\n",
    "        rand_y = int(np.random.rand() * (IMG_Y-2*block_w)) + block_w\n",
    "        orient = int(np.random.rand() * 180)  # .random() is range [0.0, 1.0).\n",
    "        orient = math.radians(orient) # math.cos takes radians!\n",
    "\n",
    "        true_coords.append(np.array((rand_x, rand_y, orient)))\n",
    "\n",
    "        rect_vertices = makeRectangle(block_l, block_w, orient, offset=(rand_x,\n",
    "                                                                        rand_y))\n",
    "\n",
    "        idraw = ImageDraw.Draw(img)\n",
    "        idraw.polygon(rect_vertices, fill='white')\n",
    "\n",
    "        # use a truetype font\n",
    "        #font = imagefont.truetype(\"dejavusans.ttf\", 15)\n",
    "        #font = imagefont.truetype(\"arial.ttf\",14)\n",
    "        #idraw.text((10, 25), '('+ str(rand_x) + ', ' + str(rand_y) +')')\n",
    "        img.save(newpath + '/rect'+str(i)+'.png')\n",
    "\n",
    "make_dataset('data', 500)\n",
    "make_dataset('data/test', 100)\n",
    "    \n",
    "class RectDepthImgsDataset(Dataset):\n",
    "    \"\"\"Artificially generated depth images dataset\"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, coords, transform=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.true_coords = coords\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        print(len(self.true_coords))\n",
    "        return len(self.true_coords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # image = self.images[idx]\n",
    "        image = io.imread(self.img_dir + '/rect'+str(idx)+'.png')\n",
    "        image = torch.FloatTensor(image).permute(2, 0, 1) #PIL and torch expect difft orders\n",
    "        coords = torch.FloatTensor(true_coords[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # sample = {'image': image, 'grasp': str(coords[0]) + str(coords[1])}\n",
    "        sample = {'image': image, 'grasp': coords}\n",
    "        sample = image, coords\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available? device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameters\n",
    "\n",
    "num_epochs = 50 \n",
    "num_classes = 3 # predicting x,y,orientation\n",
    "batch_size = 15\n",
    "learning_rate = 0.001\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA available? device: \", device)\n",
    "\n",
    "# Dataset is depth images of rectangular blocks\n",
    "train_dataset = RectDepthImgsDataset(img_dir='./data', coords=true_coords)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "\n",
    "test_dataset = RectDepthImgsDataset(img_dir='./data/test', coords=true_coords)\n",
    "\n",
    "# Data loader\n",
    "test_loader = DataLoader(dataset=train_dataset, batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "const = 16*47*47\n",
    "#const = 16*97*72\n",
    "\n",
    "\n",
    "# 300x300\n",
    "#torch.Size([4, 300, 300, 3])\n",
    "#torch.Size([4, 3, 300, 300])\n",
    "#torch.Size([4, 6, 148, 148])\n",
    "#torch.Size([4, 16, 72, 72])\n",
    "\n",
    "\n",
    "# 200x200\n",
    "#const = 16*47*47*4\n",
    "#torch.Size([4, 200, 200, 3])\n",
    "#torch.Size([4, 3, 200, 200])\n",
    "#torch.Size([4, 6, 98, 98])\n",
    "#torch.Size([4, 16, 47, 47])\n",
    "\n",
    "# = 200-4 / 2\n",
    "# where 4 = 5x5 (conv) -1 = area of image after conv without padding = 200->98\n",
    "# = 98-4 / 2\n",
    "# where 4 = 5x5 (conv) -1 = area of image after conv without padding = 98->47\n",
    "# output is 47x47 image, with 16 output channels\n",
    "\n",
    "\n",
    "# 400x300\n",
    "#torch.Size([4, 300, 400, 3])\n",
    "#torch.Size([4, 3, 400, 300])\n",
    "#torch.Size([4, 6, 198, 148])\n",
    "#torch.Size([4, 16, 97, 72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):  # CIFAR is 32x32x3, MNIST is 28x28x1)\n",
    "    def __init__(self, IMG_X, IMG_Y):\n",
    "        super(Net, self).__init__()\n",
    "        self._imgx = IMG_X\n",
    "        self._imgy = IMG_Y\n",
    "        _pool = 2\n",
    "        _stride = 5\n",
    "        _outputlayers = 16\n",
    "        \n",
    "        def _calc(val):\n",
    "            layer_size = (val- (_stride-1)) / _pool\n",
    "            return layer_size \n",
    "        \n",
    "        #print(self._imgx)\n",
    "        self._const = _calc(_calc(self._imgx))\n",
    "        self._const *= _calc(_calc(self._imgy))\n",
    "        self._const *= _outputlayers \n",
    "        #print(self._const)\n",
    "        self._const = int(self._const)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 6, _stride).to(device)\n",
    "        self.pool = nn.MaxPool2d(_pool, _pool).to(device)\n",
    "        self.conv2 = nn.Conv2d(6, _outputlayers, _stride).to(device)\n",
    "        self.fc1 = nn.Linear(self._const, 120).to(device)\n",
    "        self.fc2 = nn.Linear(120, 84).to(device)\n",
    "        self.fc3 = nn.Linear(84, num_classes).to(device)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        x = x.to(device)\n",
    "        x = x.view(-1, 3, IMG_X, IMG_Y)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self._const)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net(IMG_X, IMG_Y)\n",
    "model = model.to(device)\n",
    "\n",
    "# ONLY FOR DEBUGGING (check if code runs at all)\n",
    "#images = iter(train_loader)\n",
    "##outputs = model(images.next()[0])\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model now...\n",
      "600\n",
      "600\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/rect576.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0426fc08e7cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtotal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlenv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlenv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a765638d4f88>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# image = self.images[idx]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/rect'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#PIL and torch expect difft orders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_coords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlenv/lib/python3.6/site-packages/skimage/io/_io.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, flatten, **plugin_args)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_or_url_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imread'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlenv/lib/python3.6/site-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                                (plugin, kind))\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlenv/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, dtype, img_num, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \"\"\"\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpil_to_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/rect576.png'"
     ]
    }
   ],
   "source": [
    "losses_list = []\n",
    "print('Training model now...')\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i_batch, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images).to(device)\n",
    "        # magnify orientation loss\n",
    "        outputs[2] *= 1 \n",
    "        labels[2] *= 1 \n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #if (i_batch+1) % 1 == 0:\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1,\n",
    "                                                                      num_epochs,\n",
    "                                                                      i_batch+1,\n",
    "                                                                      total_step,\n",
    "                                                                      loss.item()))\n",
    "            losses_list.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  \n",
    "# eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "\n",
    "# evaluation: MSE loss of center predictions\n",
    "# evaluation: MSE loss of degree predictions\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "xy_err = 0\n",
    "orient_err = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "    #for i_batch, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images).to(device)\n",
    "        x, y, orient = outputs #radians\n",
    "        true_x, true_y, true_orient = labels \n",
    "        xy_err += criterion((x, y), (true_x, true_y))\n",
    "        orient_err += criterion((orient), (true_oreint))\n",
    "        \n",
    "        #_, predicted = torch.max(outputs.data, 1)\n",
    "        #total += labels.size(0)\n",
    "        #correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# diagnose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display # to display images\n",
    "#font = ImageFont.truetype(\"Arial.ttf\",14) # OSError: cannot open resource\n",
    "font = ImageFont.truetype(\"/usr/share/fonts/dejavu/DejaVuSans.ttf\",14)\n",
    "def imshow_coord(img, a_label):\n",
    "    img = torchvision.transforms.ToPILImage()(img)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    #draw.text((10, 25), '(' + np.array2string(np.around(a_label.numpy()), separator=', ') + ')')\n",
    "    x,y,orient = a_label\n",
    "    rect_vertices = makeRectangle(5, 15, orient, offset=(x,y))\n",
    "    draw.polygon(rect_vertices, fill='red')\n",
    "    draw = draw_crosspointer(a_label, draw, 'green', 'white', length=8)\n",
    "    display(img)\n",
    "    \n",
    "def draw_crosspointer(xy, draw, fillcolor_X='green', fillcolor='white', length=2):\n",
    "    a,b, orient = tuple(xy)\n",
    "    draw.line((a-length, b+length, a+length, b-length), fill=fillcolor_X)\n",
    "    draw.line((a-length, b-length, a+length, b+length), fill=fillcolor_X)\n",
    "    draw.point((a,b))\n",
    "    return draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torchvision \n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "# get some random training images\n",
    "with torch.no_grad():\n",
    "    dataiter = iter(train_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    outputs = model(images)\n",
    "    \n",
    "    # show images\n",
    "    #imshow_coord(torchvision.utils.make_grid(images), (outputs))\n",
    "\n",
    "\n",
    "    # print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                                  # for j in range(4)))\n",
    "        \n",
    "    loss = criterion(outputs.to(device), labels.to(device))\n",
    " \n",
    "n =0 \n",
    "#print(len(labels))\n",
    "print('x,y truth', labels[n])\n",
    "print('x,y guess by net', outputs[n])\n",
    "\n",
    "images_copy = copy.deepcopy(images)\n",
    "print('image size: ', images_copy[n].size())\n",
    "print(loss)\n",
    "print('net output: ')\n",
    "imshow_coord(images_copy[n], outputs[n])\n",
    "print('truth label: ')\n",
    "imshow_coord(images_copy[n], labels[n])\n",
    "\n",
    "#imagePIL = torchvision.transforms.ToPILImage()(images[n])\n",
    "#print('PIL image size', imagePIL.size)\n",
    "#imagePIL.save('test.png')\n",
    "#display(imagePIL)\n",
    "\n",
    "#display(Image.open('./data/rect'+str(n)+'.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#im = Image.open(\"Mew.jpg\")\n",
    "n = 12\n",
    "n = np.random.rand()*len(images)\n",
    "n = int(n)\n",
    "\n",
    "image = io.imread('./data/rect'+str(n)+'.png')\n",
    "image_tensor = torch.FloatTensor(image).permute(2, 0, 1) #PIL and torch expect difft orders\n",
    "coords = torch.FloatTensor(true_coords[n])\n",
    "with torch.no_grad():\n",
    "    output = model(image_tensor)\n",
    "\n",
    "print(true_coords[n])\n",
    "print(output)\n",
    "x,y,orient = output.cpu().numpy().flatten()\n",
    "imshow_coord(image, (x,y, orient))\n",
    "plt.imshow(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#img = Image.new('RGB', (300,400), 'gray')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"ticks\")\n",
    "n = 0\n",
    "#imshow_coord(images[n], outputs[n], labels[n])\n",
    "\n",
    "print(images.size())\n",
    "a = torchvision.utils.make_grid(images)\n",
    "print(a.max(), a.min(), a.size())\n",
    "#a = a / 2 + 0.5     # unnormalize\n",
    "\n",
    "a = a.cpu().numpy()\n",
    "a = np.transpose(a,(1,2,0))\n",
    "print(a.shape)\n",
    "print(a.ndim)\n",
    "\n",
    "#ran = a.max() - a.min()\n",
    "#a = (a/ran )\n",
    "\n",
    "fig,ax = plt.subplots(1)\n",
    "# Display the image\n",
    "ax.imshow(a)\n",
    "#x,y = labels[0].numpy()\n",
    "#plt.scatter(x,y, color='g', marker='x', linewidth='1')\n",
    "labels = labels.cpu()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [30,5]\n",
    "import matplotlib.patches as patches\n",
    "for i in range(len(labels)):\n",
    "    x,y, orient = labels[i].numpy()\n",
    "\n",
    "# class matplotlib.patches.Rectangle(xy, width, height, angle=0.0, **kwargs)[source]\n",
    "\n",
    "    rect = patches.Rectangle((x + i*IMG_X, y), 1,20,angle=orient, fill=True, color='orange')\n",
    "    ax.add_patch(rect)\n",
    "    ax.scatter(x + i*IMG_X, y, color='r', marker='x', linewidth='1')\n",
    "#plt.imshow(np.transpose(a, (2,0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "f = plt.subplot()\n",
    "f.plot(range(len(losses_list)), losses_list)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [30, 5]\n",
    "xposition = np.array(range(num_epoch))  * (total_step)\n",
    "#print(xposition)\n",
    "for xc in xposition:\n",
    "    #plt.axvline(x=xc, color='k', linestyle='--')\n",
    "    pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = plt.subplot()\n",
    "trunc = 100\n",
    "g.plot(range(trunc), losses_list[-trunc:])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't run the following cell (takes long time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = [10,20,50,70,100,150] \n",
    "num_classes = 3 # predicting x,y,orientation\n",
    "batch_size = 5 \n",
    "learning_rates = [0.001, 0.005, 0.01, 0.1]\n",
    "\n",
    "meta_losses_list = []\n",
    "for i in range(len(num_epochs)):\n",
    "    num_epoch = num_epochs[i]\n",
    "    learning_rate = learning_rates[0]\n",
    "    losses_list = []\n",
    "    print('Training model now... with: %d num_epoch and %d learning rate' % (num_epoch, learning_rate))\n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epoch):\n",
    "        for i_batch, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            # magnify orientation loss\n",
    "            outputs[2] *= 10\n",
    "            labels[2] *= 10\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i_batch+1) % 1 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1,\n",
    "                                                                          num_epochs,\n",
    "                                                                          i_batch+1,\n",
    "                                                                          total_step,\n",
    "                                                                          loss.item()))\n",
    "                losses_list.append(loss.item())\n",
    "    meta_losses_list.append(losses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots()\n",
    "trunc = 100\n",
    "for l in meta_losses_list:\n",
    "    plt.plot(range(len(l[-trunc:])), l[-trunc:])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
