{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import io\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "IMG_X, IMG_Y = 300, 400\n",
    "# length and width of blocks (fixed for now)\n",
    "block_l, block_w = 25, 25\n",
    "num_images = 50\n",
    "\n",
    "# img_list = []\n",
    "true_coords = []\n",
    "\n",
    "\n",
    "# Calc rectangle vertices. makeRectangle() credit Sparkler, stackoverflow, feb 17\n",
    "def makeRectangle(l, w, theta, offset=(0, 0)):\n",
    "    c, s = math.cos(theta), math.sin(theta)\n",
    "    rectCoords = [(l/2.0, w/2.0), (l/2.0, -w/2.0), (-l/2.0, -w/2.0), (-l/2.0, w/2.0)]\n",
    "    return [(c*x-s*y+offset[0], s*x+c*y+offset[1]) for (x, y) in rectCoords]\n",
    "\n",
    "\n",
    "# ---- Make depth images ---\n",
    "for i in range(num_images):\n",
    "    orient = 0 # degrees\n",
    "    img = Image.new('RGB', (IMG_X, IMG_Y), 'black')\n",
    "\n",
    "    # block_l and _w offset so blocks don't run off edge of image\n",
    "    rand_x = int(np.random.rand() * (IMG_X-block_l))\n",
    "    rand_y = int(np.random.rand() * (IMG_Y-block_w))\n",
    "\n",
    "    true_coords.append(np.array((rand_x, rand_y)))\n",
    "\n",
    "    rect_vertices = makeRectangle(block_l, block_w, orient, offset=(rand_x,\n",
    "                                                                    rand_y))\n",
    "    idraw = ImageDraw.Draw(img)\n",
    "    idraw.polygon(rect_vertices, fill='white')\n",
    "\n",
    "    # use a truetype font\n",
    "    #font = ImageFont.truetype(\"DejavuSans.ttf\", 15)\n",
    "    #font = ImageFont.truetype(\"Arial.ttf\",14)\n",
    "    #idraw.text((10, 25), '('+ str(rand_x) + ', ' + str(rand_y) +')')\n",
    "\n",
    "\n",
    "    img.save('./data/rect'+str(i)+'.png')\n",
    "\n",
    "\n",
    "    \n",
    "class RectDepthImgsDataset(Dataset):\n",
    "    \"\"\"Artificially generated depth images dataset\"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, coords, transform=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.true_coords = coords\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.true_coords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # image = self.images[idx]\n",
    "        image = io.imread(self.img_dir + '/rect'+str(idx)+'.png')\n",
    "        image = torch.FloatTensor(image).permute(2, 0, 1) #PIL and torch expect difft orders\n",
    "        coords = torch.FloatTensor(true_coords[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # sample = {'image': image, 'grasp': str(coords[0]) + str(coords[1])}\n",
    "        sample = {'image': image, 'grasp': coords}\n",
    "        sample = image, coords\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 35 \n",
    "num_classes = 2\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Dataset is depth images of rectangular blocks\n",
    "train_dataset = RectDepthImgsDataset(img_dir='./data', coords=true_coords)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "const = 16*97*72\n",
    "\n",
    "\n",
    "class Net(nn.Module):  # CIFAR is 32x32x3, MNIST is 28x28x1)\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc1 = nn.Linear(const, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3, 400, 300)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, const)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model now...\n",
      "Epoch [1/35], Step [1/5], Loss: 31865.7852\n",
      "Epoch [1/35], Step [2/5], Loss: 24503.2715\n",
      "Epoch [1/35], Step [3/5], Loss: 23115.9160\n",
      "Epoch [1/35], Step [4/5], Loss: 26240.4609\n",
      "Epoch [1/35], Step [5/5], Loss: 29449.6465\n",
      "Epoch [2/35], Step [1/5], Loss: 8246.0332\n",
      "Epoch [2/35], Step [2/5], Loss: 13452.0244\n",
      "Epoch [2/35], Step [3/5], Loss: 4255.1592\n",
      "Epoch [2/35], Step [4/5], Loss: 7020.5352\n",
      "Epoch [2/35], Step [5/5], Loss: 14358.9551\n",
      "Epoch [3/35], Step [1/5], Loss: 4867.4443\n",
      "Epoch [3/35], Step [2/5], Loss: 3885.5684\n",
      "Epoch [3/35], Step [3/5], Loss: 1988.8433\n",
      "Epoch [3/35], Step [4/5], Loss: 6469.7471\n",
      "Epoch [3/35], Step [5/5], Loss: 5224.8535\n",
      "Epoch [4/35], Step [1/5], Loss: 5343.7568\n",
      "Epoch [4/35], Step [2/5], Loss: 5311.8833\n",
      "Epoch [4/35], Step [3/5], Loss: 5229.7847\n",
      "Epoch [4/35], Step [4/5], Loss: 4639.0986\n",
      "Epoch [4/35], Step [5/5], Loss: 5386.0947\n",
      "Epoch [5/35], Step [1/5], Loss: 2045.7527\n",
      "Epoch [5/35], Step [2/5], Loss: 1245.0614\n",
      "Epoch [5/35], Step [3/5], Loss: 3539.7554\n",
      "Epoch [5/35], Step [4/5], Loss: 3034.9705\n",
      "Epoch [5/35], Step [5/5], Loss: 4443.2515\n",
      "Epoch [6/35], Step [1/5], Loss: 854.4388\n",
      "Epoch [6/35], Step [2/5], Loss: 1316.9373\n",
      "Epoch [6/35], Step [3/5], Loss: 1854.0596\n",
      "Epoch [6/35], Step [4/5], Loss: 494.0562\n",
      "Epoch [6/35], Step [5/5], Loss: 950.4551\n",
      "Epoch [7/35], Step [1/5], Loss: 1583.3461\n",
      "Epoch [7/35], Step [2/5], Loss: 1235.6136\n",
      "Epoch [7/35], Step [3/5], Loss: 1286.9680\n",
      "Epoch [7/35], Step [4/5], Loss: 546.3549\n",
      "Epoch [7/35], Step [5/5], Loss: 769.5959\n",
      "Epoch [8/35], Step [1/5], Loss: 377.3289\n",
      "Epoch [8/35], Step [2/5], Loss: 311.2166\n",
      "Epoch [8/35], Step [3/5], Loss: 498.4444\n",
      "Epoch [8/35], Step [4/5], Loss: 566.8727\n",
      "Epoch [8/35], Step [5/5], Loss: 886.7910\n",
      "Epoch [9/35], Step [1/5], Loss: 308.5164\n",
      "Epoch [9/35], Step [2/5], Loss: 264.3731\n",
      "Epoch [9/35], Step [3/5], Loss: 437.2801\n",
      "Epoch [9/35], Step [4/5], Loss: 118.5543\n",
      "Epoch [9/35], Step [5/5], Loss: 447.4707\n",
      "Epoch [10/35], Step [1/5], Loss: 412.8325\n",
      "Epoch [10/35], Step [2/5], Loss: 370.9893\n",
      "Epoch [10/35], Step [3/5], Loss: 159.7677\n",
      "Epoch [10/35], Step [4/5], Loss: 171.5848\n",
      "Epoch [10/35], Step [5/5], Loss: 303.8600\n",
      "Epoch [11/35], Step [1/5], Loss: 96.4746\n",
      "Epoch [11/35], Step [2/5], Loss: 113.7680\n",
      "Epoch [11/35], Step [3/5], Loss: 191.0439\n",
      "Epoch [11/35], Step [4/5], Loss: 196.3135\n",
      "Epoch [11/35], Step [5/5], Loss: 202.1194\n",
      "Epoch [12/35], Step [1/5], Loss: 133.7345\n",
      "Epoch [12/35], Step [2/5], Loss: 33.3347\n",
      "Epoch [12/35], Step [3/5], Loss: 73.8742\n",
      "Epoch [12/35], Step [4/5], Loss: 38.4286\n",
      "Epoch [12/35], Step [5/5], Loss: 98.6984\n",
      "Epoch [13/35], Step [1/5], Loss: 70.1870\n",
      "Epoch [13/35], Step [2/5], Loss: 39.9795\n",
      "Epoch [13/35], Step [3/5], Loss: 61.4299\n",
      "Epoch [13/35], Step [4/5], Loss: 55.9645\n",
      "Epoch [13/35], Step [5/5], Loss: 58.8083\n",
      "Epoch [14/35], Step [1/5], Loss: 67.1472\n",
      "Epoch [14/35], Step [2/5], Loss: 40.7317\n",
      "Epoch [14/35], Step [3/5], Loss: 63.9287\n",
      "Epoch [14/35], Step [4/5], Loss: 65.1769\n",
      "Epoch [14/35], Step [5/5], Loss: 35.0771\n",
      "Epoch [15/35], Step [1/5], Loss: 53.9302\n",
      "Epoch [15/35], Step [2/5], Loss: 10.4098\n",
      "Epoch [15/35], Step [3/5], Loss: 41.1855\n",
      "Epoch [15/35], Step [4/5], Loss: 19.1693\n",
      "Epoch [15/35], Step [5/5], Loss: 34.8513\n",
      "Epoch [16/35], Step [1/5], Loss: 19.0517\n",
      "Epoch [16/35], Step [2/5], Loss: 20.6164\n",
      "Epoch [16/35], Step [3/5], Loss: 6.6002\n",
      "Epoch [16/35], Step [4/5], Loss: 25.2281\n",
      "Epoch [16/35], Step [5/5], Loss: 11.7932\n",
      "Epoch [17/35], Step [1/5], Loss: 25.3919\n",
      "Epoch [17/35], Step [2/5], Loss: 14.7954\n",
      "Epoch [17/35], Step [3/5], Loss: 13.9400\n",
      "Epoch [17/35], Step [4/5], Loss: 9.0097\n",
      "Epoch [17/35], Step [5/5], Loss: 10.6184\n",
      "Epoch [18/35], Step [1/5], Loss: 12.1028\n",
      "Epoch [18/35], Step [2/5], Loss: 20.6981\n",
      "Epoch [18/35], Step [3/5], Loss: 7.2242\n",
      "Epoch [18/35], Step [4/5], Loss: 4.5401\n",
      "Epoch [18/35], Step [5/5], Loss: 9.8474\n",
      "Epoch [19/35], Step [1/5], Loss: 1.2730\n",
      "Epoch [19/35], Step [2/5], Loss: 4.0273\n",
      "Epoch [19/35], Step [3/5], Loss: 8.5371\n",
      "Epoch [19/35], Step [4/5], Loss: 11.9009\n",
      "Epoch [19/35], Step [5/5], Loss: 4.2336\n",
      "Epoch [20/35], Step [1/5], Loss: 2.1041\n",
      "Epoch [20/35], Step [2/5], Loss: 3.8455\n",
      "Epoch [20/35], Step [3/5], Loss: 5.0431\n",
      "Epoch [20/35], Step [4/5], Loss: 12.0153\n",
      "Epoch [20/35], Step [5/5], Loss: 8.3133\n",
      "Epoch [21/35], Step [1/5], Loss: 2.4771\n",
      "Epoch [21/35], Step [2/5], Loss: 2.9244\n",
      "Epoch [21/35], Step [3/5], Loss: 2.7213\n",
      "Epoch [21/35], Step [4/5], Loss: 4.7199\n",
      "Epoch [21/35], Step [5/5], Loss: 9.7880\n",
      "Epoch [22/35], Step [1/5], Loss: 2.1414\n",
      "Epoch [22/35], Step [2/5], Loss: 1.3971\n",
      "Epoch [22/35], Step [3/5], Loss: 1.2038\n",
      "Epoch [22/35], Step [4/5], Loss: 4.4750\n",
      "Epoch [22/35], Step [5/5], Loss: 4.1911\n",
      "Epoch [23/35], Step [1/5], Loss: 2.3284\n",
      "Epoch [23/35], Step [2/5], Loss: 1.6672\n",
      "Epoch [23/35], Step [3/5], Loss: 0.6265\n",
      "Epoch [23/35], Step [4/5], Loss: 3.9251\n",
      "Epoch [23/35], Step [5/5], Loss: 2.4962\n",
      "Epoch [24/35], Step [1/5], Loss: 1.1504\n",
      "Epoch [24/35], Step [2/5], Loss: 0.7911\n",
      "Epoch [24/35], Step [3/5], Loss: 1.1339\n",
      "Epoch [24/35], Step [4/5], Loss: 0.5223\n",
      "Epoch [24/35], Step [5/5], Loss: 1.1119\n",
      "Epoch [25/35], Step [1/5], Loss: 1.8274\n",
      "Epoch [25/35], Step [2/5], Loss: 2.0713\n",
      "Epoch [25/35], Step [3/5], Loss: 2.7952\n",
      "Epoch [25/35], Step [4/5], Loss: 0.6030\n",
      "Epoch [25/35], Step [5/5], Loss: 1.6179\n",
      "Epoch [26/35], Step [1/5], Loss: 1.8306\n",
      "Epoch [26/35], Step [2/5], Loss: 1.2006\n",
      "Epoch [26/35], Step [3/5], Loss: 2.7605\n",
      "Epoch [26/35], Step [4/5], Loss: 0.7092\n",
      "Epoch [26/35], Step [5/5], Loss: 0.3355\n",
      "Epoch [27/35], Step [1/5], Loss: 0.5540\n",
      "Epoch [27/35], Step [2/5], Loss: 2.0829\n",
      "Epoch [27/35], Step [3/5], Loss: 1.7635\n",
      "Epoch [27/35], Step [4/5], Loss: 0.6696\n",
      "Epoch [27/35], Step [5/5], Loss: 1.2065\n",
      "Epoch [28/35], Step [1/5], Loss: 0.4325\n",
      "Epoch [28/35], Step [2/5], Loss: 1.2844\n",
      "Epoch [28/35], Step [3/5], Loss: 1.3609\n",
      "Epoch [28/35], Step [4/5], Loss: 0.9828\n",
      "Epoch [28/35], Step [5/5], Loss: 1.0568\n",
      "Epoch [29/35], Step [1/5], Loss: 0.4803\n",
      "Epoch [29/35], Step [2/5], Loss: 1.2231\n",
      "Epoch [29/35], Step [3/5], Loss: 1.0460\n",
      "Epoch [29/35], Step [4/5], Loss: 1.8749\n",
      "Epoch [29/35], Step [5/5], Loss: 0.9998\n",
      "Epoch [30/35], Step [1/5], Loss: 0.2972\n",
      "Epoch [30/35], Step [2/5], Loss: 0.6024\n",
      "Epoch [30/35], Step [3/5], Loss: 0.8439\n",
      "Epoch [30/35], Step [4/5], Loss: 1.7833\n",
      "Epoch [30/35], Step [5/5], Loss: 0.9283\n",
      "Epoch [31/35], Step [1/5], Loss: 0.2318\n",
      "Epoch [31/35], Step [2/5], Loss: 0.2864\n",
      "Epoch [31/35], Step [3/5], Loss: 1.1674\n",
      "Epoch [31/35], Step [4/5], Loss: 0.2938\n",
      "Epoch [31/35], Step [5/5], Loss: 0.7477\n",
      "Epoch [32/35], Step [1/5], Loss: 0.3223\n",
      "Epoch [32/35], Step [2/5], Loss: 0.4291\n",
      "Epoch [32/35], Step [3/5], Loss: 0.5107\n",
      "Epoch [32/35], Step [4/5], Loss: 0.3371\n",
      "Epoch [32/35], Step [5/5], Loss: 0.2875\n",
      "Epoch [33/35], Step [1/5], Loss: 0.2939\n",
      "Epoch [33/35], Step [2/5], Loss: 0.1781\n",
      "Epoch [33/35], Step [3/5], Loss: 0.1578\n",
      "Epoch [33/35], Step [4/5], Loss: 1.5651\n",
      "Epoch [33/35], Step [5/5], Loss: 0.2085\n",
      "Epoch [34/35], Step [1/5], Loss: 0.1897\n",
      "Epoch [34/35], Step [2/5], Loss: 0.1162\n",
      "Epoch [34/35], Step [3/5], Loss: 0.3642\n",
      "Epoch [34/35], Step [4/5], Loss: 0.3994\n",
      "Epoch [34/35], Step [5/5], Loss: 0.0913\n",
      "Epoch [35/35], Step [1/5], Loss: 0.2507\n",
      "Epoch [35/35], Step [2/5], Loss: 0.3226\n",
      "Epoch [35/35], Step [3/5], Loss: 0.1407\n",
      "Epoch [35/35], Step [4/5], Loss: 0.1451\n",
      "Epoch [35/35], Step [5/5], Loss: 0.1425\n"
     ]
    }
   ],
   "source": [
    "print('Training model now...')\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i_batch, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i_batch+1) % 1 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1,\n",
    "                                                                      num_epochs,\n",
    "                                                                      i_batch+1,\n",
    "                                                                      total_step,\n",
    "                                                                      loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def imshow_coord(img, a_label):\n",
    "    img = torchvision.transforms.ToPILImage()(img)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw = draw_crosspointer(a_label, draw, 'green', 'white', length=8)\n",
    "    display(img)\n",
    "    \n",
    "def draw_crosspointer(xy, draw, fillcolor_X='green', fillcolor='white', length=2):\n",
    "    a,b = tuple(xy)\n",
    "    draw.line((a-length, b+length, a+length, b-length), fill=fillcolor_X)\n",
    "    draw.line((a-length, b-length, a+length, b+length), fill=fillcolor_X)\n",
    "    draw.point((a,b))\n",
    "    return draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "truth tensor([250., 262.])\n",
      "net output tensor([250.6488, 261.7675], grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 400, 300])\n",
      "net output: \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASwAAAGQCAIAAACbF8osAAACRElEQVR4nO3awQ6CMBBAQeiP65dTDyYeBGlLoGt15mS0sWziiyBOEwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABfZ44+ADjBPJ/5Sc45n/huRannZsCaCCGYCCGYCPlB+dZ2Ude6Hng3r0z3jSc3rVdGTwMDqqyrck30NDCgpsaKr0ZPAwM6UtrnPqOngQHtRLjZW+EbEmi1H+FbdcVrxehpYEDFCF/tVf1a0/ngO+8HV6gsZ1mWlMr3xv13FC6Rbzml5L48XKLyXHT9+BtOR+EX1BdY02H0NDCgpgKLHUZPAwNqLbDQJ9DqQIE7a6KngQEdK/DTyuhpYECHC9xc3/vgO+8HVzi3HDfr4b+IEIKJEIKJEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAB4egCB/CgAvRXOYgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=300x400 at 0x7FBD21E1ACC0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth label: \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASwAAAGQCAIAAACbF8osAAACRElEQVR4nO3awQ6CMBBAQeiP65dTDyYeBGlLoGt15mS0sWziiwhOEwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABfZ44+ADjBPJ/5Sc45n/huRannZsCaCCGYCCGYCCGYCPlB+dZ2ZaV1PfBuXpnuG09uWq+MngYGVFlX5ZroaWBATY0VX42eBgZ0pLTPfUZPAwPaiXCzt8I3JNBqP8K36oq/FaOngQEVI3y1V3W1pvPBd94PrlBZzrIsKZXvjfsDN1wi33JKyX15uETluej68TecjsIvqC+wpsPoaWBATQUWO4yeBgbUWmChT6DVgQJ31kRPAwM6VuCnldHTwIAOF7i5vvfBd94PrnBuOW7Ww38RIQQTIQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADw9AD4YigAabJLCAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=300x400 at 0x7FBD21E1A3C8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision \n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from IPython.display import display # to display images\n",
    "# get some random training images\n",
    "with torch.no_grad():\n",
    "    dataiter = iter(train_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    outputs = model(images)\n",
    "    \n",
    "    # show images\n",
    "    #imshow_coord(torchvision.utils.make_grid(images), (outputs))\n",
    "    \n",
    "    #imgA = torchvision.transforms.ToPILImage()(images[0])\n",
    "    #draw = ImageDraw.Draw(imgA)\n",
    "    #a,b = tuple(outputs[0])\n",
    "    #draw.point(tuple(outputs[0]), fill='white')\n",
    "    #imgA.show()\n",
    "\n",
    "    # _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                                  # for j in range(4)))\n",
    " \n",
    "n =5 \n",
    "outputs = model(images)\n",
    "print(len(labels))\n",
    "print('truth', labels[n])\n",
    "print('net output', outputs[n])\n",
    "\n",
    "images_copy = copy.deepcopy(images)\n",
    "print(images_copy[n].size())\n",
    "print('net output: ')\n",
    "imshow_coord(images_copy[n], outputs[n])\n",
    "print('truth label: ')\n",
    "imshow_coord(images_copy[n], labels[n])\n",
    "#imagePIL = torchvision.transforms.ToPILImage()(images[n])\n",
    "#imagePIL.save('test.png')\n",
    "#display(imagePIL)\n",
    "\n",
    "#display(Image.open('./data/rect'+str(n)+'.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250 262]\n",
      "tensor([[250.6488, 261.7675]], grad_fn=<ThAddmmBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbd21428630>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#im = Image.open(\"Mew.jpg\")\n",
    "n = 2\n",
    "\n",
    "image = io.imread('./data/rect'+str(n)+'.png')\n",
    "image_tensor = torch.FloatTensor(image).permute(2, 0, 1) #PIL and torch expect difft orders\n",
    "coords = torch.FloatTensor(true_coords[n])\n",
    "output = model(image_tensor)\n",
    "\n",
    "print(true_coords[n])\n",
    "print(output)\n",
    "plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 400, 300])\n",
      "tensor(255.) tensor(0.) torch.Size([3, 806, 2418])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbd1dadf160>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#img = Image.new('RGB', (300,400), 'gray')\n",
    "n = 0\n",
    "#imshow_coord(images[n], outputs[n], labels[n])\n",
    "\n",
    "print(images.size())\n",
    "a = torchvision.utils.make_grid(images)\n",
    "print(a.max(), a.min(), a.size())\n",
    "#a = a / 2 + 0.5     # unnormalize\n",
    "a = a.numpy()\n",
    "ran = a.max() - a.min()\n",
    "a = (a/ran) - 0.5\n",
    "plt.imshow(np.transpose(a, (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
